{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a983a5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import nlpsig\n",
    "import nlpsig_networks\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "from nlpsig.classification_utils import split_dataset\n",
    "from nlpsig_networks.pytorch_utils import training_pytorch, testing_pytorch, set_seed\n",
    "from nlpsig_networks.ffn import FeedforwardNeuralNetModel\n",
    "from nlpsig_networks.deepsignet import StackedDeepSigNet\n",
    "from nlpsig_networks.focal_loss import FocalLoss, ClassBalanced_FocalLoss\n",
    "from sklearn import metrics\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "seed = 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15e93764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import signatory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7409a03",
   "metadata": {},
   "source": [
    "## AnnoMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f00bb922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mi_quality</th>\n",
       "      <th>transcript_id</th>\n",
       "      <th>topic</th>\n",
       "      <th>utterance_id</th>\n",
       "      <th>interlocutor</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>utterance_text</th>\n",
       "      <th>annotator_id</th>\n",
       "      <th>therapist_input_exists</th>\n",
       "      <th>therapist_input_subtype</th>\n",
       "      <th>reflection_exists</th>\n",
       "      <th>reflection_subtype</th>\n",
       "      <th>question_exists</th>\n",
       "      <th>question_subtype</th>\n",
       "      <th>main_therapist_behaviour</th>\n",
       "      <th>client_talk_type</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>high</td>\n",
       "      <td>0</td>\n",
       "      <td>reducing alcohol consumption</td>\n",
       "      <td>0</td>\n",
       "      <td>therapist</td>\n",
       "      <td>00:00:13</td>\n",
       "      <td>Thanks for filling it out. We give this form t...</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>open</td>\n",
       "      <td>question</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-05-10 00:00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>high</td>\n",
       "      <td>0</td>\n",
       "      <td>reducing alcohol consumption</td>\n",
       "      <td>1</td>\n",
       "      <td>client</td>\n",
       "      <td>00:00:24</td>\n",
       "      <td>Sure.</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2023-05-10 00:00:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>high</td>\n",
       "      <td>0</td>\n",
       "      <td>reducing alcohol consumption</td>\n",
       "      <td>2</td>\n",
       "      <td>therapist</td>\n",
       "      <td>00:00:25</td>\n",
       "      <td>So, let's see. It looks that you put-- You dri...</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>information</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>therapist_input</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-05-10 00:00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>high</td>\n",
       "      <td>0</td>\n",
       "      <td>reducing alcohol consumption</td>\n",
       "      <td>3</td>\n",
       "      <td>client</td>\n",
       "      <td>00:00:34</td>\n",
       "      <td>Mm-hmm.</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2023-05-10 00:00:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>high</td>\n",
       "      <td>0</td>\n",
       "      <td>reducing alcohol consumption</td>\n",
       "      <td>4</td>\n",
       "      <td>therapist</td>\n",
       "      <td>00:00:34</td>\n",
       "      <td>-and you usually have three to four drinks whe...</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>information</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>therapist_input</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-05-10 00:00:34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mi_quality  transcript_id                         topic  utterance_id  \\\n",
       "0       high              0  reducing alcohol consumption             0   \n",
       "1       high              0  reducing alcohol consumption             1   \n",
       "2       high              0  reducing alcohol consumption             2   \n",
       "3       high              0  reducing alcohol consumption             3   \n",
       "4       high              0  reducing alcohol consumption             4   \n",
       "\n",
       "  interlocutor timestamp                                     utterance_text  \\\n",
       "0    therapist  00:00:13  Thanks for filling it out. We give this form t...   \n",
       "1       client  00:00:24                                              Sure.   \n",
       "2    therapist  00:00:25  So, let's see. It looks that you put-- You dri...   \n",
       "3       client  00:00:34                                            Mm-hmm.   \n",
       "4    therapist  00:00:34  -and you usually have three to four drinks whe...   \n",
       "\n",
       "   annotator_id therapist_input_exists therapist_input_subtype  \\\n",
       "0             3                  False                     NaN   \n",
       "1             3                    NaN                     NaN   \n",
       "2             3                   True             information   \n",
       "3             3                    NaN                     NaN   \n",
       "4             3                   True             information   \n",
       "\n",
       "  reflection_exists reflection_subtype question_exists question_subtype  \\\n",
       "0             False                NaN            True             open   \n",
       "1               NaN                NaN             NaN              NaN   \n",
       "2             False                NaN           False              NaN   \n",
       "3               NaN                NaN             NaN              NaN   \n",
       "4             False                NaN           False              NaN   \n",
       "\n",
       "  main_therapist_behaviour client_talk_type            datetime  \n",
       "0                 question              NaN 2023-05-10 00:00:13  \n",
       "1                      NaN          neutral 2023-05-10 00:00:24  \n",
       "2          therapist_input              NaN 2023-05-10 00:00:25  \n",
       "3                      NaN          neutral 2023-05-10 00:00:34  \n",
       "4          therapist_input              NaN 2023-05-10 00:00:34  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anno_mi = pd.read_csv(\"AnnoMI-full.csv\")\n",
    "anno_mi[\"datetime\"] = pd.to_datetime(anno_mi[\"timestamp\"])\n",
    "anno_mi = anno_mi.drop(columns=[\"video_title\", \"video_url\"])\n",
    "anno_mi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cea8099b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13551"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(anno_mi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44217a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral    0.627063\n",
       "change     0.248030\n",
       "sustain    0.124907\n",
       "Name: client_talk_type, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anno_mi[\"client_talk_type\"].value_counts() / anno_mi[\"interlocutor\"].value_counts()[\"client\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "367137a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "therapist    6826\n",
       "client       6725\n",
       "Name: interlocutor, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anno_mi[\"interlocutor\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a5a319f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reducing alcohol consumption                                                          2326\n",
       "more exercise / increasing activity                                                   2034\n",
       "reducing recidivism                                                                   1303\n",
       "reducing drug use                                                                     1104\n",
       "diabetes management                                                                    948\n",
       "smoking cessation                                                                      923\n",
       "smoking cessation                                                                      541\n",
       "taking medicine / following medical procedure                                          448\n",
       "asthma management                                                                      431\n",
       "avoiding DOI                                                                           394\n",
       "changing approach to disease                                                           315\n",
       "reducing gambling                                                                      297\n",
       "weight loss                                                                            294\n",
       "unidentifiable                                                                         287\n",
       "smoking cessation; reducing alcohol consumption                                        254\n",
       "overcoming issues at school                                                            167\n",
       "compliance with rules                                                                  146\n",
       "supporting client to live in more alignment with her values                            133\n",
       "increasing activity; taking medicine / following medical procedure                     126\n",
       "better oral health                                                                      97\n",
       "managing life                                                                           86\n",
       "anxiety management                                                                      79\n",
       "more exercise / increasing activity; weight loss                                        66\n",
       "Being assertive with flatmate about moving out                                          62\n",
       "reducing drug use; following medical procedure                                          59\n",
       "taking steps towards getting help with day-care                                         57\n",
       "reducing alcohol consumption; safe sex                                                  55\n",
       "reducing self-harm                                                                      54\n",
       "reducing coffee consumption                                                             51\n",
       "increasing self-confidence                                                              50\n",
       "completion of community service                                                         46\n",
       "reducing violence                                                                       41\n",
       "diet; reducing alcohol consumption; diabetes management                                 40\n",
       "weight loss; diet                                                                       36\n",
       "birth control                                                                           36\n",
       "reducing alcohol consumption; compliance with rules                                     31\n",
       "charging battery                                                                        29\n",
       "diagnosis                                                                               21\n",
       "providing information on medicines                                                      19\n",
       "engaging in community activities                                                        17\n",
       "problem recognition                                                                     16\n",
       "opening up                                                                              15\n",
       "recognising success                                                                     11\n",
       "not getting into a car with someone who is under the influence of drugs or alcohol       6\n",
       "Name: topic, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anno_mi[\"topic\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "507f8a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(anno_mi[\"transcript_id\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab55eae",
   "metadata": {},
   "source": [
    "## Only considering client for now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "987e21e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6725"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_index = [isinstance(x, str) for x in anno_mi[\"client_talk_type\"]]\n",
    "sum(client_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eaaa3757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6725,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data = anno_mi[\"client_talk_type\"][client_index]\n",
    "y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f831d287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     neutral\n",
       "3     neutral\n",
       "5     neutral\n",
       "7     neutral\n",
       "9     neutral\n",
       "11    neutral\n",
       "13    neutral\n",
       "15    neutral\n",
       "17    neutral\n",
       "19    neutral\n",
       "21    neutral\n",
       "23    neutral\n",
       "25    neutral\n",
       "27    neutral\n",
       "29    neutral\n",
       "31    neutral\n",
       "33    neutral\n",
       "35     change\n",
       "37     change\n",
       "39     change\n",
       "Name: client_talk_type, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a645315",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_id = {y_data.unique()[i]: i for i in range(len(y_data.unique()))}\n",
    "id_to_label = {v: k for k, v in label_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a419b2bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neutral': 0, 'change': 1, 'sustain': 2}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d63baf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'neutral', 1: 'change', 2: 'sustain'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e98f57b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data = [label_to_id[x] for x in y_data]\n",
    "y_data[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a35e43",
   "metadata": {},
   "source": [
    "## Obtaining SBERT Embeddings\n",
    "\n",
    "We can use the `SentenceEncoder` class within `nlpsig` to obtain sentence embeddings from a model. This class uses the [`sentence-transformer`](https://www.sbert.net/docs/package_reference/SentenceTransformer.html) package and here, we have use the pre-trained `all-mpnet-base-v2` model by passing this name as a string to the class - alternative models can be found [here](https://www.sbert.net/docs/pretrained_models.html).\n",
    "\n",
    "We can pass these into the constructor of the class to initialise our text encoder as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6585f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sbert_768_embeddings = np.load(\"anno_mi_sentence_embeddings_768.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07fe57d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the Text Encoder\n",
    "sbert_model_768 = \"all-mpnet-base-v2\"\n",
    "text_encoder_sbert_768 = nlpsig.SentenceEncoder(df=anno_mi,\n",
    "                                                feature_name=\"utterance_text\",\n",
    "                                                model_name=sbert_model_768)\n",
    "text_encoder_sbert_768.load_pretrained_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798b0890",
   "metadata": {},
   "source": [
    "The class has a `.encode_sentence_transformer()` method which first loads in the model (using the `model_name` and `model_args` attributes) and then obtains an embedding for each sentence. These sentence embeddings are then stored in the `embeddings_sentence` attribute of the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b984327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] number of sentences to encode: 13551\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a3a67e3c8e4ba29e300e29d5b8c509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_encoder_sbert_768.obtain_embeddings()\n",
    "sbert_768_embeddings = text_encoder_sbert_768.sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4dbab3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"anno_mi_sentence_embeddings_768\", sbert_768_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c061526d",
   "metadata": {},
   "source": [
    "## SBERT with 384 dimension vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fed711d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sbert_384_embeddings = np.load(\"anno_mi_sentence_embeddings_384.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f34295a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the Text Encoder\n",
    "sbert_model_384 = \"all-MiniLM-L12-v2\"\n",
    "text_encoder_sbert_384 = nlpsig.SentenceEncoder(df=anno_mi,\n",
    "                                                feature_name=\"utterance_text\",\n",
    "                                                model_name=sbert_model_384)\n",
    "text_encoder_sbert_384.load_pretrained_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91d943d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] number of sentences to encode: 13551\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dda6f6b4ccb14589ad271ae010fc6d13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_encoder_sbert_384.obtain_embeddings()\n",
    "sbert_384_embeddings = text_encoder_sbert_384.sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eebe5622",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"anno_mi_sentence_embeddings_384\", sbert_384_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c3551e",
   "metadata": {},
   "source": [
    "## Pretrained BERT and pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc1dd870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pooled_mean_pretrained = np.load(\"anno_mi_pretrained_BERT_mean.npy\")\n",
    "# pooled_max_pretrained = np.load(\"anno_mi_pretrained_BERT_max.npy\")\n",
    "# pooled_sum_pretrained = np.load(\"anno_mi_pretrained_BERT_sum.npy\")\n",
    "# pooled_cls_pretrained = np.load(\"anno_mi_pretrained_BERT_cls.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bcbef39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42342f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "text_encoder_pretrained_BERT = nlpsig.TextEncoder(df=anno_mi,\n",
    "                                                  feature_name=\"utterance_text\",\n",
    "                                                  model_name=bert_model)\n",
    "text_encoder_pretrained_BERT.load_pretrained_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f63f07ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Setting return_special_tokens_mask=True\n",
      "[INFO] Tokenizing the dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13551 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving the tokenized text for each sentence into `.df['tokens']`...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13551 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating tokenized dataframe and setting in `.tokenized_df` attribute...\n",
      "[INFO] Note: 'text_id' is the column name for denoting the corresponding text id\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['mi_quality', 'transcript_id', 'topic', 'utterance_id', 'interlocutor', 'timestamp', 'utterance_text', 'annotator_id', 'therapist_input_exists', 'therapist_input_subtype', 'reflection_exists', 'reflection_subtype', 'question_exists', 'question_subtype', 'main_therapist_behaviour', 'client_talk_type', 'datetime', 'input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask', 'tokens'],\n",
       "    num_rows: 13551\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder_pretrained_BERT.tokenize_text(skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ae1dfbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27e0296f8c2244c0bd37a79b2ed773f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/136 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "token_embeddings_pretrained = text_encoder_pretrained_BERT.obtain_embeddings(method=\"hidden_layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0cd9526f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af56fde9d7974848b3ee2f426e5312d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a5e2741f2cd4db3a1428eabfde89e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "020c2619b18f4c348df9add3d7e62a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2abc947b4dde41aeb72d417dc46e4681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pooled_mean_pretrained = text_encoder_pretrained_BERT.pool_token_embeddings()\n",
    "pooled_max_pretrained = text_encoder_pretrained_BERT.pool_token_embeddings(method=\"max\")\n",
    "pooled_sum_pretrained = text_encoder_pretrained_BERT.pool_token_embeddings(method=\"sum\") \n",
    "pooled_cls_pretrained = text_encoder_pretrained_BERT.pool_token_embeddings(method=\"cls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3edcc5f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13551, 768)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_mean_pretrained.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "083f7bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13551, 768)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_max_pretrained.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "316562b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13551, 768)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_sum_pretrained.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0e9d0f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13551, 768)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_cls_pretrained.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "365c3c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"anno_mi_pretrained_BERT_mean\", pooled_mean_pretrained)\n",
    "np.save(\"anno_mi_pretrained_BERT_max\", pooled_max_pretrained)\n",
    "np.save(\"anno_mi_pretrained_BERT_sum\", pooled_sum_pretrained)\n",
    "np.save(\"anno_mi_pretrained_BERT_cls\", pooled_cls_pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f49f72",
   "metadata": {},
   "source": [
    "## Fine-tuning BERT and pooling\n",
    "\n",
    "### (Ignoring this part for now while, but will run this on GPU cluster soon...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4a6f131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pooled_mean = np.load(\"anno_mi_BERT_mean.npy\")\n",
    "# pooled_max = np.load(\"anno_mi_BERT_max.npy\")\n",
    "# pooled_sum = np.load(\"anno_mi_BERT_sum.npy\")\n",
    "# pooled_cls = np.load(\"anno_mi_BERT_cls.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "34a4fbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(bert_model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_model)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "190dc237",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder_BERT = nlpsig.TextEncoder(df=anno_mi,\n",
    "                                       feature_name=\"utterance_text\",\n",
    "                                       model=model,\n",
    "                                       tokenizer=tokenizer,\n",
    "                                       data_collator=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "339552bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Setting return_special_tokens_mask=True\n",
      "[INFO] Tokenizing the dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13551 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving the tokenized text for each sentence into `.df['tokens']`...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13551 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating tokenized dataframe and setting in `.tokenized_df` attribute...\n",
      "[INFO] Note: 'text_id' is the column name for denoting the corresponding text id\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['mi_quality', 'transcript_id', 'topic', 'utterance_id', 'interlocutor', 'timestamp', 'utterance_text', 'annotator_id', 'therapist_input_exists', 'therapist_input_subtype', 'reflection_exists', 'reflection_subtype', 'question_exists', 'question_subtype', 'main_therapist_behaviour', 'client_talk_type', 'datetime', 'tokens', 'input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "    num_rows: 13551\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder_BERT.tokenize_text(skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6103088",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "92b78f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up data_collator for language modelling (has dynamic padding)\n",
    "data_collator_for_LM = DataCollatorForLanguageModeling(tokenizer=tokenizer,\n",
    "                                                       mlm=True,\n",
    "                                                       mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "67967841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Splitting up dataset into train / validation / test sets, and saving to `.dataset_split`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['mi_quality', 'transcript_id', 'topic', 'utterance_id', 'interlocutor', 'timestamp', 'utterance_text', 'annotator_id', 'therapist_input_exists', 'therapist_input_subtype', 'reflection_exists', 'reflection_subtype', 'question_exists', 'question_subtype', 'main_therapist_behaviour', 'client_talk_type', 'datetime', 'tokens', 'input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 10840\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['mi_quality', 'transcript_id', 'topic', 'utterance_id', 'interlocutor', 'timestamp', 'utterance_text', 'annotator_id', 'therapist_input_exists', 'therapist_input_subtype', 'reflection_exists', 'reflection_subtype', 'question_exists', 'question_subtype', 'main_therapist_behaviour', 'client_talk_type', 'datetime', 'tokens', 'input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 1356\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['mi_quality', 'transcript_id', 'topic', 'utterance_id', 'interlocutor', 'timestamp', 'utterance_text', 'annotator_id', 'therapist_input_exists', 'therapist_input_subtype', 'reflection_exists', 'reflection_subtype', 'question_exists', 'question_subtype', 'main_therapist_behaviour', 'client_talk_type', 'datetime', 'tokens', 'input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 1355\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder_BERT.split_dataset(seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "80f5416e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text_encoder_BERT.dataset_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "681b96f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Setting up TrainingArguments object and saving to `.training_args`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=0,\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "auto_find_batch_size=False,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "data_seed=None,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_pin_memory=True,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "ddp_timeout=1800,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "do_eval=True,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_delay=0,\n",
       "eval_steps=None,\n",
       "evaluation_strategy=epoch,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "fsdp=[],\n",
       "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
       "fsdp_min_num_params=0,\n",
       "fsdp_transformer_layer_cls_to_wrap=None,\n",
       "full_determinism=False,\n",
       "gradient_accumulation_steps=1,\n",
       "gradient_checkpointing=False,\n",
       "greater_is_better=None,\n",
       "group_by_length=False,\n",
       "half_precision_backend=auto,\n",
       "hub_model_id=None,\n",
       "hub_private_repo=False,\n",
       "hub_strategy=every_save,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "include_inputs_for_metrics=False,\n",
       "jit_mode_eval=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=5e-05,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=False,\n",
       "local_rank=-1,\n",
       "log_level=passive,\n",
       "log_level_replica=warning,\n",
       "log_on_each_node=True,\n",
       "logging_dir=bert-base-uncased-anno-mi/runs/May05_13-23-13_MAC-ATI1167,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=500,\n",
       "logging_strategy=steps,\n",
       "lr_scheduler_type=linear,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=None,\n",
       "mp_parameters=,\n",
       "no_cuda=False,\n",
       "num_train_epochs=600,\n",
       "optim=adamw_hf,\n",
       "optim_args=None,\n",
       "output_dir=bert-base-uncased-anno-mi,\n",
       "overwrite_output_dir=False,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=8,\n",
       "per_device_train_batch_size=128,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "ray_scope=last,\n",
       "remove_unused_columns=True,\n",
       "report_to=[],\n",
       "resume_from_checkpoint=None,\n",
       "run_name=bert-base-uncased-anno-mi,\n",
       "save_on_each_node=False,\n",
       "save_steps=10000,\n",
       "save_strategy=steps,\n",
       "save_total_limit=None,\n",
       "seed=2023,\n",
       "sharded_ddp=[],\n",
       "skip_memory_metrics=True,\n",
       "tf32=None,\n",
       "torch_compile=False,\n",
       "torch_compile_backend=None,\n",
       "torch_compile_mode=None,\n",
       "torchdynamo=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_ipex=False,\n",
       "use_legacy_prediction_loop=False,\n",
       "use_mps_device=False,\n",
       "warmup_ratio=0.0,\n",
       "warmup_steps=0,\n",
       "weight_decay=0.0,\n",
       "xpu_backend=None,\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased-anno-mi\"\n",
    "text_encoder_BERT.set_up_training_args(output_dir=model_name,\n",
    "                                  num_train_epochs=600,\n",
    "                                  per_device_train_batch_size=128,\n",
    "                                  disable_tqdm=False,\n",
    "                                  save_strategy=\"steps\",\n",
    "                                  save_steps=10000,\n",
    "                                  seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "56d2eb68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.training_args.TrainingArguments"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text_encoder_BERT.training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "35c64c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Setting up Trainer object, and saving to `.trainer`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<transformers.trainer.Trainer at 0x2b04c1e80>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder_BERT.set_up_trainer(data_collator=data_collator_for_LM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a071795d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.trainer.Trainer"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text_encoder_BERT.trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a12991d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "610e7780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f60f7f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to only report errors to avoid excessing logging\n",
    "transformers.utils.logging.set_verbosity(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da010a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training model with 109514298 parameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rchan/opt/miniconda3/envs/nlpsig-networks/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='51000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    6/51000 01:31 < 323:48:12, 0.04 it/s, Epoch 0.06/600]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_encoder_BERT.fit_transformer_with_trainer_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0450c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder_BERT.trainer.save_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8962990a",
   "metadata": {},
   "source": [
    "### Evaluating model on masked language modelling task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c267755",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder_BERT.tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a77822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_masked_character_accuracy(fill_mask, words):\n",
    "    was_correct = []\n",
    "    print(f\"Evaluating with {len(words)} words\")\n",
    "    for word in tqdm(words):\n",
    "        masked_strings = [word[:i] + '<mask>' + word[i+1:] for i in range(len(word))]\n",
    "        predictions = [fill_mask(word)[0]['sequence'] for word in masked_strings]\n",
    "        was_correct += [pred == word for pred in predictions]\n",
    "    \n",
    "    acc = np.sum(was_correct) / len(was_correct)\n",
    "    print(f\"Accuracy: {acc}\")\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0067a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask = pipeline(\"fill-mask\",\n",
    "                     model=model_name,\n",
    "                     tokenizer=model_name)\n",
    "\n",
    "compute_masked_character_accuracy(fill_mask, text_encoder_BERT.dataset_split[\"test\"][\"word\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabb27fd",
   "metadata": {},
   "source": [
    "### Obtain embeddings from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d22ec72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the model to CPU (might not be always necessary to run this)\n",
    "text_encoder_BERT.model.to('cpu')\n",
    "token_embeddings = text_encoder_BERT.obtain_embeddings(method=\"hidden_layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e4bf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76db0094",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_mean = text_encoder_BERT.pool_token_embeddings()\n",
    "pooled_max = text_encoder_BERT.pool_token_embeddings(method=\"max\")\n",
    "pooled_sum = text_encoder_BERT.pool_token_embeddings(method=\"sum\")\n",
    "pooled_cls = text_encoder_BERT.pool_token_embeddings(method=\"cls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c215fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed90b53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_max.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82dbeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_sum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c7930e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_cls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019a0b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"anno_mi_BERT_mean\", pooled_mean)\n",
    "np.save(\"anno_mi_BERT_max\", pooled_max)\n",
    "np.save(\"anno_mi_BERT_sum\", pooled_sum)\n",
    "np.save(\"anno_mi_BERT_cls\", pooled_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0b63a6",
   "metadata": {},
   "source": [
    "# Baseline 1: FFN baseline\n",
    "\n",
    "Using the embeddings for the sentences directly in a FFN.\n",
    "\n",
    "Below is a function that takes in some inputs x_data, y_data and fits a FFN. Will do early stopping if the F1 score continually gets worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "499c61c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def implement_ffn(x_data,\n",
    "                  y_data,\n",
    "                  hidden_dim,\n",
    "                  learning_rate,\n",
    "                  loss,\n",
    "                  gamma=0):\n",
    "    # set seed\n",
    "    set_seed(seed)\n",
    "    \n",
    "    # initialise FFN\n",
    "    ffn_model = FeedforwardNeuralNetModel(input_dim=x_data.shape[1],\n",
    "                                          hidden_dim=hidden_dim,\n",
    "                                          output_dim=len(label_to_id),\n",
    "                                          dropout_rate=0.1)\n",
    "    # print(ffn_model)\n",
    "    \n",
    "    # split dataset\n",
    "    train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n",
    "                                       y_data=torch.tensor(y_data),\n",
    "                                       train_size=0.8,\n",
    "                                       valid_size=0.5,\n",
    "                                       shuffle=True,\n",
    "                                       as_DataLoader=True,\n",
    "                                       seed=seed)\n",
    "\n",
    "    # define loss\n",
    "    if loss == \"focal\":\n",
    "        criterion = FocalLoss(gamma = gamma)\n",
    "    elif loss == \"cross_entropy\":\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # define optimizer\n",
    "    optimizer = torch.optim.Adam(ffn_model.parameters(), lr=learning_rate)\n",
    "    # define scheduler for adjusting the learning rate\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "    # scheduler = StepLR(optimizer, step_size = 4, gamma = 0.5)\n",
    "    # scheduler = None\n",
    "    \n",
    "    ffn_model = training_pytorch(model=ffn_model,\n",
    "                                 train_loader=train,\n",
    "                                 criterion=criterion,\n",
    "                                 optimizer=optimizer,\n",
    "                                 num_epochs=10000,\n",
    "                                 scheduler=scheduler,\n",
    "                                 valid_loader=valid,\n",
    "                                 seed=seed,\n",
    "                                 early_stopping=True,\n",
    "                                 early_stopping_metric=\"f1\",\n",
    "                                 patience=10,\n",
    "                                 verbose=True,\n",
    "                                 verbose_epoch=100)\n",
    "\n",
    "    pred, label = testing_pytorch(ffn_model, test, criterion)\n",
    "    print(f\"proportion of labels in prediction: {[sum(pred==i)/len(pred) for i in label_to_id.values()]}\")\n",
    "    print(f\"proportion of labels in data: {[sum(label==i)/len(label) for i in label_to_id.values()]}\")\n",
    "    \n",
    "    f1_scores = metrics.f1_score(label, pred, average=None)\n",
    "    print(f\"- f1: {f1_scores}\")\n",
    "    print(f\"- f1 (average): {sum(f1_scores)/len(f1_scores)}\")\n",
    "    print(f\"- accuracy: {sum(pred==label)/len(pred)}\")\n",
    "    \n",
    "    return ffn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f571b0",
   "metadata": {},
   "source": [
    "Going to try out some variations (1 hidden layer, 2 hidden layers and 3 hidden layers - all of size 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5ec4d326",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim_trials = [[100]*i for i in range(1, 6)]\n",
    "learning_rate = 1e-4\n",
    "loss = \"cross_entropy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c512182c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[100],\n",
       " [100, 100],\n",
       " [100, 100, 100],\n",
       " [100, 100, 100, 100],\n",
       " [100, 100, 100, 100, 100]]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_dim_trials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bba019",
   "metadata": {},
   "source": [
    "## SBERT 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1064c00c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** hidden_dim: [100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "818e8a75f89b499084b69374bd1a99fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0885392427444458\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.023568868637085\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.016163945198059 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 40!\n",
      "Accuracy on dataset of size 672: 71.2797622680664 %.\n",
      "Average loss: 0.6995780386707999\n",
      "proportion of labels in prediction: [tensor(0.7173), tensor(0.1845), tensor(0.0982)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.81243063 0.53284672 0.47337278]\n",
      "- f1 (average): 0.6062167096746555\n",
      "- accuracy: 0.7127976417541504\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f32b1b3abc3a4057a950931edeb9402f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.116580843925476\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0521271228790283\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.026239974932237 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 45!\n",
      "Accuracy on dataset of size 672: 70.08928680419922 %.\n",
      "Average loss: 0.7389487461610273\n",
      "proportion of labels in prediction: [tensor(0.6875), tensor(0.2024), tensor(0.1101)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.80817253 0.51748252 0.46327684]\n",
      "- f1 (average): 0.5963106282850795\n",
      "- accuracy: 0.7008928656578064\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fb4803d4be244efbf7659308677c3d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0993850231170654\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0102291107177734\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.994836606762626 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 42!\n",
      "Accuracy on dataset of size 672: 70.23809814453125 %.\n",
      "Average loss: 0.7553030956875194\n",
      "proportion of labels in prediction: [tensor(0.6801), tensor(0.2098), tensor(0.1101)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.80821918 0.52920962 0.46327684]\n",
      "- f1 (average): 0.600235212077837\n",
      "- accuracy: 0.7023809552192688\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bc31cf5a42447fdb810ea87df98981a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.072503924369812\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.993831992149353\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9791417772119696 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 47!\n",
      "Accuracy on dataset of size 672: 69.04762268066406 %.\n",
      "Average loss: 0.8187992193482139\n",
      "proportion of labels in prediction: [tensor(0.6801), tensor(0.1890), tensor(0.1310)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.79908676 0.50541516 0.46073298]\n",
      "- f1 (average): 0.5884116349129783\n",
      "- accuracy: 0.6904761791229248\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b82ac0c25ed4c5f8f6b07ff12573fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1058584451675415\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0399665832519531\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0396888039328835 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 50!\n",
      "Accuracy on dataset of size 672: 69.19642639160156 %.\n",
      "Average loss: 0.8239231001247059\n",
      "proportion of labels in prediction: [tensor(0.7113), tensor(0.1815), tensor(0.1071)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.79821628 0.51470588 0.42285714]\n",
      "- f1 (average): 0.57859310056241\n",
      "- accuracy: 0.6919642686843872\n"
     ]
    }
   ],
   "source": [
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=sbert_768_embeddings[client_index],\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b783fc",
   "metadata": {},
   "source": [
    "## SBERT 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "88eadfca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** hidden_dim: [100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd64fa3ff5b46c7b17003a200c057f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0692368745803833\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0163383483886719\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0099996599284085 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 52!\n",
      "Accuracy on dataset of size 672: 71.875 %.\n",
      "Average loss: 0.7035864103924144\n",
      "proportion of labels in prediction: [tensor(0.7232), tensor(0.1696), tensor(0.1071)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.81767956 0.51515152 0.51428571]\n",
      "- f1 (average): 0.6157055958160931\n",
      "- accuracy: 0.71875\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e56c90c16cf146c0bfe8d03d23c9f4ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1142284870147705\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0404751300811768\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.043692480434071 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 45!\n",
      "Accuracy on dataset of size 672: 70.53571319580078 %.\n",
      "Average loss: 0.7237868959253485\n",
      "proportion of labels in prediction: [tensor(0.7247), tensor(0.1741), tensor(0.1012)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.80573951 0.51685393 0.46783626]\n",
      "- f1 (average): 0.5968099014143323\n",
      "- accuracy: 0.7053571343421936\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ad6268f02a84439ac02fb076f1e3fe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0598180294036865\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.9991912245750427\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.004194275899367 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 67!\n",
      "Accuracy on dataset of size 672: 70.68452453613281 %.\n",
      "Average loss: 0.7685691226612438\n",
      "proportion of labels in prediction: [tensor(0.6979), tensor(0.1830), tensor(0.1190)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.8018018  0.54212454 0.49180328]\n",
      "- f1 (average): 0.6119098742049561\n",
      "- accuracy: 0.706845223903656\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e608003236c74290b2930e1dd694b274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.075019359588623\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.9795213937759399\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0088304281234741 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 31!\n",
      "Accuracy on dataset of size 672: 65.92262268066406 %.\n",
      "Average loss: 0.8153174790469083\n",
      "proportion of labels in prediction: [tensor(0.7188), tensor(0.2812), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.81374723 0.44837758 0.        ]\n",
      "- f1 (average): 0.42070826983410625\n",
      "- accuracy: 0.6592261791229248\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b05b48342974e369c6946ca769d2474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1322060823440552\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0921815633773804\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0522005774758079 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 49!\n",
      "Accuracy on dataset of size 672: 66.81547546386719 %.\n",
      "Average loss: 0.814079609784213\n",
      "proportion of labels in prediction: [tensor(0.7009), tensor(0.2321), tensor(0.0670)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.80449438 0.43137255 0.33783784]\n",
      "- f1 (average): 0.524568256293306\n",
      "- accuracy: 0.668154776096344\n"
     ]
    }
   ],
   "source": [
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=sbert_384_embeddings[client_index],\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e54eab",
   "metadata": {},
   "source": [
    "## Pretrained BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4357ebd5",
   "metadata": {},
   "source": [
    "### Mean pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a839b7f7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** hidden_dim: [100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc243a02bc74d45925d01b228697c02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0839786529541016\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.7457855343818665\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.8354992433027788 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 43!\n",
      "Accuracy on dataset of size 672: 69.79166412353516 %.\n",
      "Average loss: 0.7122157866304571\n",
      "proportion of labels in prediction: [tensor(0.7039), tensor(0.2083), tensor(0.0878)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.80269058 0.51724138 0.44444444]\n",
      "- f1 (average): 0.5881254689048102\n",
      "- accuracy: 0.6979166865348816\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d89dc209fd48b9913f083fb2543fff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1131868362426758\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.8533899784088135\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.8532686233520508 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 35!\n",
      "Accuracy on dataset of size 672: 70.98213958740234 %.\n",
      "Average loss: 0.701988697052002\n",
      "proportion of labels in prediction: [tensor(0.6949), tensor(0.2128), tensor(0.0923)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.81038375 0.53924915 0.47272727]\n",
      "- f1 (average): 0.6074533888877603\n",
      "- accuracy: 0.7098214030265808\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0bde0b9336140c09371d1191f99c3dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.098788857460022\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.8470583558082581\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.8576378388838335 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 36!\n",
      "Accuracy on dataset of size 672: 70.83333587646484 %.\n",
      "Average loss: 0.7304971489039335\n",
      "proportion of labels in prediction: [tensor(0.6830), tensor(0.2188), tensor(0.0982)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.80637813 0.55892256 0.46153846]\n",
      "- f1 (average): 0.6089463841931573\n",
      "- accuracy: 0.7083333134651184\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fc03731998848909a2a2ccdc3a0a198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0730299949645996\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.835125207901001\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.8732638359069824 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 45!\n",
      "Accuracy on dataset of size 672: 70.68452453613281 %.\n",
      "Average loss: 0.7486193478107452\n",
      "proportion of labels in prediction: [tensor(0.7068), tensor(0.1994), tensor(0.0938)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.81208054 0.52112676 0.45783133]\n",
      "- f1 (average): 0.5970128742591122\n",
      "- accuracy: 0.706845223903656\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ba9a436d3ab44e18e5f0fc31dcec0c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1056439876556396\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.9540625214576721\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.882446364922957 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 51!\n",
      "Accuracy on dataset of size 672: 72.76786041259766 %.\n",
      "Average loss: 0.7767329216003418\n",
      "proportion of labels in prediction: [tensor(0.6905), tensor(0.1979), tensor(0.1116)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.81993205 0.55123675 0.5505618 ]\n",
      "- f1 (average): 0.6405768655665137\n",
      "- accuracy: 0.7276785969734192\n"
     ]
    }
   ],
   "source": [
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=pooled_mean_pretrained[client_index],\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe521c3",
   "metadata": {},
   "source": [
    "### Max pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "84f21d81",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** hidden_dim: [100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c5bc6f5e4e42e0b41d03328e524828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0957067012786865\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.7537550926208496\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.8680760372768749 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 52!\n",
      "Accuracy on dataset of size 672: 69.79166412353516 %.\n",
      "Average loss: 0.7068305503238331\n",
      "proportion of labels in prediction: [tensor(0.7173), tensor(0.2054), tensor(0.0774)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.80133185 0.52777778 0.41290323]\n",
      "- f1 (average): 0.580670952360115\n",
      "- accuracy: 0.6979166865348816\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77a168ac0f8e47e788b2845ea34e9487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1076050996780396\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.9455707669258118\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9177429025823419 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 53!\n",
      "Accuracy on dataset of size 672: 71.42857360839844 %.\n",
      "Average loss: 0.7085082368417219\n",
      "proportion of labels in prediction: [tensor(0.7202), tensor(0.2024), tensor(0.0774)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.80841639 0.53846154 0.49032258]\n",
      "- f1 (average): 0.6124001696394794\n",
      "- accuracy: 0.7142857313156128\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2fc7daa66544bd1acac762cdd97d92a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0997354984283447\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.929989755153656\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9196747053753246 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 53!\n",
      "Accuracy on dataset of size 672: 70.08928680419922 %.\n",
      "Average loss: 0.7250180461189963\n",
      "proportion of labels in prediction: [tensor(0.7024), tensor(0.2143), tensor(0.0833)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.8013468  0.53741497 0.44025157]\n",
      "- f1 (average): 0.5930044465534133\n",
      "- accuracy: 0.7008928656578064\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6815235e5060498dba04d6d09310f06d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0746076107025146\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.9190363883972168\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9312605370174755 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 53!\n",
      "Accuracy on dataset of size 672: 69.64286041259766 %.\n",
      "Average loss: 0.7155440043319355\n",
      "proportion of labels in prediction: [tensor(0.7083), tensor(0.2054), tensor(0.0863)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.79553073 0.52083333 0.45962733]\n",
      "- f1 (average): 0.5919971295942877\n",
      "- accuracy: 0.6964285969734192\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb2d81bee4f44e72a7c64c7c3cfaa446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1055570840835571\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.035354733467102\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9438045024871826 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 58!\n",
      "Accuracy on dataset of size 672: 70.23809814453125 %.\n",
      "Average loss: 0.7381011681123213\n",
      "proportion of labels in prediction: [tensor(0.7024), tensor(0.2068), tensor(0.0908)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.80359147 0.52595156 0.46341463]\n",
      "- f1 (average): 0.5976525538326346\n",
      "- accuracy: 0.7023809552192688\n"
     ]
    }
   ],
   "source": [
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=pooled_max_pretrained[client_index],\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c98576f",
   "metadata": {},
   "source": [
    "### Sum pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1a9e3600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** hidden_dim: [100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d30807b61e94c3791eb5de2ec0de99e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 2.0271031856536865\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.8685790300369263\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.8226919986984946 || Accuracy: 0.6463595628738403 || F1-score: 0.4247232595366326\n",
      "Early stopping at epoch 31!\n",
      "Accuracy on dataset of size 672: 70.98213958740234 %.\n",
      "Average loss: 0.774150935086337\n",
      "proportion of labels in prediction: [tensor(0.7173), tensor(0.2039), tensor(0.0789)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.81021088 0.55052265 0.42307692]\n",
      "- f1 (average): 0.5946034826546994\n",
      "- accuracy: 0.7098214030265808\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "887360b473bf49aa8a0a7fb95cd8c7d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.114878535270691\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.7568445205688477\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.8589342778379266 || Accuracy: 0.6508172154426575 || F1-score: 0.46548398969451604\n",
      "Early stopping at epoch 35!\n",
      "Accuracy on dataset of size 672: 70.83333587646484 %.\n",
      "Average loss: 0.7620173096656799\n",
      "proportion of labels in prediction: [tensor(0.7039), tensor(0.2113), tensor(0.0848)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.80493274 0.56849315 0.425     ]\n",
      "- f1 (average): 0.5994752953703134\n",
      "- accuracy: 0.7083333134651184\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb8d898dcbd74a9e8aad6e29f1ede595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.111946940422058\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.8133658766746521\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.8259054476564581 || Accuracy: 0.637444257736206 || F1-score: 0.4151506273197447\n",
      "Early stopping at epoch 29!\n",
      "Accuracy on dataset of size 672: 70.38690185546875 %.\n",
      "Average loss: 0.8185790181159973\n",
      "proportion of labels in prediction: [tensor(0.6815), tensor(0.2277), tensor(0.0908)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.8027366  0.55445545 0.45121951]\n",
      "- f1 (average): 0.6028038532640426\n",
      "- accuracy: 0.7038690447807312\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44cf8d5e6c5a42c7bf22197b6d13c34e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0758711099624634\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.8967788219451904\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9189632643352855 || Accuracy: 0.6315007209777832 || F1-score: 0.30164060280829436\n",
      "Early stopping at epoch 37!\n",
      "Accuracy on dataset of size 672: 72.32142639160156 %.\n",
      "Average loss: 0.8004374504089355\n",
      "proportion of labels in prediction: [tensor(0.7024), tensor(0.1935), tensor(0.1042)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.81481481 0.56428571 0.50867052]\n",
      "- f1 (average): 0.6292570164439142\n",
      "- accuracy: 0.7232142686843872\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1c0d80ebf624573b6ab48a42c1e7304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1112335920333862\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.9961893558502197\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.975874434817921 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 41!\n",
      "Accuracy on dataset of size 672: 70.08928680419922 %.\n",
      "Average loss: 0.9458087682723999\n",
      "proportion of labels in prediction: [tensor(0.6830), tensor(0.2158), tensor(0.1012)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.79954442 0.53559322 0.47953216]\n",
      "- f1 (average): 0.6048899344053565\n",
      "- accuracy: 0.7008928656578064\n"
     ]
    }
   ],
   "source": [
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=pooled_sum_pretrained[client_index],\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0389cfc",
   "metadata": {},
   "source": [
    "### CLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4f08c2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** hidden_dim: [100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92467b7cfb594cb1836f34e26b603671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.072586178779602\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.7834479808807373\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.8409460566260598 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 43!\n",
      "Accuracy on dataset of size 672: 71.875 %.\n",
      "Average loss: 0.6774346611716531\n",
      "proportion of labels in prediction: [tensor(0.7366), tensor(0.1786), tensor(0.0848)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.81619256 0.53333333 0.475     ]\n",
      "- f1 (average): 0.6081752978361293\n",
      "- accuracy: 0.71875\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "971770230441481ca223ce6e4ad422b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1158097982406616\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.9066270589828491\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.8640799901702187 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 38!\n",
      "Accuracy on dataset of size 672: 72.02381134033203 %.\n",
      "Average loss: 0.673450849272988\n",
      "proportion of labels in prediction: [tensor(0.7351), tensor(0.1741), tensor(0.0908)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.81489595 0.52434457 0.51219512]\n",
      "- f1 (average): 0.617145212888559\n",
      "- accuracy: 0.7202380895614624\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9206f44232d143bca47e50b29b436dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.099726676940918\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.9140689969062805\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.8757737441496416 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 44!\n",
      "Accuracy on dataset of size 672: 71.57737731933594 %.\n",
      "Average loss: 0.6988080143928528\n",
      "proportion of labels in prediction: [tensor(0.7068), tensor(0.1964), tensor(0.0967)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.81208054 0.5248227  0.52380952]\n",
      "- f1 (average): 0.6202375852525788\n",
      "- accuracy: 0.7157738208770752\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ce91de422843e28ac7caca38c6804e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0739099979400635\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.8497714996337891\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.8862914334643971 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 36!\n",
      "Accuracy on dataset of size 672: 70.08928680419922 %.\n",
      "Average loss: 0.7242212187160145\n",
      "proportion of labels in prediction: [tensor(0.6786), tensor(0.2381), tensor(0.0833)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.79771429 0.5483871  0.46540881]\n",
      "- f1 (average): 0.6038367291733087\n",
      "- accuracy: 0.7008928656578064\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5336895762ff41abaf43f039c8871f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1050199270248413\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0113009214401245\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.8908429525115273 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 60!\n",
      "Accuracy on dataset of size 672: 70.38690185546875 %.\n",
      "Average loss: 0.7129268104379828\n",
      "proportion of labels in prediction: [tensor(0.7217), tensor(0.1771), tensor(0.1012)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.80530973 0.49814126 0.49122807]\n",
      "- f1 (average): 0.5982263562097445\n",
      "- accuracy: 0.7038690447807312\n"
     ]
    }
   ],
   "source": [
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=pooled_cls_pretrained[client_index],\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f14f09",
   "metadata": {},
   "source": [
    "## Fine-tuned BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961a8d10",
   "metadata": {},
   "source": [
    "### Mean pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d413c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=pooled_mean_pretrained[client_index],\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=1e-5,\n",
    "                  loss=\"cross_entropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f00565",
   "metadata": {},
   "source": [
    "### Max pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff32803",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=pooled_max_pretrained[client_index],\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bf8ce1",
   "metadata": {},
   "source": [
    "### Sum pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f0f615",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=pooled_sum_pretrained[client_index],\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686c005a",
   "metadata": {},
   "source": [
    "### CLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2189644",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=pooled_sum_pretrained[client_index],\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a394c6",
   "metadata": {},
   "source": [
    "# Baseline 2: Averaging history and use FFN\n",
    "\n",
    "Here, we will use `nlpsig` to construct some paths of embeddings which we will average and use those in a FFN.\n",
    "\n",
    "First, we define the arguments for how we want to construct our path. As we're going to just do a simple average of embeddings, I'll set zero padding as false, and construct the path by looking at the last `k` posts.\n",
    "\n",
    "We will consider one where we average their histories and that is the only inputs to the FFN. Alternatively, we can concatenate the full post embedding as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bee6c81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_specifics = {\"pad_by\": \"history\",\n",
    "                  \"zero_padding\": False,\n",
    "                  \"method\": \"k_last\",\n",
    "                  \"k\": 5,\n",
    "                  \"time_feature\": None,\n",
    "                  \"embeddings\": \"full\",\n",
    "                  \"include_current_embedding\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "05873982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_mean_history(embeddings, path_specifics, concatenate_current = True):\n",
    "    paths = nlpsig.PrepareData(anno_mi,\n",
    "                               id_column=\"transcript_id\",\n",
    "                               label_column=\"client_talk_type\",\n",
    "                               embeddings=embeddings)\n",
    "    path = paths.pad(**path_specifics)\n",
    "    # remove last two columns (which contains the id and the label)\n",
    "    path = path[client_index][:,:,:-2]\n",
    "    # average in the first dimension\n",
    "    path = path.mean(1).astype(\"float\")\n",
    "    # concatenate with current embedding\n",
    "    if concatenate_current:\n",
    "        path = np.concatenate([path, embeddings[client_index]], axis=1)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ec5d80",
   "metadata": {},
   "source": [
    "## SBERT 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5a43fc4e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Adding 'time_encoding' and feature...\n",
      "[INFO] Adding 'time_diff' and feature...\n",
      "[INFO] Adding 'timeline_index' feature...\n",
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55fb05bd9f4a49aa8547b38e29e617dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** hidden_dim: [100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04eff464f3c9436dbc0695eef876e233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0953056812286377\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.9892064332962036\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9894395037130876 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 33!\n",
      "Accuracy on dataset of size 672: 71.13095092773438 %.\n",
      "Average loss: 0.6916292418133129\n",
      "proportion of labels in prediction: [tensor(0.7068), tensor(0.1964), tensor(0.0967)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.80536913 0.57446809 0.44047619]\n",
      "- f1 (average): 0.6067711343664507\n",
      "- accuracy: 0.711309552192688\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db30e61f4b2e499483eae7ba4e90b7e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0840489864349365\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.9823988676071167\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9628760164434259 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 39!\n",
      "Accuracy on dataset of size 672: 71.2797622680664 %.\n",
      "Average loss: 0.7162079377607866\n",
      "proportion of labels in prediction: [tensor(0.6920), tensor(0.1964), tensor(0.1116)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.80542986 0.59574468 0.43820225]\n",
      "- f1 (average): 0.6131255974318229\n",
      "- accuracy: 0.7127976417541504\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc6a94c8d454253b3860fea978c4e21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0726913213729858\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.004361629486084\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9863766540180553 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 36!\n",
      "Accuracy on dataset of size 672: 70.98213958740234 %.\n",
      "Average loss: 0.7356059171936729\n",
      "proportion of labels in prediction: [tensor(0.6801), tensor(0.2054), tensor(0.1146)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.80593607 0.56944444 0.46666667]\n",
      "- f1 (average): 0.6140157280568239\n",
      "- accuracy: 0.7098214030265808\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a96d0be99dc441aa34d5c5c2c6baeb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.094172477722168\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0252065658569336\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9813522750681097 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 49!\n",
      "Accuracy on dataset of size 672: 71.57737731933594 %.\n",
      "Average loss: 0.8058877749876543\n",
      "proportion of labels in prediction: [tensor(0.6667), tensor(0.2217), tensor(0.1116)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.80968858 0.60200669 0.4494382 ]\n",
      "- f1 (average): 0.6203778241750936\n",
      "- accuracy: 0.7157738208770752\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cbdc2c77d8546b0aa27337a25bbcaa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.079013705253601\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0607026815414429\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.000501123341647 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 42!\n",
      "Accuracy on dataset of size 672: 67.55952453613281 %.\n",
      "Average loss: 0.7973260446028276\n",
      "proportion of labels in prediction: [tensor(0.7083), tensor(0.2917), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.82011173 0.50289017 0.        ]\n",
      "- f1 (average): 0.44100063508466003\n",
      "- accuracy: 0.675595223903656\n"
     ]
    }
   ],
   "source": [
    "path_history = obtain_mean_history(sbert_768_embeddings, path_specifics)\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=path_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "abcaa6b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Adding 'time_encoding' and feature...\n",
      "[INFO] Adding 'time_diff' and feature...\n",
      "[INFO] Adding 'timeline_index' feature...\n",
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8894ce605794257bf83eee63958815b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** hidden_dim: [100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12dd3d0eb12044b7acbba5a1b33df21a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.088150978088379\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0530861616134644\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.024484547701749 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.6441184282302856\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.6339896321296692\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.8492850823835894 || Accuracy: 0.6285289525985718 || F1-score: 0.4134916549087558\n",
      "Early stopping at epoch 109!\n",
      "Accuracy on dataset of size 672: 61.60714340209961 %.\n",
      "Average loss: 0.8589339093728499\n",
      "proportion of labels in prediction: [tensor(0.8155), tensor(0.1429), tensor(0.0417)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.74870734 0.30081301 0.22900763]\n",
      "- f1 (average): 0.4261759946712092\n",
      "- accuracy: 0.6160714030265808\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbb7ad8d9725480584fc872cb622a04e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1165460348129272\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0676894187927246\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.034143946387551 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 55!\n",
      "Accuracy on dataset of size 672: 61.904762268066406 %.\n",
      "Average loss: 0.8625814156098799\n",
      "proportion of labels in prediction: [tensor(0.8482), tensor(0.1027), tensor(0.0491)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.760364   0.20091324 0.26470588]\n",
      "- f1 (average): 0.4086610428021877\n",
      "- accuracy: 0.6190476417541504\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2daf8419fc6846a58c3833f036d06e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0994921922683716\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0285676717758179\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0022970925677905 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 56!\n",
      "Accuracy on dataset of size 672: 64.13690185546875 %.\n",
      "Average loss: 0.9409157254479148\n",
      "proportion of labels in prediction: [tensor(0.8080), tensor(0.1265), tensor(0.0655)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.77754678 0.24680851 0.38095238]\n",
      "- f1 (average): 0.46843588971248545\n",
      "- accuracy: 0.6413690447807312\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da6d3e604dd542af844f6b87f41c4432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0725284814834595\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0014491081237793\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.983965044671839 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 62!\n",
      "Accuracy on dataset of size 672: 63.54166793823242 %.\n",
      "Average loss: 0.9649328806183555\n",
      "proportion of labels in prediction: [tensor(0.7708), tensor(0.1429), tensor(0.0863)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.77267876 0.25203252 0.42236025]\n",
      "- f1 (average): 0.4823571769262706\n",
      "- accuracy: 0.6354166865348816\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b67be2d3ab44d05819a75c64f77cff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.105851650238037\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0425540208816528\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.040925459428267 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 54!\n",
      "Accuracy on dataset of size 672: 62.2023811340332 %.\n",
      "Average loss: 1.0156917788765647\n",
      "proportion of labels in prediction: [tensor(0.7440), tensor(0.1771), tensor(0.0789)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.75952122 0.29739777 0.37179487]\n",
      "- f1 (average): 0.476237953342532\n",
      "- accuracy: 0.6220238208770752\n"
     ]
    }
   ],
   "source": [
    "path_history = obtain_mean_history(sbert_768_embeddings, path_specifics, concatenate_current=False)\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=path_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae00f3cd",
   "metadata": {},
   "source": [
    "## SBERT 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0b67725f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Adding 'time_encoding' and feature...\n",
      "[INFO] Adding 'time_diff' and feature...\n",
      "[INFO] Adding 'timeline_index' feature...\n",
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "776f871f588a4e6ca80efdeaecd034f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** hidden_dim: [100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fc117945d72459a95c349fd728663e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0886486768722534\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.024571180343628\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0028003996068782 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 50!\n",
      "Accuracy on dataset of size 672: 72.32142639160156 %.\n",
      "Average loss: 0.7124848663806915\n",
      "proportion of labels in prediction: [tensor(0.7217), tensor(0.1637), tensor(0.1146)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.82522124 0.51538462 0.51111111]\n",
      "- f1 (average): 0.6172389884779266\n",
      "- accuracy: 0.7232142686843872\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c607e7900cb4334a41a74768f88db8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1162652969360352\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0470809936523438\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0164382349361072 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 51!\n",
      "Accuracy on dataset of size 672: 71.2797622680664 %.\n",
      "Average loss: 0.7254285487261686\n",
      "proportion of labels in prediction: [tensor(0.6964), tensor(0.1801), tensor(0.1235)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.82074408 0.52398524 0.47311828]\n",
      "- f1 (average): 0.6059492001982608\n",
      "- accuracy: 0.7127976417541504\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f71bf694c4347af98a8d8a605aa7c82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.099664330482483\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0039958953857422\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.982631347396157 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 42!\n",
      "Accuracy on dataset of size 672: 71.13095092773438 %.\n",
      "Average loss: 0.7638534090735696\n",
      "proportion of labels in prediction: [tensor(0.6801), tensor(0.2068), tensor(0.1131)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.81278539 0.56747405 0.44692737]\n",
      "- f1 (average): 0.6090622702908121\n",
      "- accuracy: 0.711309552192688\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb5d17f4d67b4c83818a3858111ddcc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0726091861724854\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.9895793199539185\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9734922322359952 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 58!\n",
      "Accuracy on dataset of size 672: 70.08928680419922 %.\n",
      "Average loss: 0.7831103612076152\n",
      "proportion of labels in prediction: [tensor(0.7158), tensor(0.2188), tensor(0.0655)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.81555556 0.51851852 0.36734694]\n",
      "- f1 (average): 0.567140337616528\n",
      "- accuracy: 0.7008928656578064\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a4b036e744b442dac170b43b660f8a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1058921813964844\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0393548011779785\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0376879410310225 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 38!\n",
      "Accuracy on dataset of size 672: 66.96428680419922 %.\n",
      "Average loss: 0.8052765293554827\n",
      "proportion of labels in prediction: [tensor(0.7083), tensor(0.2917), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.82011173 0.47976879 0.        ]\n",
      "- f1 (average): 0.4332935059902477\n",
      "- accuracy: 0.6696428656578064\n"
     ]
    }
   ],
   "source": [
    "path_history = obtain_mean_history(sbert_384_embeddings, path_specifics)\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=path_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a58770aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Adding 'time_encoding' and feature...\n",
      "[INFO] Adding 'time_diff' and feature...\n",
      "[INFO] Adding 'timeline_index' feature...\n",
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a7cca8af2d641fbb3d1ff6a11ea8862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** hidden_dim: [100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e5070f335314ab68317754818dea1d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0689338445663452\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.029910683631897\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0149357481436296 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 87!\n",
      "Accuracy on dataset of size 672: 62.94643020629883 %.\n",
      "Average loss: 0.8852156509052623\n",
      "proportion of labels in prediction: [tensor(0.8497), tensor(0.1235), tensor(0.0268)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.75959596 0.29184549 0.21487603]\n",
      "- f1 (average): 0.4221058287386808\n",
      "- accuracy: 0.6294642686843872\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d132c23a6b9b484fb3dc03efc333e77a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.114090919494629\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0463939905166626\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0466171394694934 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 55!\n",
      "Accuracy on dataset of size 672: 63.244049072265625 %.\n",
      "Average loss: 0.896749659018083\n",
      "proportion of labels in prediction: [tensor(0.8899), tensor(0.0804), tensor(0.0298)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.77089479 0.18627451 0.22764228]\n",
      "- f1 (average): 0.3949371916068632\n",
      "- accuracy: 0.632440447807312\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8e7316df3264854bf3901ca91cb5552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0598030090332031\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.002704381942749\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0059379068287937 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 54!\n",
      "Accuracy on dataset of size 672: 62.35118865966797 %.\n",
      "Average loss: 0.9146971485831521\n",
      "proportion of labels in prediction: [tensor(0.8854), tensor(0.0774), tensor(0.0372)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.77317554 0.13861386 0.203125  ]\n",
      "- f1 (average): 0.37163813459748346\n",
      "- accuracy: 0.6235119104385376\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b39b0ea48eb54b8ea315cb1705a82830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0749456882476807\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.9822454452514648\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0100843906402588 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 38!\n",
      "Accuracy on dataset of size 672: 62.35118865966797 %.\n",
      "Average loss: 0.9153450619090687\n",
      "proportion of labels in prediction: [tensor(0.8854), tensor(0.1146), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.78106509 0.20264317 0.        ]\n",
      "- f1 (average): 0.327902753521188\n",
      "- accuracy: 0.6235119104385376\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a89e7140b2164ef38f74d7b314926a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.132216215133667\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0948266983032227\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0533526160500266 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 62!\n",
      "Accuracy on dataset of size 672: 63.988094329833984 %.\n",
      "Average loss: 0.9347266392274336\n",
      "proportion of labels in prediction: [tensor(0.8854), tensor(0.0060), tensor(0.1086)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.77712032 0.02597403 0.38636364]\n",
      "- f1 (average): 0.39648599263983875\n",
      "- accuracy: 0.6398809552192688\n"
     ]
    }
   ],
   "source": [
    "path_history = obtain_mean_history(sbert_384_embeddings, path_specifics, concatenate_current=False)\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=path_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2df8ec",
   "metadata": {},
   "source": [
    "## Pretrained BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc18ca48",
   "metadata": {},
   "source": [
    "### Mean pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4f6153fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Adding 'time_encoding' and feature...\n",
      "[INFO] Adding 'time_diff' and feature...\n",
      "[INFO] Adding 'timeline_index' feature...\n",
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8744389dbed49cda1f058354a09dcbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** hidden_dim: [100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f28517728154c298ce94914459a2396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1021214723587036\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.8234513998031616\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.8453716310587797 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 30!\n",
      "Accuracy on dataset of size 672: 70.83333587646484 %.\n",
      "Average loss: 0.7448050704869357\n",
      "proportion of labels in prediction: [tensor(0.6830), tensor(0.2664), tensor(0.0506)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.81548975 0.55927052 0.37956204]\n",
      "- f1 (average): 0.5847741033144898\n",
      "- accuracy: 0.7083333134651184\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76d012f335c3470cb432e11ff6e048d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0823612213134766\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.7924083471298218\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.8414776433597911 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 46!\n",
      "Accuracy on dataset of size 672: 70.23809814453125 %.\n",
      "Average loss: 0.7604863047599792\n",
      "proportion of labels in prediction: [tensor(0.6801), tensor(0.2336), tensor(0.0863)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.80593607 0.54071661 0.44720497]\n",
      "- f1 (average): 0.5979525514604368\n",
      "- accuracy: 0.7023809552192688\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ba21293f8c3490f950eadbd102fed00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0727800130844116\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.8618066906929016\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.8695350614461032 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 36!\n",
      "Accuracy on dataset of size 672: 71.57737731933594 %.\n",
      "Average loss: 0.7291001677513123\n",
      "proportion of labels in prediction: [tensor(0.7024), tensor(0.2039), tensor(0.0938)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.81705948 0.54355401 0.46987952]\n",
      "- f1 (average): 0.6101643362556937\n",
      "- accuracy: 0.7157738208770752\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236db7c2919c47c5b85b5abdae808913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0944077968597412\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.9358684420585632\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.8738451112400402 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 49!\n",
      "Accuracy on dataset of size 672: 71.2797622680664 %.\n",
      "Average loss: 0.7574839646166022\n",
      "proportion of labels in prediction: [tensor(0.7068), tensor(0.2039), tensor(0.0893)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.81655481 0.52961672 0.46625767]\n",
      "- f1 (average): 0.6041430677645776\n",
      "- accuracy: 0.7127976417541504\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e78169a382604fce9e47cc774312fe05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0790690183639526\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0292452573776245\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.893870928070762 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 48!\n",
      "Accuracy on dataset of size 672: 72.32142639160156 %.\n",
      "Average loss: 0.8019252961332147\n",
      "proportion of labels in prediction: [tensor(0.6920), tensor(0.2068), tensor(0.1012)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.82352941 0.55363322 0.49122807]\n",
      "- f1 (average): 0.6227968999777415\n",
      "- accuracy: 0.7232142686843872\n"
     ]
    }
   ],
   "source": [
    "path_history = obtain_mean_history(pooled_mean_pretrained, path_specifics)\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=path_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "974f4758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Adding 'time_encoding' and feature...\n",
      "[INFO] Adding 'time_diff' and feature...\n",
      "[INFO] Adding 'timeline_index' feature...\n",
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b09aa4cc19714d2e9828b99ebba2b02d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** hidden_dim: [100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29f50a1b7b1948cfaf03d2f9adf64950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0827751159667969\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.9896537661552429\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9003090154040944 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 43!\n",
      "Accuracy on dataset of size 672: 64.28571319580078 %.\n",
      "Average loss: 0.8660504817962646\n",
      "proportion of labels in prediction: [tensor(0.9241), tensor(0.0655), tensor(0.0104)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.77307692 0.2371134  0.12727273]\n",
      "- f1 (average): 0.3791543508038353\n",
      "- accuracy: 0.6428571343421936\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b176eae085a749c2b8f013d8c7ac1156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1143455505371094\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.9700237512588501\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.8952573537826538 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 62!\n",
      "Accuracy on dataset of size 672: 63.69047546386719 %.\n",
      "Average loss: 0.8783302090384744\n",
      "proportion of labels in prediction: [tensor(0.8512), tensor(0.1310), tensor(0.0179)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.76286579 0.33613445 0.17391304]\n",
      "- f1 (average): 0.42430442979631194\n",
      "- accuracy: 0.636904776096344\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c336479ac4471b9c8fb22209832c80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0997600555419922\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.9804369211196899\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9060063145377419 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 63!\n",
      "Accuracy on dataset of size 672: 65.0297622680664 %.\n",
      "Average loss: 0.9121526696465232\n",
      "proportion of labels in prediction: [tensor(0.7619), tensor(0.1830), tensor(0.0551)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.76476907 0.42490842 0.32857143]\n",
      "- f1 (average): 0.5060829730002662\n",
      "- accuracy: 0.6502976417541504\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6965e0e5a4d4e5d9ef65fa84e8a456e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0729719400405884\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.8793776631355286\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9026380343870684 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 42!\n",
      "Accuracy on dataset of size 672: 63.69047546386719 %.\n",
      "Average loss: 0.9111166000366211\n",
      "proportion of labels in prediction: [tensor(0.8943), tensor(0.0491), tensor(0.0565)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.76862745 0.12021858 0.35460993]\n",
      "- f1 (average): 0.41448531976445974\n",
      "- accuracy: 0.636904776096344\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f615db7d20574cc58a3c422a0f23c862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1056631803512573\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0125389099121094\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9033064083619551 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 74!\n",
      "Accuracy on dataset of size 672: 61.60714340209961 %.\n",
      "Average loss: 0.9209303693337874\n",
      "proportion of labels in prediction: [tensor(0.8765), tensor(0.0551), tensor(0.0685)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.75992063 0.11764706 0.26845638]\n",
      "- f1 (average): 0.3820080231943635\n",
      "- accuracy: 0.6160714030265808\n"
     ]
    }
   ],
   "source": [
    "path_history = obtain_mean_history(pooled_mean_pretrained, path_specifics, concatenate_current=False)\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=path_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62853774",
   "metadata": {},
   "source": [
    "### Max pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bf131ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Adding 'time_encoding' and feature...\n",
      "[INFO] Adding 'time_diff' and feature...\n",
      "[INFO] Adding 'timeline_index' feature...\n",
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "677b3937531a467196ec7ba43743e197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** hidden_dim: [100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "884dece6b08244059b1c7dba4c3b7ea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1084880828857422\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.7761728763580322\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.8622845031998374 || Accuracy: 0.6255571842193604 || F1-score: 0.27174123669475575\n",
      "Early stopping at epoch 65!\n",
      "Accuracy on dataset of size 672: 70.98213958740234 %.\n",
      "Average loss: 0.7004730430516329\n",
      "proportion of labels in prediction: [tensor(0.7039), tensor(0.2143), tensor(0.0818)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.80493274 0.55102041 0.46835443]\n",
      "- f1 (average): 0.6081025246563404\n",
      "- accuracy: 0.7098214030265808\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fda1af534cec449997496b98e400fe6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0752454996109009\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.8784411549568176\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.8661506284366954 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 53!\n",
      "Accuracy on dataset of size 672: 69.94047546386719 %.\n",
      "Average loss: 0.7430935881354592\n",
      "proportion of labels in prediction: [tensor(0.6473), tensor(0.2798), tensor(0.0729)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.79625293 0.56213018 0.46052632]\n",
      "- f1 (average): 0.6063031402349117\n",
      "- accuracy: 0.699404776096344\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46833737fa5345f7bf95d46610006d0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0746275186538696\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.8988773822784424\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.8991819565946405 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 42!\n",
      "Accuracy on dataset of size 672: 68.75 %.\n",
      "Average loss: 0.7781992012804205\n",
      "proportion of labels in prediction: [tensor(0.6696), tensor(0.2887), tensor(0.0417)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.8009206  0.54651163 0.30534351]\n",
      "- f1 (average): 0.5509252459154371\n",
      "- accuracy: 0.6875\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e43be07b74184387b356868b9bdeebee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0943225622177124\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0057708024978638\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9081212932413275 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 69!\n",
      "Accuracy on dataset of size 672: 71.2797622680664 %.\n",
      "Average loss: 0.7408329302614386\n",
      "proportion of labels in prediction: [tensor(0.6801), tensor(0.2068), tensor(0.1131)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.80593607 0.55363322 0.51396648]\n",
      "- f1 (average): 0.6245119238331226\n",
      "- accuracy: 0.7127976417541504\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "728e6366a6d94a5c851b9114faab1cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0788952112197876\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0040643215179443\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9158212109045549 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 77!\n",
      "Accuracy on dataset of size 672: 71.875 %.\n",
      "Average loss: 0.7152866341850974\n",
      "proportion of labels in prediction: [tensor(0.6964), tensor(0.2039), tensor(0.0997)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.80947012 0.57142857 0.49411765]\n",
      "- f1 (average): 0.6250054475003078\n",
      "- accuracy: 0.71875\n"
     ]
    }
   ],
   "source": [
    "path_history = obtain_mean_history(pooled_max_pretrained, path_specifics)\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=path_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c3073dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Adding 'time_encoding' and feature...\n",
      "[INFO] Adding 'time_diff' and feature...\n",
      "[INFO] Adding 'timeline_index' feature...\n",
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "856928ebf75f49a09780fbce6a32e1b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** hidden_dim: [100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "797b7e58478046619ad4708b66bb5459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0947773456573486\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0492063760757446\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9014952128583734 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 43!\n",
      "Accuracy on dataset of size 672: 61.60714340209961 %.\n",
      "Average loss: 0.905403272672133\n",
      "proportion of labels in prediction: [tensor(0.9866), tensor(0.0134), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.76155268 0.02515723 0.        ]\n",
      "- f1 (average): 0.26223663764207134\n",
      "- accuracy: 0.6160714030265808\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47ea130c4e4349e698d85dced3a95de6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1085178852081299\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.9874736070632935\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.8997597694396973 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 55!\n",
      "Accuracy on dataset of size 672: 61.904762268066406 %.\n",
      "Average loss: 0.9084995226426558\n",
      "proportion of labels in prediction: [tensor(0.9464), tensor(0.0491), tensor(0.0045)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.76208531 0.12021858 0.05660377]\n",
      "- f1 (average): 0.3129692202922501\n",
      "- accuracy: 0.6190476417541504\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86c144e36ad5457fb431d277948b3fa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.099276065826416\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.9842522144317627\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9089005426927046 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 56!\n",
      "Accuracy on dataset of size 672: 62.05356979370117 %.\n",
      "Average loss: 0.9172859354452654\n",
      "proportion of labels in prediction: [tensor(0.9107), tensor(0.0729), tensor(0.0164)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.76042677 0.17085427 0.14035088]\n",
      "- f1 (average): 0.3572106395586192\n",
      "- accuracy: 0.6205357313156128\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba88bcea4ce4a48898832ddd9f9af7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0751796960830688\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.8894482254981995\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9061743021011353 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 94!\n",
      "Accuracy on dataset of size 672: 62.5 %.\n",
      "Average loss: 0.9255607344887473\n",
      "proportion of labels in prediction: [tensor(0.9643), tensor(0.0015), tensor(0.0342)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.76663543 0.         0.17460317]\n",
      "- f1 (average): 0.31374620034413847\n",
      "- accuracy: 0.625\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "368641cb096e44248a89fef2da7a5221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1056452989578247\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0734694004058838\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9058432199738242 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 68!\n",
      "Accuracy on dataset of size 672: 61.755950927734375 %.\n",
      "Average loss: 0.9206284392963756\n",
      "proportion of labels in prediction: [tensor(0.9643), tensor(0.0357), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.76663543 0.06896552 0.        ]\n",
      "- f1 (average): 0.27853364789020674\n",
      "- accuracy: 0.617559552192688\n"
     ]
    }
   ],
   "source": [
    "path_history = obtain_mean_history(pooled_max_pretrained, path_specifics, concatenate_current=False)\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=path_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8a099e",
   "metadata": {},
   "source": [
    "### Sum pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2494f772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Adding 'time_encoding' and feature...\n",
      "[INFO] Adding 'time_diff' and feature...\n",
      "[INFO] Adding 'timeline_index' feature...\n",
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dcd76b7281145aebc9b08d115c7779f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** hidden_dim: [100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1fbe07baa8d4354a1b9823a01794e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.727465033531189\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.6842592358589172\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.8152492263100364 || Accuracy: 0.6508172154426575 || F1-score: 0.45380069492888103\n",
      "Early stopping at epoch 43!\n",
      "Accuracy on dataset of size 672: 71.875 %.\n",
      "Average loss: 0.8815002387220209\n",
      "proportion of labels in prediction: [tensor(0.7054), tensor(0.2128), tensor(0.0818)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.81522956 0.5665529  0.4556962 ]\n",
      "- f1 (average): 0.6124928889418044\n",
      "- accuracy: 0.71875\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "313183804909425eb98b6e1f4565c36a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1402323246002197\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.7112229466438293\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.8173842213370583 || Accuracy: 0.6448736786842346 || F1-score: 0.4025425170419203\n",
      "Early stopping at epoch 39!\n",
      "Accuracy on dataset of size 672: 71.2797622680664 %.\n",
      "Average loss: 0.9226987957954407\n",
      "proportion of labels in prediction: [tensor(0.6920), tensor(0.2143), tensor(0.0938)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.80542986 0.55782313 0.4939759 ]\n",
      "- f1 (average): 0.6190762990398507\n",
      "- accuracy: 0.7127976417541504\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bd71a825c6f40e8ab56770430b4cdb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0828022956848145\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.8738025426864624\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.8608328808437694 || Accuracy: 0.6344724893569946 || F1-score: 0.3448035645816642\n",
      "Early stopping at epoch 35!\n",
      "Accuracy on dataset of size 672: 71.2797622680664 %.\n",
      "Average loss: 0.84981154311787\n",
      "proportion of labels in prediction: [tensor(0.6860), tensor(0.2247), tensor(0.0893)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.80681818 0.57142857 0.46625767]\n",
      "- f1 (average): 0.6148348073194699\n",
      "- accuracy: 0.7127976417541504\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26b34d988ea46afbd5cba34a895763e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.094190001487732\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.9194858074188232\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.8628855564377524 || Accuracy: 0.6181277632713318 || F1-score: 0.25466789103152737\n",
      "Early stopping at epoch 27!\n",
      "Accuracy on dataset of size 672: 72.02381134033203 %.\n",
      "Average loss: 0.8458069942214272\n",
      "proportion of labels in prediction: [tensor(0.6533), tensor(0.2485), tensor(0.0982)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.81118881 0.5615142  0.55621302]\n",
      "- f1 (average): 0.642972008174629\n",
      "- accuracy: 0.7202380895614624\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "816ade21401f48aa93efb3649d41111f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0835667848587036\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.9686885476112366\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9038537090474908 || Accuracy: 0.6196136474609375 || F1-score: 0.2803511448714408\n",
      "Early stopping at epoch 33!\n",
      "Accuracy on dataset of size 672: 72.32142639160156 %.\n",
      "Average loss: 0.8853492032397877\n",
      "proportion of labels in prediction: [tensor(0.6786), tensor(0.2247), tensor(0.0967)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.816      0.55813953 0.53571429]\n",
      "- f1 (average): 0.6366179401993355\n",
      "- accuracy: 0.7232142686843872\n"
     ]
    }
   ],
   "source": [
    "path_history = obtain_mean_history(pooled_sum_pretrained, path_specifics)\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=path_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "05f813cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Adding 'time_encoding' and feature...\n",
      "[INFO] Adding 'time_diff' and feature...\n",
      "[INFO] Adding 'timeline_index' feature...\n",
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1524ff63bd1448c4892e8a13d078473c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** hidden_dim: [100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f32d2da23194899a4d55b56c1f6d8d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.5255906581878662\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.133524775505066\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.8961977579376914 || Accuracy: 0.6210995316505432 || F1-score: 0.25951624835194914\n",
      "Early stopping at epoch 59!\n",
      "Accuracy on dataset of size 672: 66.51786041259766 %.\n",
      "Average loss: 0.9034843119707975\n",
      "proportion of labels in prediction: [tensor(0.7589), tensor(0.1920), tensor(0.0491)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.76856835 0.45878136 0.38235294]\n",
      "- f1 (average): 0.5365675520838179\n",
      "- accuracy: 0.6651785969734192\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e6c2c7f1d064e2b9955d70cc39da85d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0804401636123657\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0644162893295288\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9117570194331083 || Accuracy: 0.6210995316505432 || F1-score: 0.25542315918117936\n",
      "Early stopping at epoch 51!\n",
      "Accuracy on dataset of size 672: 66.2202377319336 %.\n",
      "Average loss: 0.8689939108761874\n",
      "proportion of labels in prediction: [tensor(0.8006), tensor(0.1503), tensor(0.0491)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.76907001 0.43027888 0.33823529]\n",
      "- f1 (average): 0.5125280630097064\n",
      "- accuracy: 0.6622023582458496\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a32ca892e3d947029bf3c0f19c77969b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0874210596084595\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.9809868931770325\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9239951751448892 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 41!\n",
      "Accuracy on dataset of size 672: 65.17857360839844 %.\n",
      "Average loss: 0.9270960688591003\n",
      "proportion of labels in prediction: [tensor(0.7396), tensor(0.1875), tensor(0.0729)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.75982533 0.46376812 0.34210526]\n",
      "- f1 (average): 0.5218995688702802\n",
      "- accuracy: 0.6517857313156128\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cc1a143225942ce95a4edfc2a19d845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0957541465759277\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0000782012939453\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.941019968553023 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 52!\n",
      "Accuracy on dataset of size 672: 66.36904907226562 %.\n",
      "Average loss: 0.9391943487254056\n",
      "proportion of labels in prediction: [tensor(0.7723), tensor(0.1637), tensor(0.0640)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.77185501 0.41538462 0.4109589 ]\n",
      "- f1 (average): 0.5327328433850617\n",
      "- accuracy: 0.663690447807312\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf3bccbc22334067b0962e3e1d4f5ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1101337671279907\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0370057821273804\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9489994157444347 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 48!\n",
      "Accuracy on dataset of size 672: 64.58333587646484 %.\n",
      "Average loss: 0.9552247036587108\n",
      "proportion of labels in prediction: [tensor(0.7411), tensor(0.1890), tensor(0.0699)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.7568157  0.44043321 0.34666667]\n",
      "- f1 (average): 0.5146385276812152\n",
      "- accuracy: 0.6458333134651184\n"
     ]
    }
   ],
   "source": [
    "path_history = obtain_mean_history(pooled_sum_pretrained, path_specifics, concatenate_current=False)\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=path_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1410b12",
   "metadata": {},
   "source": [
    "### CLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2446b4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Adding 'time_encoding' and feature...\n",
      "[INFO] Adding 'time_diff' and feature...\n",
      "[INFO] Adding 'timeline_index' feature...\n",
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "741c3d576fdd43ae875a6e47673c19da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** hidden_dim: [100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6270bf341ae3474faeea702e4fab302d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.099766492843628\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.7809126377105713\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.8579818931492892 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 61!\n",
      "Accuracy on dataset of size 672: 72.32142639160156 %.\n",
      "Average loss: 0.6975206190889532\n",
      "proportion of labels in prediction: [tensor(0.6979), tensor(0.2128), tensor(0.0893)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.82207207 0.55972696 0.47852761]\n",
      "- f1 (average): 0.6201088806304578\n",
      "- accuracy: 0.7232142686843872\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5f1fb8cbed248509a619250c55f082b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0808454751968384\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.8400460481643677\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.8549019748514349 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 40!\n",
      "Accuracy on dataset of size 672: 72.61904907226562 %.\n",
      "Average loss: 0.6978309317068621\n",
      "proportion of labels in prediction: [tensor(0.7485), tensor(0.1533), tensor(0.0982)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.82429501 0.52173913 0.49704142]\n",
      "- f1 (average): 0.614358520466371\n",
      "- accuracy: 0.726190447807312\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb2857175e4f4c72b437000628e49d8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0731648206710815\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.8906587362289429\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.8822337497364391 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 56!\n",
      "Accuracy on dataset of size 672: 72.61904907226562 %.\n",
      "Average loss: 0.6981977766210382\n",
      "proportion of labels in prediction: [tensor(0.6979), tensor(0.2068), tensor(0.0952)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.81756757 0.56747405 0.51497006]\n",
      "- f1 (average): 0.6333372252969044\n",
      "- accuracy: 0.726190447807312\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6aa398361094536aabb19639b00d83e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0950363874435425\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.977047324180603\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.8841557882048867 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 49!\n",
      "Accuracy on dataset of size 672: 71.875 %.\n",
      "Average loss: 0.7547444159334357\n",
      "proportion of labels in prediction: [tensor(0.6801), tensor(0.2158), tensor(0.1042)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.81278539 0.53559322 0.55491329]\n",
      "- f1 (average): 0.6344306344215082\n",
      "- accuracy: 0.71875\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf5eba2b1cb64ec5929ebda51195b430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0791399478912354\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0230438709259033\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.8975516286763278 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 49!\n",
      "Accuracy on dataset of size 672: 73.66071319580078 %.\n",
      "Average loss: 0.7267429070039229\n",
      "proportion of labels in prediction: [tensor(0.6905), tensor(0.2113), tensor(0.0982)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.82899207 0.57534247 0.53254438]\n",
      "- f1 (average): 0.6456263056439435\n",
      "- accuracy: 0.7366071343421936\n"
     ]
    }
   ],
   "source": [
    "path_history = obtain_mean_history(pooled_cls_pretrained, path_specifics)\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=path_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2608869b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Adding 'time_encoding' and feature...\n",
      "[INFO] Adding 'time_diff' and feature...\n",
      "[INFO] Adding 'timeline_index' feature...\n",
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dba6286991c47b79cd9ef69481a068d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** hidden_dim: [100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d4a35a936034ee2864c585172f4189d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0739248991012573\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.9890086650848389\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9000857418233698 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 28!\n",
      "Accuracy on dataset of size 672: 62.2023811340332 %.\n",
      "Average loss: 0.9152315746654164\n",
      "proportion of labels in prediction: [tensor(0.9955), tensor(0.0045), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.76654412 0.0130719  0.        ]\n",
      "- f1 (average): 0.25987200435729846\n",
      "- accuracy: 0.6220238208770752\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bafa1887c114afcbb08275e08a7f671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.116005539894104\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.9669814705848694\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.8972187475724653 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 70!\n",
      "Accuracy on dataset of size 672: 62.64881134033203 %.\n",
      "Average loss: 0.8904514421116222\n",
      "proportion of labels in prediction: [tensor(0.8705), tensor(0.1057), tensor(0.0238)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.76095618 0.26244344 0.16806723]\n",
      "- f1 (average): 0.3971556137011962\n",
      "- accuracy: 0.6264880895614624\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6437f455f754335bdcc29727efaae3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.100395917892456\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.9996486306190491\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9076314135031267 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 56!\n",
      "Accuracy on dataset of size 672: 62.64881134033203 %.\n",
      "Average loss: 0.9038712111386386\n",
      "proportion of labels in prediction: [tensor(0.7768), tensor(0.1830), tensor(0.0402)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.75451647 0.35164835 0.27692308]\n",
      "- f1 (average): 0.4610293001366328\n",
      "- accuracy: 0.6264880895614624\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46fe0fad044e40a7b43380518c6fbe04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0736373662948608\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.8722844123840332\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9062130667946555 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 72!\n",
      "Accuracy on dataset of size 672: 63.244049072265625 %.\n",
      "Average loss: 0.900691958990964\n",
      "proportion of labels in prediction: [tensor(0.8289), tensor(0.1176), tensor(0.0536)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.76434426 0.26200873 0.31654676]\n",
      "- f1 (average): 0.44763325283648814\n",
      "- accuracy: 0.632440447807312\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d14bd7c638174fc5a81bbfcd89f4a2c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1049774885177612\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0856366157531738\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9055847525596619 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 59!\n",
      "Accuracy on dataset of size 672: 61.60714340209961 %.\n",
      "Average loss: 0.8962898362766613\n",
      "proportion of labels in prediction: [tensor(0.8988), tensor(0.0506), tensor(0.0506)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.76441838 0.06521739 0.24817518]\n",
      "- f1 (average): 0.3592703170359009\n",
      "- accuracy: 0.6160714030265808\n"
     ]
    }
   ],
   "source": [
    "path_history = obtain_mean_history(pooled_cls_pretrained, path_specifics, concatenate_current=False)\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=path_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51207e2c",
   "metadata": {},
   "source": [
    "## Fine-tuned BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244ea57d",
   "metadata": {},
   "source": [
    "### Mean pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "1a6fbdaf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pooled_mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[175], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m path_history \u001b[38;5;241m=\u001b[39m obtain_mean_history(\u001b[43mpooled_mean\u001b[49m, path_specifics)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hidden_dim \u001b[38;5;129;01min\u001b[39;00m hidden_dim_trials:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m********** hidden_dim: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhidden_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pooled_mean' is not defined"
     ]
    }
   ],
   "source": [
    "path_history = obtain_mean_history(pooled_mean, path_specifics)\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=path_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccdeca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_history = obtain_mean_history(pooled_mean, path_specifics, concatenate_current=False)\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=path_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143a37e8",
   "metadata": {},
   "source": [
    "### Max pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c21d4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_history = obtain_mean_history(pooled_max, path_specifics)\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=path_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebfc8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_history = obtain_mean_history(pooled_max, path_specifics, concatenate_current=False)\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=path_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70b5e8e",
   "metadata": {},
   "source": [
    "### Sum pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dba00c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_history = obtain_mean_history(pooled_sum, path_specifics, concatenate_current=False)\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=path_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3426e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_history = obtain_mean_history(pooled_sum, path_specifics)\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=path_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9578c834",
   "metadata": {},
   "source": [
    "### CLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f838d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_history = obtain_mean_history(pooled_cls, path_specifics)\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=path_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb56f7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_history = obtain_mean_history(pooled_cls, path_specifics, concatenate_current=False)\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=path_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe18b1c",
   "metadata": {},
   "source": [
    "# Baseline 3: LSTM classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00ae6f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1689cfcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92103aa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7a79d90",
   "metadata": {},
   "source": [
    "# Baseline 4: FFN using signatures\n",
    "\n",
    "First, we dimension reduce these and then take signatures. We use the path signature as input to the FFN for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e61ad20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_specifics = {\"pad_by\": \"history\",\n",
    "                  \"zero_padding\": False,\n",
    "                  \"method\": \"k_last\",\n",
    "                  \"k\": 5,\n",
    "                  \"time_feature\": None,\n",
    "                  \"embeddings\": \"dim_reduced\",\n",
    "                  \"include_current_embedding\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a79dbfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_signatures_history(embeddings, path_specifics, dimension, sig_depth, concatenate_current=True):\n",
    "    # dimension reduce\n",
    "    reduction = nlpsig.DimReduce(method=\"gaussian_random_projection\", n_components=dimension)\n",
    "    # reduction = nlpsig.DimReduce(method=\"umap\", n_components=dimension)\n",
    "    embeddings_reduced = reduction.fit_transform(embeddings, random_state=seed)\n",
    "    \n",
    "    paths = nlpsig.PrepareData(anno_mi,\n",
    "                               id_column=\"transcript_id\",\n",
    "                               label_column=\"client_talk_type\",\n",
    "                               embeddings=embeddings,\n",
    "                               embeddings_reduced=embeddings_reduced)\n",
    "    path = paths.pad(**path_specifics)\n",
    "    # remove last two columns (which contains the id and the label)\n",
    "    path = path[client_index][:,:,:-2].astype(\"float\")\n",
    "    \n",
    "    # convert to torch tensor to compute signature using signatory\n",
    "    path = torch.from_numpy(path).float()\n",
    "    sig = signatory.signature(path, sig_depth).float()\n",
    "    \n",
    "    # concatenate with current embedding\n",
    "    if concatenate_current:\n",
    "        sig = torch.cat([sig, torch.tensor(embeddings[client_index])], dim=1)\n",
    "\n",
    "    return sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d6d39901",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = 5\n",
    "sig_depth = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392cbcfb",
   "metadata": {},
   "source": [
    "## SBERT 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "32d3917a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] - columns beginning with 'd' denote the dimension reduced embeddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Adding 'time_encoding' and feature...\n",
      "[INFO] Adding 'time_diff' and feature...\n",
      "[INFO] Adding 'timeline_index' feature...\n",
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f62ef344cfb4050b68cd84bbd43491c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signature_history.shape = torch.Size([6725, 1548])\n",
      "\n",
      "********** hidden_dim: [100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "850cada32542450d939816e1fec8964d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1171363592147827\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.053240180015564\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0347932902249424 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 40!\n",
      "Accuracy on dataset of size 672: 67.11309814453125 %.\n",
      "Average loss: 0.7697976990179582\n",
      "proportion of labels in prediction: [tensor(0.6920), tensor(0.2143), tensor(0.0938)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.78054299 0.50340136 0.38554217]\n",
      "- f1 (average): 0.5564955052147519\n",
      "- accuracy: 0.6711309552192688\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56c10343763f4b6f8f44c01662092e1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1026197671890259\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0218136310577393\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9916267178275369 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 36!\n",
      "Accuracy on dataset of size 672: 65.47618865966797 %.\n",
      "Average loss: 0.8570250272750854\n",
      "proportion of labels in prediction: [tensor(0.6786), tensor(0.2143), tensor(0.1071)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.768      0.4829932  0.37714286]\n",
      "- f1 (average): 0.5427120181405897\n",
      "- accuracy: 0.6547619104385376\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2194297774a43a682b5df19c17f0529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0876795053482056\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0153101682662964\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9809776111082598 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 34!\n",
      "Accuracy on dataset of size 672: 64.58333587646484 %.\n",
      "Average loss: 0.9729593179442666\n",
      "proportion of labels in prediction: [tensor(0.6622), tensor(0.2247), tensor(0.1131)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.76388889 0.47840532 0.3575419 ]\n",
      "- f1 (average): 0.5332787013149493\n",
      "- accuracy: 0.6458333134651184\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d39680945742988fc9d41d7edb6f2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0767967700958252\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.02414870262146\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9755970781499689 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 26!\n",
      "Accuracy on dataset of size 672: 62.05356979370117 %.\n",
      "Average loss: 0.9041527916084636\n",
      "proportion of labels in prediction: [tensor(0.6443), tensor(0.3557), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.77230047 0.45244216 0.        ]\n",
      "- f1 (average): 0.4082475429555337\n",
      "- accuracy: 0.6205357313156128\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9ac9927e38f409e8d762e4ce8326672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0699200630187988\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.9752826690673828\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9633186188611117 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 25!\n",
      "Accuracy on dataset of size 672: 61.904762268066406 %.\n",
      "Average loss: 0.9764535427093506\n",
      "proportion of labels in prediction: [tensor(0.6429), tensor(0.3571), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.76850764 0.45641026 0.        ]\n",
      "- f1 (average): 0.4083059648277039\n",
      "- accuracy: 0.6190476417541504\n"
     ]
    }
   ],
   "source": [
    "signature_history = obtain_signatures_history(sbert_768_embeddings, path_specifics, dimension, sig_depth)\n",
    "print(f\"signature_history.shape = {signature_history.shape}\")\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=signature_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a471c363",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] - columns beginning with 'd' denote the dimension reduced embeddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Adding 'time_encoding' and feature...\n",
      "[INFO] Adding 'time_diff' and feature...\n",
      "[INFO] Adding 'timeline_index' feature...\n",
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46b53685e08e49698e63199e0d998d8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signature_history.shape = torch.Size([6725, 780])\n",
      "\n",
      "********** hidden_dim: [100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a3ef3331350420284216e3c2c0d78bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1296181678771973\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.106845498085022\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0863242040980945 || Accuracy: 0.49034175276756287 || F1-score: 0.27059271026782716\n",
      "Early stopping at epoch 12!\n",
      "Accuracy on dataset of size 672: 62.35118865966797 %.\n",
      "Average loss: 0.9313619353554465\n",
      "proportion of labels in prediction: [tensor(1.), tensor(0.), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.76810266 0.         0.        ]\n",
      "- f1 (average): 0.25603421937060805\n",
      "- accuracy: 0.6235119104385376\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76d28f0402184f959749bff7b3077926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1389586925506592\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.082794189453125\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0830620310523293 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 26!\n",
      "Accuracy on dataset of size 672: 61.904762268066406 %.\n",
      "Average loss: 0.9399037523703142\n",
      "proportion of labels in prediction: [tensor(0.9881), tensor(0.0119), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.76638966 0.01265823 0.        ]\n",
      "- f1 (average): 0.25968262873483955\n",
      "- accuracy: 0.6190476417541504\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d8dca0b03f54e2ea87fe6c6d582e4b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0805878639221191\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0350637435913086\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0200353373180737 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 35!\n",
      "Accuracy on dataset of size 672: 62.2023811340332 %.\n",
      "Average loss: 0.9935095581141385\n",
      "proportion of labels in prediction: [tensor(0.9821), tensor(0.0179), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.7673772  0.04938272 0.        ]\n",
      "- f1 (average): 0.2722533057205079\n",
      "- accuracy: 0.6220238208770752\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e8bbb2ff594450dbf48a985a2ea5791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0874937772750854\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0431901216506958\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0160185857252642 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.8067488074302673\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.42302054166793823\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 1.12337866154584 || Accuracy: 0.570579469203949 || F1-score: 0.2948740694523326\n",
      "Early stopping at epoch 102!\n",
      "Accuracy on dataset of size 672: 57.58928680419922 %.\n",
      "Average loss: 1.1595185507427563\n",
      "proportion of labels in prediction: [tensor(0.8914), tensor(0.0729), tensor(0.0357)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.7367387  0.08040201 0.06299213]\n",
      "- f1 (average): 0.2933776131247951\n",
      "- accuracy: 0.5758928656578064\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "036e24d00acc47059fc1f4f513b0c294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0989593267440796\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0831507444381714\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0347781506451694 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.8126439452171326\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.9172384142875671\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 1.0070092190395703 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 201/10000 || Item: 0/85 || Loss: 0.6922479271888733\n",
      "--------------------------------------------------\n",
      "##### Epoch: 201/10000 || Loss: 1.134027123451233\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 201 || Loss: 1.0152975104071877 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 301/10000 || Item: 0/85 || Loss: 0.7323262095451355\n",
      "--------------------------------------------------\n",
      "##### Epoch: 301/10000 || Loss: 0.7771136164665222\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 301 || Loss: 1.013195888562636 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 401/10000 || Item: 0/85 || Loss: 0.7270156741142273\n",
      "--------------------------------------------------\n",
      "##### Epoch: 401/10000 || Loss: 1.0095038414001465\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 401 || Loss: 1.0276589014313438 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 501/10000 || Item: 0/85 || Loss: 0.9167084097862244\n",
      "--------------------------------------------------\n",
      "##### Epoch: 501/10000 || Loss: 1.297402262687683\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 501 || Loss: 1.0069988857616077 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 601/10000 || Item: 0/85 || Loss: 0.7659298777580261\n",
      "--------------------------------------------------\n",
      "##### Epoch: 601/10000 || Loss: 0.3361002504825592\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 601 || Loss: 1.0141081647439436 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 701/10000 || Item: 0/85 || Loss: 0.8066194653511047\n",
      "--------------------------------------------------\n",
      "##### Epoch: 701/10000 || Loss: 1.0158820152282715\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 701 || Loss: 1.0159841559150002 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 801/10000 || Item: 0/85 || Loss: 0.797673225402832\n",
      "--------------------------------------------------\n",
      "##### Epoch: 801/10000 || Loss: 1.0612021684646606\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 801 || Loss: 1.0060570240020752 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 901/10000 || Item: 0/85 || Loss: 0.8667025566101074\n",
      "--------------------------------------------------\n",
      "##### Epoch: 901/10000 || Loss: 0.9432721138000488\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 901 || Loss: 1.0279247056354175 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 1001/10000 || Item: 0/85 || Loss: 0.8365276455879211\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1001/10000 || Loss: 0.6479430198669434\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1001 || Loss: 1.0184671066024087 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 1101/10000 || Item: 0/85 || Loss: 0.7767313122749329\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1101/10000 || Loss: 0.8214415311813354\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1101 || Loss: 1.0206751444123008 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 1201/10000 || Item: 0/85 || Loss: 0.6046144366264343\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1201/10000 || Loss: 0.8326704502105713\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1201 || Loss: 1.0222699696367437 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 1301/10000 || Item: 0/85 || Loss: 0.7988095283508301\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1301/10000 || Loss: 0.24917344748973846\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1301 || Loss: 1.0218127478252759 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 1401/10000 || Item: 0/85 || Loss: 0.8395476341247559\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1401/10000 || Loss: 0.8746047019958496\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1401 || Loss: 1.0066633766347712 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 1501/10000 || Item: 0/85 || Loss: 0.750222384929657\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1501/10000 || Loss: 0.900796115398407\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1501 || Loss: 1.0242182937535373 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 1601/10000 || Item: 0/85 || Loss: 0.7142002582550049\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1601/10000 || Loss: 0.4533419609069824\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1601 || Loss: 1.017387872392481 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 1701/10000 || Item: 0/85 || Loss: 0.8615820407867432\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1701/10000 || Loss: 1.419440746307373\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1701 || Loss: 1.0135444023392417 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 1801/10000 || Item: 0/85 || Loss: 0.9391782879829407\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1801/10000 || Loss: 0.6792323589324951\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1801 || Loss: 1.019064729863947 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 1901/10000 || Item: 0/85 || Loss: 0.7760879993438721\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1901/10000 || Loss: 0.60826575756073\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1901 || Loss: 1.024169141596014 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 2001/10000 || Item: 0/85 || Loss: 0.5902829170227051\n",
      "--------------------------------------------------\n",
      "##### Epoch: 2001/10000 || Loss: 0.4679310619831085\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 2001 || Loss: 1.0257864431901411 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 2101/10000 || Item: 0/85 || Loss: 0.8040127754211426\n",
      "--------------------------------------------------\n",
      "##### Epoch: 2101/10000 || Loss: 0.3056190609931946\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 2101 || Loss: 1.0218444249846719 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 2201/10000 || Item: 0/85 || Loss: 0.7688440680503845\n",
      "--------------------------------------------------\n",
      "##### Epoch: 2201/10000 || Loss: 0.7016164064407349\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 2201 || Loss: 1.0271872607144443 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 2301/10000 || Item: 0/85 || Loss: 0.7774472832679749\n",
      "--------------------------------------------------\n",
      "##### Epoch: 2301/10000 || Loss: 0.7092254161834717\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 2301 || Loss: 1.0186377438631924 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 2401/10000 || Item: 0/85 || Loss: 0.9809486269950867\n",
      "--------------------------------------------------\n",
      "##### Epoch: 2401/10000 || Loss: 0.8127239942550659\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 2401 || Loss: 1.0157662955197422 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2501/10000 || Item: 0/85 || Loss: 0.8530833721160889\n",
      "--------------------------------------------------\n",
      "##### Epoch: 2501/10000 || Loss: 0.39883291721343994\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 2501 || Loss: 1.0210722034627742 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 2601/10000 || Item: 0/85 || Loss: 0.7698783874511719\n",
      "--------------------------------------------------\n",
      "##### Epoch: 2601/10000 || Loss: 0.5527031421661377\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 2601 || Loss: 1.0221493352543225 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 2701/10000 || Item: 0/85 || Loss: 0.8497977256774902\n",
      "--------------------------------------------------\n",
      "##### Epoch: 2701/10000 || Loss: 0.745418131351471\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 2701 || Loss: 1.045716865496202 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 2801/10000 || Item: 0/85 || Loss: 0.7570958137512207\n",
      "--------------------------------------------------\n",
      "##### Epoch: 2801/10000 || Loss: 0.6498628258705139\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 2801 || Loss: 1.0299142544919795 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 2901/10000 || Item: 0/85 || Loss: 0.8245310187339783\n",
      "--------------------------------------------------\n",
      "##### Epoch: 2901/10000 || Loss: 0.9797484874725342\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 2901 || Loss: 1.0169466896490618 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 3001/10000 || Item: 0/85 || Loss: 0.8575553894042969\n",
      "--------------------------------------------------\n",
      "##### Epoch: 3001/10000 || Loss: 0.8895618319511414\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 3001 || Loss: 1.0269086956977844 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 3101/10000 || Item: 0/85 || Loss: 0.7511343955993652\n",
      "--------------------------------------------------\n",
      "##### Epoch: 3101/10000 || Loss: 0.4367678761482239\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 3101 || Loss: 1.0141893787817522 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 3201/10000 || Item: 0/85 || Loss: 0.7798851728439331\n",
      "--------------------------------------------------\n",
      "##### Epoch: 3201/10000 || Loss: 0.9779647588729858\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 3201 || Loss: 1.0164816704663364 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 3301/10000 || Item: 0/85 || Loss: 0.696100115776062\n",
      "--------------------------------------------------\n",
      "##### Epoch: 3301/10000 || Loss: 0.6922577619552612\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 3301 || Loss: 1.0118458379398694 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 3401/10000 || Item: 0/85 || Loss: 0.8389911651611328\n",
      "--------------------------------------------------\n",
      "##### Epoch: 3401/10000 || Loss: 0.9951604604721069\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 3401 || Loss: 1.0277740359306335 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 3501/10000 || Item: 0/85 || Loss: 0.9304811954498291\n",
      "--------------------------------------------------\n",
      "##### Epoch: 3501/10000 || Loss: 0.651368260383606\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 3501 || Loss: 1.0246222561055964 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 3601/10000 || Item: 0/85 || Loss: 0.7899249196052551\n",
      "--------------------------------------------------\n",
      "##### Epoch: 3601/10000 || Loss: 0.2809697985649109\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 3601 || Loss: 1.0224294770847668 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 3701/10000 || Item: 0/85 || Loss: 0.7249509692192078\n",
      "--------------------------------------------------\n",
      "##### Epoch: 3701/10000 || Loss: 0.9442073106765747\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 3701 || Loss: 1.0162605047225952 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 3801/10000 || Item: 0/85 || Loss: 0.7148638367652893\n",
      "--------------------------------------------------\n",
      "##### Epoch: 3801/10000 || Loss: 0.5918748378753662\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 3801 || Loss: 1.0295160466974431 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 3901/10000 || Item: 0/85 || Loss: 0.8750899434089661\n",
      "--------------------------------------------------\n",
      "##### Epoch: 3901/10000 || Loss: 0.596534252166748\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 3901 || Loss: 1.0216285911473362 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 4001/10000 || Item: 0/85 || Loss: 0.7829965353012085\n",
      "--------------------------------------------------\n",
      "##### Epoch: 4001/10000 || Loss: 0.5300670266151428\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 4001 || Loss: 1.0347484187646345 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 4101/10000 || Item: 0/85 || Loss: 0.7412446737289429\n",
      "--------------------------------------------------\n",
      "##### Epoch: 4101/10000 || Loss: 1.0382254123687744\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 4101 || Loss: 1.0154892964796587 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 4201/10000 || Item: 0/85 || Loss: 0.7394151091575623\n",
      "--------------------------------------------------\n",
      "##### Epoch: 4201/10000 || Loss: 0.7746332883834839\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 4201 || Loss: 1.0151815035126426 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 4301/10000 || Item: 0/85 || Loss: 0.7318466901779175\n",
      "--------------------------------------------------\n",
      "##### Epoch: 4301/10000 || Loss: 1.3841590881347656\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 4301 || Loss: 1.023349637334997 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 4401/10000 || Item: 0/85 || Loss: 0.8690871000289917\n",
      "--------------------------------------------------\n",
      "##### Epoch: 4401/10000 || Loss: 1.043344497680664\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 4401 || Loss: 1.0242074673826045 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 4501/10000 || Item: 0/85 || Loss: 0.8045402765274048\n",
      "--------------------------------------------------\n",
      "##### Epoch: 4501/10000 || Loss: 1.0987341403961182\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 4501 || Loss: 1.0144702846353704 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 4601/10000 || Item: 0/85 || Loss: 0.7747846245765686\n",
      "--------------------------------------------------\n",
      "##### Epoch: 4601/10000 || Loss: 0.3312572240829468\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 4601 || Loss: 1.038085780360482 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 4701/10000 || Item: 0/85 || Loss: 0.8020861744880676\n",
      "--------------------------------------------------\n",
      "##### Epoch: 4701/10000 || Loss: 0.38826999068260193\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 4701 || Loss: 1.0283889445391567 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 4801/10000 || Item: 0/85 || Loss: 0.886292576789856\n",
      "--------------------------------------------------\n",
      "##### Epoch: 4801/10000 || Loss: 0.7673075199127197\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 4801 || Loss: 1.0284074599092656 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 4901/10000 || Item: 0/85 || Loss: 0.6583952307701111\n",
      "--------------------------------------------------\n",
      "##### Epoch: 4901/10000 || Loss: 1.2935343980789185\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 4901 || Loss: 1.0186600089073181 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5001/10000 || Item: 0/85 || Loss: 0.7900695204734802\n",
      "--------------------------------------------------\n",
      "##### Epoch: 5001/10000 || Loss: 1.849651575088501\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 5001 || Loss: 1.0109401995485479 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 5101/10000 || Item: 0/85 || Loss: 0.7636962532997131\n",
      "--------------------------------------------------\n",
      "##### Epoch: 5101/10000 || Loss: 0.452621191740036\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 5101 || Loss: 1.0351335406303406 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 5201/10000 || Item: 0/85 || Loss: 0.8651379346847534\n",
      "--------------------------------------------------\n",
      "##### Epoch: 5201/10000 || Loss: 0.9647705554962158\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 5201 || Loss: 1.014183147387071 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 5301/10000 || Item: 0/85 || Loss: 0.7953484654426575\n",
      "--------------------------------------------------\n",
      "##### Epoch: 5301/10000 || Loss: 0.7285590171813965\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 5301 || Loss: 1.0250921086831526 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 5401/10000 || Item: 0/85 || Loss: 0.6726941466331482\n",
      "--------------------------------------------------\n",
      "##### Epoch: 5401/10000 || Loss: 1.1263127326965332\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 5401 || Loss: 1.0199665752324192 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 5501/10000 || Item: 0/85 || Loss: 0.8674876093864441\n",
      "--------------------------------------------------\n",
      "##### Epoch: 5501/10000 || Loss: 0.6843538284301758\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 5501 || Loss: 1.0148405595259233 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 5601/10000 || Item: 0/85 || Loss: 0.8212641477584839\n",
      "--------------------------------------------------\n",
      "##### Epoch: 5601/10000 || Loss: 0.8526732921600342\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 5601 || Loss: 1.0215012268586592 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 5701/10000 || Item: 0/85 || Loss: 0.7263515591621399\n",
      "--------------------------------------------------\n",
      "##### Epoch: 5701/10000 || Loss: 1.5693888664245605\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 5701 || Loss: 1.01382454958829 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 5801/10000 || Item: 0/85 || Loss: 0.7836751341819763\n",
      "--------------------------------------------------\n",
      "##### Epoch: 5801/10000 || Loss: 0.963763952255249\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 5801 || Loss: 1.0344581007957458 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 5901/10000 || Item: 0/85 || Loss: 0.8930330276489258\n",
      "--------------------------------------------------\n",
      "##### Epoch: 5901/10000 || Loss: 0.5593289136886597\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 5901 || Loss: 1.025304983962666 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 6001/10000 || Item: 0/85 || Loss: 0.8307792544364929\n",
      "--------------------------------------------------\n",
      "##### Epoch: 6001/10000 || Loss: 0.538273811340332\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 6001 || Loss: 1.0291492938995361 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 6101/10000 || Item: 0/85 || Loss: 0.6851978302001953\n",
      "--------------------------------------------------\n",
      "##### Epoch: 6101/10000 || Loss: 0.6527889370918274\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 6101 || Loss: 1.0432184555313804 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 6201/10000 || Item: 0/85 || Loss: 0.8016021847724915\n",
      "--------------------------------------------------\n",
      "##### Epoch: 6201/10000 || Loss: 0.9623663425445557\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 6201 || Loss: 1.0185402685945684 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 6301/10000 || Item: 0/85 || Loss: 0.871366024017334\n",
      "--------------------------------------------------\n",
      "##### Epoch: 6301/10000 || Loss: 0.7731473445892334\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 6301 || Loss: 1.0363677740097046 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 6401/10000 || Item: 0/85 || Loss: 0.7305225729942322\n",
      "--------------------------------------------------\n",
      "##### Epoch: 6401/10000 || Loss: 0.5794941186904907\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 6401 || Loss: 1.0157239491289312 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 6501/10000 || Item: 0/85 || Loss: 0.7054173350334167\n",
      "--------------------------------------------------\n",
      "##### Epoch: 6501/10000 || Loss: 0.5287256240844727\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 6501 || Loss: 1.027551157908006 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 6601/10000 || Item: 0/85 || Loss: 0.8779476881027222\n",
      "--------------------------------------------------\n",
      "##### Epoch: 6601/10000 || Loss: 0.6485331058502197\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 6601 || Loss: 1.045614849437367 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 6701/10000 || Item: 0/85 || Loss: 0.6176970601081848\n",
      "--------------------------------------------------\n",
      "##### Epoch: 6701/10000 || Loss: 0.40261662006378174\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 6701 || Loss: 1.0307720466093584 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 6801/10000 || Item: 0/85 || Loss: 0.7765376567840576\n",
      "--------------------------------------------------\n",
      "##### Epoch: 6801/10000 || Loss: 0.5930060744285583\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 6801 || Loss: 1.0264333269812844 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 6901/10000 || Item: 0/85 || Loss: 0.8267241716384888\n",
      "--------------------------------------------------\n",
      "##### Epoch: 6901/10000 || Loss: 0.7650272846221924\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 6901 || Loss: 1.028663082556291 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 7001/10000 || Item: 0/85 || Loss: 0.8490378856658936\n",
      "--------------------------------------------------\n",
      "##### Epoch: 7001/10000 || Loss: 0.4267275631427765\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 7001 || Loss: 1.0330870801752263 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 7101/10000 || Item: 0/85 || Loss: 0.6244630217552185\n",
      "--------------------------------------------------\n",
      "##### Epoch: 7101/10000 || Loss: 0.7832319736480713\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 7101 || Loss: 1.031566787849773 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 7201/10000 || Item: 0/85 || Loss: 0.801993727684021\n",
      "--------------------------------------------------\n",
      "##### Epoch: 7201/10000 || Loss: 0.7185300588607788\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 7201 || Loss: 1.022422893480821 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 7301/10000 || Item: 0/85 || Loss: 0.7633333206176758\n",
      "--------------------------------------------------\n",
      "##### Epoch: 7301/10000 || Loss: 1.1502647399902344\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 7301 || Loss: 1.0262579213489185 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 7401/10000 || Item: 0/85 || Loss: 0.6935725808143616\n",
      "--------------------------------------------------\n",
      "##### Epoch: 7401/10000 || Loss: 0.8050358891487122\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 7401 || Loss: 1.0290618376298384 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7501/10000 || Item: 0/85 || Loss: 0.7741328477859497\n",
      "--------------------------------------------------\n",
      "##### Epoch: 7501/10000 || Loss: 0.941972553730011\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 7501 || Loss: 1.0525735020637512 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 7601/10000 || Item: 0/85 || Loss: 0.935082197189331\n",
      "--------------------------------------------------\n",
      "##### Epoch: 7601/10000 || Loss: 0.6530162692070007\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 7601 || Loss: 1.0332807573405178 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 7701/10000 || Item: 0/85 || Loss: 0.7856796383857727\n",
      "--------------------------------------------------\n",
      "##### Epoch: 7701/10000 || Loss: 1.0814590454101562\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 7701 || Loss: 1.0234933441335505 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 7801/10000 || Item: 0/85 || Loss: 0.8184093236923218\n",
      "--------------------------------------------------\n",
      "##### Epoch: 7801/10000 || Loss: 1.1870077848434448\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 7801 || Loss: 1.0258032571185718 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 7901/10000 || Item: 0/85 || Loss: 0.9181709289550781\n",
      "--------------------------------------------------\n",
      "##### Epoch: 7901/10000 || Loss: 1.0011250972747803\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 7901 || Loss: 1.0347288901155645 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 8001/10000 || Item: 0/85 || Loss: 0.8013778924942017\n",
      "--------------------------------------------------\n",
      "##### Epoch: 8001/10000 || Loss: 0.5939688682556152\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 8001 || Loss: 1.0191800811073997 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 8101/10000 || Item: 0/85 || Loss: 0.7375029921531677\n",
      "--------------------------------------------------\n",
      "##### Epoch: 8101/10000 || Loss: 0.5598125457763672\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 8101 || Loss: 1.0333147265694358 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 8201/10000 || Item: 0/85 || Loss: 0.7141616940498352\n",
      "--------------------------------------------------\n",
      "##### Epoch: 8201/10000 || Loss: 1.3235676288604736\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 8201 || Loss: 1.0357166908004067 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 8301/10000 || Item: 0/85 || Loss: 0.7596465349197388\n",
      "--------------------------------------------------\n",
      "##### Epoch: 8301/10000 || Loss: 1.1103746891021729\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 8301 || Loss: 1.0386177463964983 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 8401/10000 || Item: 0/85 || Loss: 0.7652453184127808\n",
      "--------------------------------------------------\n",
      "##### Epoch: 8401/10000 || Loss: 0.8580446243286133\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 8401 || Loss: 1.0312536467205395 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 8501/10000 || Item: 0/85 || Loss: 0.7481508851051331\n",
      "--------------------------------------------------\n",
      "##### Epoch: 8501/10000 || Loss: 0.7503560781478882\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 8501 || Loss: 1.0386204502799294 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 8601/10000 || Item: 0/85 || Loss: 0.8238317966461182\n",
      "--------------------------------------------------\n",
      "##### Epoch: 8601/10000 || Loss: 0.6592692732810974\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 8601 || Loss: 1.024315281347795 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 8701/10000 || Item: 0/85 || Loss: 1.001186490058899\n",
      "--------------------------------------------------\n",
      "##### Epoch: 8701/10000 || Loss: 0.3513776659965515\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 8701 || Loss: 1.0246258107098667 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 8801/10000 || Item: 0/85 || Loss: 0.8337212204933167\n",
      "--------------------------------------------------\n",
      "##### Epoch: 8801/10000 || Loss: 0.6937503814697266\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 8801 || Loss: 1.0271388238126582 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 8901/10000 || Item: 0/85 || Loss: 0.7610905766487122\n",
      "--------------------------------------------------\n",
      "##### Epoch: 8901/10000 || Loss: 1.0625252723693848\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 8901 || Loss: 1.0221837217157537 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 9001/10000 || Item: 0/85 || Loss: 0.7563347220420837\n",
      "--------------------------------------------------\n",
      "##### Epoch: 9001/10000 || Loss: 1.0939576625823975\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 9001 || Loss: 1.0274318077347495 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 9101/10000 || Item: 0/85 || Loss: 0.5920257568359375\n",
      "--------------------------------------------------\n",
      "##### Epoch: 9101/10000 || Loss: 1.02228844165802\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 9101 || Loss: 1.03357472744855 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 9201/10000 || Item: 0/85 || Loss: 0.6403907537460327\n",
      "--------------------------------------------------\n",
      "##### Epoch: 9201/10000 || Loss: 0.9883867502212524\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 9201 || Loss: 1.035064084963365 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 9301/10000 || Item: 0/85 || Loss: 0.7473025918006897\n",
      "--------------------------------------------------\n",
      "##### Epoch: 9301/10000 || Loss: 1.183032751083374\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 9301 || Loss: 1.029608060013164 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 9401/10000 || Item: 0/85 || Loss: 0.9720341563224792\n",
      "--------------------------------------------------\n",
      "##### Epoch: 9401/10000 || Loss: 0.14954665303230286\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 9401 || Loss: 1.0284083702347495 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 9501/10000 || Item: 0/85 || Loss: 0.8171369433403015\n",
      "--------------------------------------------------\n",
      "##### Epoch: 9501/10000 || Loss: 0.5372164249420166\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 9501 || Loss: 1.0389667424288662 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 9601/10000 || Item: 0/85 || Loss: 0.7775603532791138\n",
      "--------------------------------------------------\n",
      "##### Epoch: 9601/10000 || Loss: 0.5846449136734009\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 9601 || Loss: 1.0284035801887512 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 9701/10000 || Item: 0/85 || Loss: 0.8441153764724731\n",
      "--------------------------------------------------\n",
      "##### Epoch: 9701/10000 || Loss: 0.9494147300720215\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 9701 || Loss: 1.0342784036289563 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 9801/10000 || Item: 0/85 || Loss: 0.7924447655677795\n",
      "--------------------------------------------------\n",
      "##### Epoch: 9801/10000 || Loss: 0.5697372555732727\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 9801 || Loss: 1.0606858513572 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 9901/10000 || Item: 0/85 || Loss: 0.7977911829948425\n",
      "--------------------------------------------------\n",
      "##### Epoch: 9901/10000 || Loss: 1.2329710721969604\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 9901 || Loss: 1.0314211357723584 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on dataset of size 672: 62.35118865966797 %.\n",
      "Average loss: 1.0316040895201943\n",
      "proportion of labels in prediction: [tensor(1.), tensor(0.), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.76810266 0.         0.        ]\n",
      "- f1 (average): 0.25603421937060805\n",
      "- accuracy: 0.6235119104385376\n"
     ]
    }
   ],
   "source": [
    "signature_history = obtain_signatures_history(sbert_768_embeddings,\n",
    "                                              path_specifics,\n",
    "                                              dimension,\n",
    "                                              sig_depth,\n",
    "                                              concatenate_current=False)\n",
    "print(f\"signature_history.shape = {signature_history.shape}\")\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=signature_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842d8490",
   "metadata": {},
   "source": [
    "## SBERT 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6c3f66a8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] - columns beginning with 'd' denote the dimension reduced embeddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Adding 'time_encoding' and feature...\n",
      "[INFO] Adding 'time_diff' and feature...\n",
      "[INFO] Adding 'timeline_index' feature...\n",
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd4b841f1b3f4cf39c25b21c85724122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signature_history.shape = torch.Size([6725, 1164])\n",
      "\n",
      "********** hidden_dim: [100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f8adfbdfe9f4a77b4d8919792b759ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0521900653839111\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.9816925525665283\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9814600131728433 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 33!\n",
      "Accuracy on dataset of size 672: 67.55952453613281 %.\n",
      "Average loss: 0.7989161989905618\n",
      "proportion of labels in prediction: [tensor(0.7098), tensor(0.1920), tensor(0.0982)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.79241071 0.48028674 0.37869822]\n",
      "- f1 (average): 0.5504652258296799\n",
      "- accuracy: 0.675595223903656\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9ff8589aea749fb989b7ea9c9bd0a9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.093984603881836\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.052046537399292\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0197868834842334 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 65!\n",
      "Accuracy on dataset of size 672: 66.36904907226562 %.\n",
      "Average loss: 0.8858089717951688\n",
      "proportion of labels in prediction: [tensor(0.6905), tensor(0.2039), tensor(0.1057)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.77689694 0.48083624 0.3908046 ]\n",
      "- f1 (average): 0.5495125922924343\n",
      "- accuracy: 0.663690447807312\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4390f4359ca4114bc37b3f860453e1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1012998819351196\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0739198923110962\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0168006257577376 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 41!\n",
      "Accuracy on dataset of size 672: 62.94643020629883 %.\n",
      "Average loss: 0.9017694700847972\n",
      "proportion of labels in prediction: [tensor(0.6949), tensor(0.3051), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.78329571 0.42816901 0.        ]\n",
      "- f1 (average): 0.40382157504848504\n",
      "- accuracy: 0.6294642686843872\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d54e3a783eae41afb65b84315b8f7aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1113770008087158\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0579935312271118\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.030761794610457 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 25!\n",
      "Accuracy on dataset of size 672: 61.30952453613281 %.\n",
      "Average loss: 0.9385421655394814\n",
      "proportion of labels in prediction: [tensor(0.6741), tensor(0.3259), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.7706422  0.41192412 0.        ]\n",
      "- f1 (average): 0.3941887736920182\n",
      "- accuracy: 0.613095223903656\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8124ce348ac44952bc4719e6dd4d4c90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0995882749557495\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0875303745269775\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0160398049788042 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 35!\n",
      "Accuracy on dataset of size 672: 61.755950927734375 %.\n",
      "Average loss: 1.000803914937106\n",
      "proportion of labels in prediction: [tensor(0.6637), tensor(0.3363), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.77456647 0.42553191 0.        ]\n",
      "- f1 (average): 0.40003279629401883\n",
      "- accuracy: 0.617559552192688\n"
     ]
    }
   ],
   "source": [
    "signature_history = obtain_signatures_history(sbert_384_embeddings, path_specifics, dimension, sig_depth)\n",
    "print(f\"signature_history.shape = {signature_history.shape}\")\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=signature_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6b9deb21",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] - columns beginning with 'd' denote the dimension reduced embeddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Adding 'time_encoding' and feature...\n",
      "[INFO] Adding 'time_diff' and feature...\n",
      "[INFO] Adding 'timeline_index' feature...\n",
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e3562b185ca4cf2a325f7a7370ca090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signature_history.shape = torch.Size([6725, 780])\n",
      "\n",
      "********** hidden_dim: [100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb578d0465854563ac03757404cc46b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1300742626190186\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.1073238849639893\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0831208662553267 || Accuracy: 0.5156017541885376 || F1-score: 0.284446079101693\n",
      "Early stopping at epoch 12!\n",
      "Accuracy on dataset of size 672: 62.35118865966797 %.\n",
      "Average loss: 0.9457747556946494\n",
      "proportion of labels in prediction: [tensor(1.), tensor(0.), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.76810266 0.         0.        ]\n",
      "- f1 (average): 0.25603421937060805\n",
      "- accuracy: 0.6235119104385376\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c6e2159105649ae9551bcf6c215acb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1388251781463623\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0828475952148438\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0809132727709683 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 41!\n",
      "Accuracy on dataset of size 672: 59.970237731933594 %.\n",
      "Average loss: 1.0203760428862139\n",
      "proportion of labels in prediction: [tensor(0.9598), tensor(0.0387), tensor(0.0015)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.75       0.04545455 0.        ]\n",
      "- f1 (average): 0.2651515151515152\n",
      "- accuracy: 0.5997023582458496\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efd5b67c02824508bf25538b2838e012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0802912712097168\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0371514558792114\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0190406333316455 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.8518695831298828\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.5540586709976196\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.9600368413058195 || Accuracy: 0.6181277632713318 || F1-score: 0.2816021054363023\n",
      "Epoch: 201/10000 || Item: 0/85 || Loss: 0.8303055763244629\n",
      "--------------------------------------------------\n",
      "##### Epoch: 201/10000 || Loss: 0.33418047428131104\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 201 || Loss: 0.9751427390358665 || Accuracy: 0.6181277632713318 || F1-score: 0.2816021054363023\n",
      "Epoch: 301/10000 || Item: 0/85 || Loss: 0.9268263578414917\n",
      "--------------------------------------------------\n",
      "##### Epoch: 301/10000 || Loss: 0.7403377890586853\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 301 || Loss: 0.9704450748183511 || Accuracy: 0.6181277632713318 || F1-score: 0.2816021054363023\n",
      "Epoch: 401/10000 || Item: 0/85 || Loss: 0.8959640860557556\n",
      "--------------------------------------------------\n",
      "##### Epoch: 401/10000 || Loss: 0.5652618408203125\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 401 || Loss: 0.9605453447862105 || Accuracy: 0.6181277632713318 || F1-score: 0.2816021054363023\n",
      "Epoch: 501/10000 || Item: 0/85 || Loss: 0.7552034854888916\n",
      "--------------------------------------------------\n",
      "##### Epoch: 501/10000 || Loss: 0.6141928434371948\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 501 || Loss: 0.9778123931451277 || Accuracy: 0.6181277632713318 || F1-score: 0.2816021054363023\n",
      "Epoch: 601/10000 || Item: 0/85 || Loss: 0.9829444289207458\n",
      "--------------------------------------------------\n",
      "##### Epoch: 601/10000 || Loss: 0.8471457362174988\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 601 || Loss: 0.9650126045400446 || Accuracy: 0.6181277632713318 || F1-score: 0.2816021054363023\n",
      "Early stopping at epoch 633!\n",
      "Accuracy on dataset of size 672: 61.16071319580078 %.\n",
      "Average loss: 1.0144365321506152\n",
      "proportion of labels in prediction: [tensor(0.9792), tensor(0.0208), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.75951718 0.02439024 0.        ]\n",
      "- f1 (average): 0.2613024737489715\n",
      "- accuracy: 0.6116071343421936\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b48d0a260d2341128f5b78f811e8ac52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.087432861328125\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.043636679649353\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0156650976701216 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.7902301549911499\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.42113643884658813\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 1.0998676256699995 || Accuracy: 0.5750371217727661 || F1-score: 0.30910314332554906\n",
      "Epoch: 201/10000 || Item: 0/85 || Loss: 0.7233343720436096\n",
      "--------------------------------------------------\n",
      "##### Epoch: 201/10000 || Loss: 1.1722097396850586\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 201 || Loss: 1.0915379144928672 || Accuracy: 0.5750371217727661 || F1-score: 0.30910314332554906\n",
      "Epoch: 301/10000 || Item: 0/85 || Loss: 0.5751740336418152\n",
      "--------------------------------------------------\n",
      "##### Epoch: 301/10000 || Loss: 0.34531813859939575\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 301 || Loss: 1.098689084703272 || Accuracy: 0.5750371217727661 || F1-score: 0.30910314332554906\n",
      "Epoch: 401/10000 || Item: 0/85 || Loss: 0.6410079002380371\n",
      "--------------------------------------------------\n",
      "##### Epoch: 401/10000 || Loss: 0.527923047542572\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 401 || Loss: 1.1114069440148093 || Accuracy: 0.5750371217727661 || F1-score: 0.30910314332554906\n",
      "Epoch: 501/10000 || Item: 0/85 || Loss: 0.7819822430610657\n",
      "--------------------------------------------------\n",
      "##### Epoch: 501/10000 || Loss: 0.6305009722709656\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 501 || Loss: 1.0996586680412292 || Accuracy: 0.5750371217727661 || F1-score: 0.30910314332554906\n",
      "Epoch: 601/10000 || Item: 0/85 || Loss: 0.761721134185791\n",
      "--------------------------------------------------\n",
      "##### Epoch: 601/10000 || Loss: 0.5359998345375061\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 601 || Loss: 1.1097294200550427 || Accuracy: 0.5750371217727661 || F1-score: 0.30910314332554906\n",
      "Epoch: 701/10000 || Item: 0/85 || Loss: 0.5839788317680359\n",
      "--------------------------------------------------\n",
      "##### Epoch: 701/10000 || Loss: 1.0059094429016113\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 701 || Loss: 1.1038995493542065 || Accuracy: 0.5750371217727661 || F1-score: 0.30910314332554906\n",
      "Epoch: 801/10000 || Item: 0/85 || Loss: 0.654366672039032\n",
      "--------------------------------------------------\n",
      "##### Epoch: 801/10000 || Loss: 0.6961441040039062\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 801 || Loss: 1.0980209003795276 || Accuracy: 0.5750371217727661 || F1-score: 0.30910314332554906\n",
      "Epoch: 901/10000 || Item: 0/85 || Loss: 0.6452459096908569\n",
      "--------------------------------------------------\n",
      "##### Epoch: 901/10000 || Loss: 0.35842156410217285\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 901 || Loss: 1.1130921678109602 || Accuracy: 0.5750371217727661 || F1-score: 0.30910314332554906\n",
      "Epoch: 1001/10000 || Item: 0/85 || Loss: 0.6303397417068481\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1001/10000 || Loss: 0.8539872169494629\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1001 || Loss: 1.1072295362299138 || Accuracy: 0.5750371217727661 || F1-score: 0.30910314332554906\n",
      "Epoch: 1101/10000 || Item: 0/85 || Loss: 0.5786576271057129\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1101/10000 || Loss: 0.6396150588989258\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1101 || Loss: 1.1010127175938 || Accuracy: 0.5750371217727661 || F1-score: 0.30910314332554906\n",
      "Epoch: 1201/10000 || Item: 0/85 || Loss: 0.7263380289077759\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1201/10000 || Loss: 0.599936842918396\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1201 || Loss: 1.1083208593455227 || Accuracy: 0.5750371217727661 || F1-score: 0.30910314332554906\n",
      "Epoch: 1301/10000 || Item: 0/85 || Loss: 0.7819236516952515\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1301/10000 || Loss: 0.4505401849746704\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1301 || Loss: 1.1042547225952148 || Accuracy: 0.5750371217727661 || F1-score: 0.30910314332554906\n",
      "Epoch: 1401/10000 || Item: 0/85 || Loss: 0.6844585537910461\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1401/10000 || Loss: 0.4391840696334839\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1401 || Loss: 1.102079293944619 || Accuracy: 0.5750371217727661 || F1-score: 0.30910314332554906\n",
      "Epoch: 1501/10000 || Item: 0/85 || Loss: 0.6167891025543213\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1501/10000 || Loss: 0.37942391633987427\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1501 || Loss: 1.1063821532509543 || Accuracy: 0.5750371217727661 || F1-score: 0.30910314332554906\n",
      "Epoch: 1601/10000 || Item: 0/85 || Loss: 0.789699912071228\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1601/10000 || Loss: 0.3384065628051758\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1601 || Loss: 1.100083432414315 || Accuracy: 0.5750371217727661 || F1-score: 0.30910314332554906\n",
      "Epoch: 1701/10000 || Item: 0/85 || Loss: 0.618873655796051\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1701/10000 || Loss: 1.5668690204620361\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1701 || Loss: 1.1120797883380542 || Accuracy: 0.5750371217727661 || F1-score: 0.30910314332554906\n",
      "Epoch: 1801/10000 || Item: 0/85 || Loss: 0.5614541172981262\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1801/10000 || Loss: 0.7705998420715332\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1801 || Loss: 1.1003012440421365 || Accuracy: 0.5750371217727661 || F1-score: 0.3091839066474353\n",
      "Epoch: 1901/10000 || Item: 0/85 || Loss: 0.6413623094558716\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1901/10000 || Loss: 0.7001355290412903\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1901 || Loss: 1.1064322048967534 || Accuracy: 0.5750371217727661 || F1-score: 0.3091839066474353\n",
      "Epoch: 2001/10000 || Item: 0/85 || Loss: 0.6632228493690491\n",
      "--------------------------------------------------\n",
      "##### Epoch: 2001/10000 || Loss: 0.7152105569839478\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 2001 || Loss: 1.0947884592142971 || Accuracy: 0.5765230059623718 || F1-score: 0.3097703413810869\n",
      "Epoch: 2101/10000 || Item: 0/85 || Loss: 0.7943467497825623\n",
      "--------------------------------------------------\n",
      "##### Epoch: 2101/10000 || Loss: 0.20375624299049377\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 2101 || Loss: 1.1039101752367886 || Accuracy: 0.5765230059623718 || F1-score: 0.3097703413810869\n",
      "Epoch: 2201/10000 || Item: 0/85 || Loss: 0.6422424912452698\n",
      "--------------------------------------------------\n",
      "##### Epoch: 2201/10000 || Loss: 0.3090302348136902\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 2201 || Loss: 1.098241475495425 || Accuracy: 0.5765230059623718 || F1-score: 0.3097703413810869\n",
      "Epoch: 2301/10000 || Item: 0/85 || Loss: 0.7044761180877686\n",
      "--------------------------------------------------\n",
      "##### Epoch: 2301/10000 || Loss: 1.1786261796951294\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 2301 || Loss: 1.1231097416444258 || Accuracy: 0.5765230059623718 || F1-score: 0.3097703413810869\n",
      "Early stopping at epoch 2355!\n",
      "Accuracy on dataset of size 672: 58.92856979370117 %.\n",
      "Average loss: 1.1903398470445112\n",
      "proportion of labels in prediction: [tensor(0.9048), tensor(0.0759), tensor(0.0193)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.74391431 0.11940299 0.03448276]\n",
      "- f1 (average): 0.2992666857432944\n",
      "- accuracy: 0.5892857313156128\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a16cd14754482d9274b324a2dee134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0990127325057983\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.083052635192871\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0344014926390215 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 41!\n",
      "Accuracy on dataset of size 672: 57.886905670166016 %.\n",
      "Average loss: 1.2762420014901594\n",
      "proportion of labels in prediction: [tensor(0.8824), tensor(0.0982), tensor(0.0193)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.72924901 0.16666667 0.03448276]\n",
      "- f1 (average): 0.3101328123816879\n",
      "- accuracy: 0.5788690447807312\n"
     ]
    }
   ],
   "source": [
    "signature_history = obtain_signatures_history(sbert_384_embeddings,\n",
    "                                              path_specifics,\n",
    "                                              dimension,\n",
    "                                              sig_depth,\n",
    "                                              concatenate_current=False)\n",
    "print(f\"signature_history.shape = {signature_history.shape}\")\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=signature_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7947d2",
   "metadata": {},
   "source": [
    "## Pretrained BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394b119c",
   "metadata": {},
   "source": [
    "### Mean pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "05205d91",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] - columns beginning with 'd' denote the dimension reduced embeddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Adding 'time_encoding' and feature...\n",
      "[INFO] Adding 'time_diff' and feature...\n",
      "[INFO] Adding 'timeline_index' feature...\n",
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d26b9292d14b9d8a602d0c9c8d8270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signature_history.shape = torch.Size([6725, 1548])\n",
      "\n",
      "********** hidden_dim: [100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97d1f92cea524d7d85325fcc1b7ef899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 6.673676013946533\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.8129488229751587\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.8461785208095203 || Accuracy: 0.5720653533935547 || F1-score: 0.3435318842984348\n",
      "Early stopping at epoch 30!\n",
      "Accuracy on dataset of size 672: 55.9523811340332 %.\n",
      "Average loss: 1.7869080738587813\n",
      "proportion of labels in prediction: [tensor(0.8006), tensor(0.1518), tensor(0.0476)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.71891327 0.21428571 0.07407407]\n",
      "- f1 (average): 0.335757686332399\n",
      "- accuracy: 0.5595238208770752\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a81cba6c570046dd98fe5e77ce0a3fb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.5531699657440186\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 2.798264741897583\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0943772250955754 || Accuracy: 0.5824665427207947 || F1-score: 0.371300486514424\n",
      "Early stopping at epoch 12!\n",
      "Accuracy on dataset of size 672: 60.863094329833984 %.\n",
      "Average loss: 1.1093834096735173\n",
      "proportion of labels in prediction: [tensor(0.8810), tensor(0.0923), tensor(0.0268)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.75568744 0.20754717 0.08264463]\n",
      "- f1 (average): 0.3486264120301714\n",
      "- accuracy: 0.6086309552192688\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3294db3b8340af8a577e5e4a30ca62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1850405931472778\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.8447656631469727\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.035943871194666 || Accuracy: 0.6136701107025146 || F1-score: 0.26448658301845257\n",
      "Early stopping at epoch 29!\n",
      "Accuracy on dataset of size 672: 58.48214340209961 %.\n",
      "Average loss: 1.5035460103641858\n",
      "proportion of labels in prediction: [tensor(0.8006), tensor(0.1711), tensor(0.0283)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.74190178 0.24150943 0.09836066]\n",
      "- f1 (average): 0.36059062202816805\n",
      "- accuracy: 0.5848214030265808\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df978d9ede314d8d99566be04454cb13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0213910341262817\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.7883847951889038\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.986276084726507 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 27!\n",
      "Accuracy on dataset of size 672: 54.31547546386719 %.\n",
      "Average loss: 1.6418841318650679\n",
      "proportion of labels in prediction: [tensor(0.7143), tensor(0.2664), tensor(0.0193)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.70300334 0.27355623 0.06896552]\n",
      "- f1 (average): 0.34850836176185856\n",
      "- accuracy: 0.543154776096344\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "517a06def54945faab12e106bfbb84f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0394494533538818\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.8005906939506531\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9860641251910817 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 29!\n",
      "Accuracy on dataset of size 672: 56.39881134033203 %.\n",
      "Average loss: 1.7352937134829434\n",
      "proportion of labels in prediction: [tensor(0.7366), tensor(0.2083), tensor(0.0551)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.71772429 0.28275862 0.14285714]\n",
      "- f1 (average): 0.3811133507956869\n",
      "- accuracy: 0.5639880895614624\n"
     ]
    }
   ],
   "source": [
    "signature_history = obtain_signatures_history(pooled_mean_pretrained, path_specifics, dimension, sig_depth)\n",
    "print(f\"signature_history.shape = {signature_history.shape}\")\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=signature_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "eebcd10c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] - columns beginning with 'd' denote the dimension reduced embeddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Adding 'time_encoding' and feature...\n",
      "[INFO] Adding 'time_diff' and feature...\n",
      "[INFO] Adding 'timeline_index' feature...\n",
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd4a537870540498da05a31df187cf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signature_history.shape = torch.Size([6725, 780])\n",
      "\n",
      "********** hidden_dim: [100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23511b0ac292498ead1a41d18306eb61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 3.7441978454589844\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 4.3166351318359375\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 2.4711484258825127 || Accuracy: 0.4041604697704315 || F1-score: 0.3118328262267227\n",
      "Early stopping at epoch 23!\n",
      "Accuracy on dataset of size 672: 55.05952453613281 %.\n",
      "Average loss: 1.7590262673117898\n",
      "proportion of labels in prediction: [tensor(0.8452), tensor(0.1071), tensor(0.0476)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.71529889 0.10810811 0.07407407]\n",
      "- f1 (average): 0.2991603558979446\n",
      "- accuracy: 0.550595223903656\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32386d425f1d441c909b19b8adc4add8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 2.1577088832855225\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.2402211427688599\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.284072149883617 || Accuracy: 0.4710252583026886 || F1-score: 0.32294703244466244\n",
      "Early stopping at epoch 15!\n",
      "Accuracy on dataset of size 672: 59.375 %.\n",
      "Average loss: 1.221100395376032\n",
      "proportion of labels in prediction: [tensor(0.9360), tensor(0.0491), tensor(0.0149)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.74427481 0.08743169 0.01769912]\n",
      "- f1 (average): 0.2831352060645414\n",
      "- accuracy: 0.59375\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bdd28fdd30c4221aa15637c8dd72abd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.4558333158493042\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.7245966792106628\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0402219241315669 || Accuracy: 0.601783037185669 || F1-score: 0.2927995350219556\n",
      "Early stopping at epoch 50!\n",
      "Accuracy on dataset of size 672: 57.886905670166016 %.\n",
      "Average loss: 1.361782810904763\n",
      "proportion of labels in prediction: [tensor(0.8720), tensor(0.0938), tensor(0.0342)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.73631841 0.12206573 0.0952381 ]\n",
      "- f1 (average): 0.3178740769659416\n",
      "- accuracy: 0.5788690447807312\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d4f4ec4394c474692aab444eff270ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1106586456298828\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0407090187072754\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.006961995905096 || Accuracy: 0.6210995316505432 || F1-score: 0.25542315918117936\n",
      "Early stopping at epoch 27!\n",
      "Accuracy on dataset of size 672: 55.654762268066406 %.\n",
      "Average loss: 1.5424025925722988\n",
      "proportion of labels in prediction: [tensor(0.8333), tensor(0.1205), tensor(0.0461)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.71910112 0.17316017 0.02985075]\n",
      "- f1 (average): 0.3073706810081118\n",
      "- accuracy: 0.5565476417541504\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38204e2458d5441bbaa28cc5c305e6c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0596250295639038\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.8599212765693665\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0026457201350818 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 27!\n",
      "Accuracy on dataset of size 672: 54.01785659790039 %.\n",
      "Average loss: 1.7785337079655041\n",
      "proportion of labels in prediction: [tensor(0.8110), tensor(0.1533), tensor(0.0357)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.70746888 0.15810277 0.03149606]\n",
      "- f1 (average): 0.2990225698195315\n",
      "- accuracy: 0.5401785969734192\n"
     ]
    }
   ],
   "source": [
    "signature_history = obtain_signatures_history(pooled_mean_pretrained,\n",
    "                                              path_specifics,\n",
    "                                              dimension,\n",
    "                                              sig_depth,\n",
    "                                              concatenate_current=False)\n",
    "print(f\"signature_history.shape = {signature_history.shape}\")\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=signature_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba973b62",
   "metadata": {},
   "source": [
    "### Max pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0d006c7e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] - columns beginning with 'd' denote the dimension reduced embeddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Adding 'time_encoding' and feature...\n",
      "[INFO] Adding 'time_diff' and feature...\n",
      "[INFO] Adding 'timeline_index' feature...\n",
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d66d2b392bd040daa9024d86748870bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signature_history.shape = torch.Size([6725, 1548])\n",
      "\n",
      "********** hidden_dim: [100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4faf3ed0dacf4f67acc7335171215eca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 29.369617462158203\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 7.37844181060791\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 5.658988670869307 || Accuracy: 0.5408617854118347 || F1-score: 0.33396321640666926\n",
      "Early stopping at epoch 28!\n",
      "Accuracy on dataset of size 672: 55.505950927734375 %.\n",
      "Average loss: 3.3138548894362017\n",
      "proportion of labels in prediction: [tensor(0.7723), tensor(0.1756), tensor(0.0521)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.70362473 0.25373134 0.13043478]\n",
      "- f1 (average): 0.36259695312258583\n",
      "- accuracy: 0.555059552192688\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6ad4a51020742f2b39d20270676c1e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 6.738647937774658\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 3.3066258430480957\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 2.2583379203622993 || Accuracy: 0.508172333240509 || F1-score: 0.3325111038910033\n",
      "Early stopping at epoch 49!\n",
      "Accuracy on dataset of size 672: 56.39881134033203 %.\n",
      "Average loss: 1.5961687889966099\n",
      "proportion of labels in prediction: [tensor(0.8006), tensor(0.1607), tensor(0.0387)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.71891327 0.2248062  0.09302326]\n",
      "- f1 (average): 0.34558090933391655\n",
      "- accuracy: 0.5639880895614624\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95745a9fbea74367a4b6e886adca137e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 3.146319627761841\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.1189898252487183\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.190718260678378 || Accuracy: 0.5661218166351318 || F1-score: 0.3281647443478431\n",
      "Early stopping at epoch 12!\n",
      "Accuracy on dataset of size 672: 59.375 %.\n",
      "Average loss: 1.1160890730944546\n",
      "proportion of labels in prediction: [tensor(0.9330), tensor(0.0595), tensor(0.0074)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.7456979  0.09473684 0.        ]\n",
      "- f1 (average): 0.2801449129515951\n",
      "- accuracy: 0.59375\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "425d361b115644ec9d272228ec8aed60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.9987678527832031\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.6346924304962158\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0325498147444292 || Accuracy: 0.601783037185669 || F1-score: 0.2609936055090999\n",
      "Early stopping at epoch 39!\n",
      "Accuracy on dataset of size 672: 58.48214340209961 %.\n",
      "Average loss: 1.368440563028509\n",
      "proportion of labels in prediction: [tensor(0.8244), tensor(0.1503), tensor(0.0253)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.73997945 0.23904382 0.05      ]\n",
      "- f1 (average): 0.34300775657220384\n",
      "- accuracy: 0.5848214030265808\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62ecc8b751434bee936dbffd0af56bf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.020685076713562\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.8861537575721741\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9861424511129205 || Accuracy: 0.6240713000297546 || F1-score: 0.2598877839178142\n",
      "Early stopping at epoch 27!\n",
      "Accuracy on dataset of size 672: 54.91071319580078 %.\n",
      "Average loss: 1.835107304833152\n",
      "proportion of labels in prediction: [tensor(0.7679), tensor(0.1860), tensor(0.0461)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.71657754 0.20363636 0.08955224]\n",
      "- f1 (average): 0.3365887141830952\n",
      "- accuracy: 0.5491071343421936\n"
     ]
    }
   ],
   "source": [
    "signature_history = obtain_signatures_history(pooled_max_pretrained, path_specifics, dimension, sig_depth)\n",
    "print(f\"signature_history.shape = {signature_history.shape}\")\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=signature_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "427412af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] - columns beginning with 'd' denote the dimension reduced embeddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Adding 'time_encoding' and feature...\n",
      "[INFO] Adding 'time_diff' and feature...\n",
      "[INFO] Adding 'timeline_index' feature...\n",
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce95a6ddeea49a3916a98fd93659bd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signature_history.shape = torch.Size([6725, 780])\n",
      "\n",
      "********** hidden_dim: [100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caee8b13740641589c345d80a6843bce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 21.10601234436035\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 7.336426734924316\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 8.051020622253418 || Accuracy: 0.40861812233924866 || F1-score: 0.29812342336125414\n",
      "Early stopping at epoch 15!\n",
      "Accuracy on dataset of size 672: 55.505950927734375 %.\n",
      "Average loss: 3.8297226320613516\n",
      "proportion of labels in prediction: [tensor(0.7753), tensor(0.1607), tensor(0.0640)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.70851064 0.23255814 0.1369863 ]\n",
      "- f1 (average): 0.3593516930675397\n",
      "- accuracy: 0.555059552192688\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b5d747112ed404d97cc40ea68fdd2b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 8.153376579284668\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 4.407326698303223\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 2.7375375032424927 || Accuracy: 0.4234769642353058 || F1-score: 0.30676960211418874\n",
      "Early stopping at epoch 16!\n",
      "Accuracy on dataset of size 672: 55.80356979370117 %.\n",
      "Average loss: 1.5238312591205945\n",
      "proportion of labels in prediction: [tensor(0.8393), tensor(0.1116), tensor(0.0491)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.71617497 0.16       0.07352941]\n",
      "- f1 (average): 0.31656812877745194\n",
      "- accuracy: 0.5580357313156128\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a5aaa394464b16b8cef6399282133e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 3.3812642097473145\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.1157633066177368\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.3977787061171099 || Accuracy: 0.5690935850143433 || F1-score: 0.3374554877625502\n",
      "Early stopping at epoch 12!\n",
      "Accuracy on dataset of size 672: 58.779762268066406 %.\n",
      "Average loss: 1.1003501631996848\n",
      "proportion of labels in prediction: [tensor(0.9301), tensor(0.0476), tensor(0.0223)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.74137931 0.07692308 0.01694915]\n",
      "- f1 (average): 0.27841717993675913\n",
      "- accuracy: 0.5877976417541504\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db9f49ddb848413b83a695ce85a7085d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.5172810554504395\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.377053141593933\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0480764779177578 || Accuracy: 0.6077265739440918 || F1-score: 0.2737765736092296\n",
      "Early stopping at epoch 12!\n",
      "Accuracy on dataset of size 672: 59.6726188659668 %.\n",
      "Average loss: 1.0093801888552578\n",
      "proportion of labels in prediction: [tensor(0.9628), tensor(0.0283), tensor(0.0089)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.74859287 0.01183432 0.01834862]\n",
      "- f1 (average): 0.25959193797464275\n",
      "- accuracy: 0.5967261791229248\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a60815161b148f7b61ba51c955554b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.110856533050537\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.9218065738677979\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0166223970326511 || Accuracy: 0.6196136474609375 || F1-score: 0.255514705882353\n",
      "Early stopping at epoch 33!\n",
      "Accuracy on dataset of size 672: 58.779762268066406 %.\n",
      "Average loss: 1.4010108384219082\n",
      "proportion of labels in prediction: [tensor(0.8631), tensor(0.1086), tensor(0.0283)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.73473473 0.20627803 0.08196721]\n",
      "- f1 (average): 0.3409933249184394\n",
      "- accuracy: 0.5877976417541504\n"
     ]
    }
   ],
   "source": [
    "signature_history = obtain_signatures_history(pooled_max_pretrained,\n",
    "                                              path_specifics,\n",
    "                                              dimension,\n",
    "                                              sig_depth,\n",
    "                                              concatenate_current=False)\n",
    "print(f\"signature_history.shape = {signature_history.shape}\")\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=signature_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b620a0",
   "metadata": {},
   "source": [
    "### Sum pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b01e2a22",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] - columns beginning with 'd' denote the dimension reduced embeddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Adding 'time_encoding' and feature...\n",
      "[INFO] Adding 'time_diff' and feature...\n",
      "[INFO] Adding 'timeline_index' feature...\n",
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e101451967a84b979f846e9228b0670d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signature_history.shape = torch.Size([6725, 1548])\n",
      "\n",
      "********** hidden_dim: [100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7d6065f121a4180a2b4ed6a38a07a95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 10158999.0\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 826402.75\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 3137068.1420454546 || Accuracy: 0.4799405634403229 || F1-score: 0.35294660410939477\n",
      "Early stopping at epoch 28!\n",
      "Accuracy on dataset of size 672: 51.04166793823242 %.\n",
      "Average loss: 2561722.2400568184\n",
      "proportion of labels in prediction: [tensor(0.7381), tensor(0.1949), tensor(0.0670)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.66448087 0.22064057 0.10810811]\n",
      "- f1 (average): 0.33107651727335524\n",
      "- accuracy: 0.5104166865348816\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c27fc17cd9f04271a599f878e1d19e1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 10571232.0\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1005705.8125\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1273091.6903409092 || Accuracy: 0.4814264476299286 || F1-score: 0.3493515365134195\n",
      "Early stopping at epoch 34!\n",
      "Accuracy on dataset of size 672: 50.89285659790039 %.\n",
      "Average loss: 705194.6590909091\n",
      "proportion of labels in prediction: [tensor(0.6458), tensor(0.2693), tensor(0.0848)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.66588511 0.26586103 0.175     ]\n",
      "- f1 (average): 0.36891537952065395\n",
      "- accuracy: 0.5089285969734192\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7765b8bc17b04f0da6a0305bdc8d1c00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 4063484.0\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 236304.796875\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 255779.0028409091 || Accuracy: 0.4739970266819 || F1-score: 0.3305498948890851\n",
      "Early stopping at epoch 35!\n",
      "Accuracy on dataset of size 672: 52.97618865966797 %.\n",
      "Average loss: 217720.61115056818\n",
      "proportion of labels in prediction: [tensor(0.7500), tensor(0.1935), tensor(0.0565)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.68472373 0.22857143 0.11347518]\n",
      "- f1 (average): 0.34225677761788037\n",
      "- accuracy: 0.5297619104385376\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21210553ffb1499b9504b88aa5945c43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 2699980.5\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 10819.095703125\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 154303.06516335226 || Accuracy: 0.539375901222229 || F1-score: 0.2983546252699524\n",
      "Early stopping at epoch 34!\n",
      "Accuracy on dataset of size 672: 50.89285659790039 %.\n",
      "Average loss: 55088.51191850142\n",
      "proportion of labels in prediction: [tensor(0.6845), tensor(0.2589), tensor(0.0565)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.65756542 0.27777778 0.11347518]\n",
      "- f1 (average): 0.3496061234424462\n",
      "- accuracy: 0.5089285969734192\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a0507c63fe54e8c8d33df6693f54884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 761477.125\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 19553.0\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 35425.541015625 || Accuracy: 0.4457652270793915 || F1-score: 0.3175629083374602\n",
      "Early stopping at epoch 13!\n",
      "Accuracy on dataset of size 672: 54.16666793823242 %.\n",
      "Average loss: 19257.948508522728\n",
      "proportion of labels in prediction: [tensor(0.7812), tensor(0.1414), tensor(0.0774)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.70550847 0.16326531 0.14193548]\n",
      "- f1 (average): 0.33690308818989595\n",
      "- accuracy: 0.5416666865348816\n"
     ]
    }
   ],
   "source": [
    "signature_history = obtain_signatures_history(pooled_sum_pretrained, path_specifics, dimension, sig_depth)\n",
    "print(f\"signature_history.shape = {signature_history.shape}\")\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=signature_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "dd169a15",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] - columns beginning with 'd' denote the dimension reduced embeddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Adding 'time_encoding' and feature...\n",
      "[INFO] Adding 'time_diff' and feature...\n",
      "[INFO] Adding 'timeline_index' feature...\n",
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b37140bd43f54d43acdc5cbf11749cd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signature_history.shape = torch.Size([6725, 780])\n",
      "\n",
      "********** hidden_dim: [100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2419e4a41be54baf844cef95bee1e142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 24631632.0\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1984559.375\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 3590062.5795454546 || Accuracy: 0.3298662602901459 || F1-score: 0.29204124151612504\n",
      "Early stopping at epoch 36!\n",
      "Accuracy on dataset of size 672: 53.125 %.\n",
      "Average loss: 2319916.460227273\n",
      "proportion of labels in prediction: [tensor(0.7530), tensor(0.1667), tensor(0.0804)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.68756757 0.22137405 0.12738854]\n",
      "- f1 (average): 0.34544338280031384\n",
      "- accuracy: 0.53125\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f0a2558ab54dd6a38283dca4dcb3f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 4012826.75\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1179324.75\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1133526.5369318181 || Accuracy: 0.46656760573387146 || F1-score: 0.3453807718135437\n",
      "Early stopping at epoch 29!\n",
      "Accuracy on dataset of size 672: 55.35714340209961 %.\n",
      "Average loss: 682893.0625\n",
      "proportion of labels in prediction: [tensor(0.7589), tensor(0.1295), tensor(0.1116)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.71474704 0.21940928 0.15730337]\n",
      "- f1 (average): 0.36381989777157026\n",
      "- accuracy: 0.5535714030265808\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21c72d79ee6640fd9be41fcdee61ac59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 4776060.5\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 455326.0\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 530180.6803977273 || Accuracy: 0.4739970266819 || F1-score: 0.3368789105631211\n",
      "Early stopping at epoch 23!\n",
      "Accuracy on dataset of size 672: 53.4226188659668 %.\n",
      "Average loss: 243556.21839488635\n",
      "proportion of labels in prediction: [tensor(0.7396), tensor(0.1994), tensor(0.0610)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.69650655 0.23239437 0.09722222]\n",
      "- f1 (average): 0.342041046212582\n",
      "- accuracy: 0.5342261791229248\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54d73651fbd3493e83270d383575ea64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 2530993.0\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 15131.3095703125\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 184071.51988636365 || Accuracy: 0.5319464802742004 || F1-score: 0.29903258102710484\n",
      "Early stopping at epoch 16!\n",
      "Accuracy on dataset of size 672: 49.404762268066406 %.\n",
      "Average loss: 65795.36257102272\n",
      "proportion of labels in prediction: [tensor(0.6830), tensor(0.1652), tensor(0.1518)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.65375854 0.16091954 0.23414634]\n",
      "- f1 (average): 0.34960814127817663\n",
      "- accuracy: 0.494047611951828\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fa86b6f19734aaa83a9af3d308705f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 318155.34375\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 30153.4765625\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 55488.094060724434 || Accuracy: 0.45170876383781433 || F1-score: 0.3292489229434595\n",
      "Early stopping at epoch 29!\n",
      "Accuracy on dataset of size 672: 52.23214340209961 %.\n",
      "Average loss: 11308.965975674715\n",
      "proportion of labels in prediction: [tensor(0.6964), tensor(0.2634), tensor(0.0402)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.68094701 0.26299694 0.09230769]\n",
      "- f1 (average): 0.3454172155350232\n",
      "- accuracy: 0.5223214030265808\n"
     ]
    }
   ],
   "source": [
    "signature_history = obtain_signatures_history(pooled_sum_pretrained,\n",
    "                                              path_specifics,\n",
    "                                              dimension,\n",
    "                                              sig_depth,\n",
    "                                              concatenate_current=False)\n",
    "print(f\"signature_history.shape = {signature_history.shape}\")\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=signature_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec96eef",
   "metadata": {},
   "source": [
    "### CLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b27b6c01",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] - columns beginning with 'd' denote the dimension reduced embeddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Adding 'time_encoding' and feature...\n",
      "[INFO] Adding 'time_diff' and feature...\n",
      "[INFO] Adding 'timeline_index' feature...\n",
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57037ec1c8464d0a93b12a8069e86490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signature_history.shape = torch.Size([6725, 1548])\n",
      "\n",
      "********** hidden_dim: [100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86b26734120e461a9f540ceb95fa7a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 6.332112789154053\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.459240198135376\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.6063351522792468 || Accuracy: 0.5423476696014404 || F1-score: 0.34955472538253995\n",
      "Early stopping at epoch 12!\n",
      "Accuracy on dataset of size 672: 58.48214340209961 %.\n",
      "Average loss: 1.2180937853726475\n",
      "proportion of labels in prediction: [tensor(0.8497), tensor(0.1146), tensor(0.0357)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.73535354 0.18502203 0.12598425]\n",
      "- f1 (average): 0.3487866045845858\n",
      "- accuracy: 0.5848214030265808\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd02c1941f7f436fa678829324c1f67b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.6944917440414429\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.6130502820014954\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.088607582178983 || Accuracy: 0.560178279876709 || F1-score: 0.31353754359978375\n",
      "Early stopping at epoch 36!\n",
      "Accuracy on dataset of size 672: 57.14285659790039 %.\n",
      "Average loss: 1.3183693994175305\n",
      "proportion of labels in prediction: [tensor(0.7649), tensor(0.1964), tensor(0.0387)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.73097535 0.26241135 0.09302326]\n",
      "- f1 (average): 0.36213665055679206\n",
      "- accuracy: 0.5714285969734192\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2711037159a4d268ecff8e2957e9326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.185005784034729\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.8278284072875977\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0067377090454102 || Accuracy: 0.591381847858429 || F1-score: 0.25419086992167733\n",
      "Early stopping at epoch 34!\n",
      "Accuracy on dataset of size 672: 59.970237731933594 %.\n",
      "Average loss: 1.3248264572837136\n",
      "proportion of labels in prediction: [tensor(0.8199), tensor(0.1443), tensor(0.0357)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.74020619 0.27530364 0.15748031]\n",
      "- f1 (average): 0.39099671475077885\n",
      "- accuracy: 0.5997023582458496\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b56bdfcc025423e9fa9b27049977fbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0278944969177246\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.8212054371833801\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9786758639595725 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 25!\n",
      "Accuracy on dataset of size 672: 58.18452453613281 %.\n",
      "Average loss: 1.6404736908999356\n",
      "proportion of labels in prediction: [tensor(0.7321), tensor(0.2292), tensor(0.0387)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.73545554 0.30263158 0.15503876]\n",
      "- f1 (average): 0.39770862733207907\n",
      "- accuracy: 0.581845223903656\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5031fbaf0444df8bb92c9eba2a07f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0648689270019531\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.8715722560882568\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.960474046793851 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 31!\n",
      "Accuracy on dataset of size 672: 60.26785659790039 %.\n",
      "Average loss: 1.6285462054339321\n",
      "proportion of labels in prediction: [tensor(0.7411), tensor(0.2039), tensor(0.0551)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.73718648 0.34146341 0.25714286]\n",
      "- f1 (average): 0.44526424980716545\n",
      "- accuracy: 0.6026785969734192\n"
     ]
    }
   ],
   "source": [
    "signature_history = obtain_signatures_history(pooled_cls_pretrained, path_specifics, dimension, sig_depth)\n",
    "print(f\"signature_history.shape = {signature_history.shape}\")\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=signature_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "54cc6943",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] - columns beginning with 'd' denote the dimension reduced embeddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Adding 'time_encoding' and feature...\n",
      "[INFO] Adding 'time_diff' and feature...\n",
      "[INFO] Adding 'timeline_index' feature...\n",
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c6b182cf049493db0d2606d303e19b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signature_history.shape = torch.Size([6725, 780])\n",
      "\n",
      "********** hidden_dim: [100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "514ed776cc774c7cb2cb585cd9e61249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 3.4455976486206055\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 2.673224687576294\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.9785998301072554 || Accuracy: 0.4011887013912201 || F1-score: 0.31288628387717954\n",
      "Early stopping at epoch 38!\n",
      "Accuracy on dataset of size 672: 57.738094329833984 %.\n",
      "Average loss: 1.6094523885033347\n",
      "proportion of labels in prediction: [tensor(0.8304), tensor(0.1250), tensor(0.0446)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.72671443 0.23931624 0.07518797]\n",
      "- f1 (average): 0.34707288039184814\n",
      "- accuracy: 0.5773809552192688\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30db8deed8054492a1a5ebb881daea09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 2.1884889602661133\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.2756325006484985\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.1500525637106462 || Accuracy: 0.4606240689754486 || F1-score: 0.31100315939063394\n",
      "Early stopping at epoch 21!\n",
      "Accuracy on dataset of size 672: 58.779762268066406 %.\n",
      "Average loss: 1.244567719372836\n",
      "proportion of labels in prediction: [tensor(0.8958), tensor(0.0818), tensor(0.0223)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.74045054 0.14634146 0.03389831]\n",
      "- f1 (average): 0.30689676906231367\n",
      "- accuracy: 0.5877976417541504\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f72254c7474de0b52e53a8e2dd9113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.3909130096435547\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0126690864562988\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0167371251366355 || Accuracy: 0.6062406897544861 || F1-score: 0.2793916609706083\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.6514841318130493\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.6830207705497742\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 1.1357428214766763 || Accuracy: 0.5898959636688232 || F1-score: 0.32296635643798327\n",
      "Epoch: 201/10000 || Item: 0/85 || Loss: 0.662000834941864\n",
      "--------------------------------------------------\n",
      "##### Epoch: 201/10000 || Loss: 0.311781108379364\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 201 || Loss: 1.15751404653896 || Accuracy: 0.5898959636688232 || F1-score: 0.32296635643798327\n",
      "Epoch: 301/10000 || Item: 0/85 || Loss: 0.6959354281425476\n",
      "--------------------------------------------------\n",
      "##### Epoch: 301/10000 || Loss: 0.4552363455295563\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 301 || Loss: 1.147547418420965 || Accuracy: 0.5898959636688232 || F1-score: 0.32296635643798327\n",
      "Epoch: 401/10000 || Item: 0/85 || Loss: 0.7757513523101807\n",
      "--------------------------------------------------\n",
      "##### Epoch: 401/10000 || Loss: 0.7596043348312378\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 401 || Loss: 1.1305032805962996 || Accuracy: 0.5898959636688232 || F1-score: 0.32296635643798327\n",
      "Early stopping at epoch 480!\n",
      "Accuracy on dataset of size 672: 57.44047546386719 %.\n",
      "Average loss: 1.1806256175041199\n",
      "proportion of labels in prediction: [tensor(0.8735), tensor(0.1116), tensor(0.0149)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.72763419 0.16888889 0.01769912]\n",
      "- f1 (average): 0.3047407329213836\n",
      "- accuracy: 0.574404776096344\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dd3b6f8a7984f1daf1a32e58b9b13d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.2129356861114502\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.9796766042709351\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9908564253286882 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 38!\n",
      "Accuracy on dataset of size 672: 56.69643020629883 %.\n",
      "Average loss: 1.493797854943709\n",
      "proportion of labels in prediction: [tensor(0.8408), tensor(0.1190), tensor(0.0402)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.71747967 0.19130435 0.09230769]\n",
      "- f1 (average): 0.3336972383101757\n",
      "- accuracy: 0.5669642686843872\n",
      "\n",
      "********** hidden_dim: [100, 100, 100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2802342863.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3c3fcc9ec644227a60193098ef7987d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0765931606292725\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.9636225700378418\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9956892999735746 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 27!\n",
      "Accuracy on dataset of size 672: 56.5476188659668 %.\n",
      "Average loss: 1.5414887558330188\n",
      "proportion of labels in prediction: [tensor(0.7738), tensor(0.1830), tensor(0.0432)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.71565495 0.25641026 0.13636364]\n",
      "- f1 (average): 0.3694762816168568\n",
      "- accuracy: 0.5654761791229248\n"
     ]
    }
   ],
   "source": [
    "signature_history = obtain_signatures_history(pooled_cls_pretrained,\n",
    "                                              path_specifics,\n",
    "                                              dimension,\n",
    "                                              sig_depth,\n",
    "                                              concatenate_current=False)\n",
    "print(f\"signature_history.shape = {signature_history.shape}\")\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=signature_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595647d8",
   "metadata": {},
   "source": [
    "### Fine-tuned BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c588b5a",
   "metadata": {},
   "source": [
    "### Mean pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c440d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "signature_history = obtain_signatures_history(pooled_mean, path_specifics, dimension, sig_depth)\n",
    "print(f\"signature_history.shape = {signature_history.shape}\")\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=signature_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89aa4d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "signature_history = obtain_signatures_history(pooled_mean,\n",
    "                                              path_specifics,\n",
    "                                              dimension,\n",
    "                                              sig_depth,\n",
    "                                              concatenate_current=False)\n",
    "print(f\"signature_history.shape = {signature_history.shape}\")\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=signature_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a28992",
   "metadata": {},
   "source": [
    "### Max pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b79016",
   "metadata": {},
   "outputs": [],
   "source": [
    "signature_history = obtain_signatures_history(pooled_max,\n",
    "                                              path_specifics,\n",
    "                                              dimension,\n",
    "                                              sig_depth)\n",
    "print(f\"signature_history.shape = {signature_history.shape}\")\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=signature_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e628801",
   "metadata": {},
   "outputs": [],
   "source": [
    "signature_history = obtain_signatures_history(pooled_max,\n",
    "                                              path_specifics,\n",
    "                                              dimension,\n",
    "                                              sig_depth,\n",
    "                                              concatenate_current=False)\n",
    "print(f\"signature_history.shape = {signature_history.shape}\")\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=signature_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b5317d",
   "metadata": {},
   "source": [
    "### Sum pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40e48eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "signature_history = obtain_signatures_history(pooled_sum, path_specifics, dimension, sig_depth)\n",
    "print(f\"signature_history.shape = {signature_history.shape}\")\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=signature_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa9f39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "signature_history = obtain_signatures_history(pooled_sum,\n",
    "                                              path_specifics,\n",
    "                                              dimension,\n",
    "                                              sig_depth,\n",
    "                                              concatenate_current=False)\n",
    "print(f\"signature_history.shape = {signature_history.shape}\")\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=signature_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45af4c30",
   "metadata": {},
   "source": [
    "### CLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa913e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "signature_history = obtain_signatures_history(pooled_cls, path_specifics, dimension, sig_depth)\n",
    "print(f\"signature_history.shape = {signature_history.shape}\")\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=signature_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4362b5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "signature_history = obtain_signatures_history(pooled_cls,\n",
    "                                              path_specifics,\n",
    "                                              dimension,\n",
    "                                              sig_depth,\n",
    "                                              concatenate_current=False)\n",
    "print(f\"signature_history.shape = {signature_history.shape}\")\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=signature_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd384d2",
   "metadata": {},
   "source": [
    "# StackedDeepSigNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1860cdd4",
   "metadata": {},
   "source": [
    "## Obtaining path by looking at post history\n",
    "\n",
    "We can obtain a path by looking at the history of each post. Here we look at the last 10 posts (and pad with vectors of zeros if there are less than 10 posts) including the current post.\n",
    "\n",
    "We only want to consider paths that correspond to a client's utterance as we want to model a change in mood at that time. Their history will still contain the therapist's utterances too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "ac4e38ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_features = [\"time_encoding\", \"timeline_index\"]\n",
    "path_specifics = {\"pad_by\": \"history\",\n",
    "                  \"zero_padding\": True,\n",
    "                  \"method\": \"k_last\",\n",
    "                  \"k\": 5,\n",
    "                  \"time_feature\": time_features,\n",
    "                  \"standardise_method\": [\"minmax\", None],\n",
    "                  \"embeddings\": \"dim_reduced\",\n",
    "                  \"include_current_embedding\": True,\n",
    "                  \"pad_from_below\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "8f712147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_SDSN_input(embeddings, path_specifics):\n",
    "    reduction = nlpsig.DimReduce(method=\"gaussian_random_projection\", n_components=50)\n",
    "    embeddings_reduced = reduction.fit_transform(embeddings, random_state=seed)\n",
    "    \n",
    "    paths = nlpsig.PrepareData(anno_mi,\n",
    "                               id_column=\"transcript_id\",\n",
    "                               label_column=\"client_talk_type\",\n",
    "                               embeddings=embeddings,\n",
    "                               embeddings_reduced=embeddings_reduced)\n",
    "    \n",
    "    paths.pad(**path_specifics)\n",
    "    \n",
    "    paths.array_padded = paths.array_padded[client_index]\n",
    "    paths.embeddings = paths.embeddings[client_index]\n",
    "    paths.embeddings_reduced = paths.embeddings_reduced[client_index]\n",
    "    \n",
    "    return paths.get_torch_path_for_SDSN(\n",
    "        include_time_features_in_path=True,\n",
    "        include_time_features_in_input=True,\n",
    "        include_embedding_in_input=True,\n",
    "        reduced_embeddings=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "c88d629a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def implement_sdsn(x_data,\n",
    "                   y_data,\n",
    "                   sig_depth,\n",
    "                   input_channels,\n",
    "                   output_channels,\n",
    "                   lstm_hidden_dim,\n",
    "                   ffn_hidden_dim,\n",
    "                   BiLSTM,\n",
    "                   learning_rate,\n",
    "                   loss,\n",
    "                   gamma = 0):\n",
    "    SDSN_args = {\n",
    "        \"input_channels\": input_channels,\n",
    "        \"output_channels\": output_channels,\n",
    "        \"num_time_features\": len(time_features),\n",
    "        \"embedding_dim\": x_data.shape[2]-input_channels-len(time_features),\n",
    "        \"sig_depth\": sig_depth,\n",
    "        \"hidden_dim_lstm\": lstm_hidden_dim,\n",
    "        \"hidden_dim_ffn\": ffn_hidden_dim,\n",
    "        \"output_dim\": len(label_to_id),\n",
    "        \"dropout_rate\": 0.1,\n",
    "        \"augmentation_type\": \"Conv1d\",\n",
    "        \"BiLSTM\": BiLSTM,\n",
    "        \"comb_method\": \"concatenation\"\n",
    "    }\n",
    "    \n",
    "    sdsn_model = StackedDeepSigNet(**SDSN_args)\n",
    "    # print(sdsn_model)\n",
    "    \n",
    "    # split dataset\n",
    "    train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n",
    "                                       y_data=torch.tensor(y_data),\n",
    "                                       train_size=0.8,\n",
    "                                       valid_size=0.5,\n",
    "                                       shuffle=True,\n",
    "                                       as_DataLoader=True,\n",
    "                                       seed=seed)\n",
    "    \n",
    "    # define loss\n",
    "    if loss == \"focal\":    \n",
    "        criterion = FocalLoss(gamma = gamma)\n",
    "    elif loss == \"cross_entropy\":\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # define optimizer\n",
    "    optimizer = torch.optim.Adam(sdsn_model.parameters(), lr=learning_rate)\n",
    "    # define scheduler for adjusting the learning rate\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "    # scheduler = StepLR(optimizer, step_size = 10, gamma = 0.5)\n",
    "    # scheduler = CosineAnnealingWarmRestarts(optimizer, \n",
    "    #                                         T_0 = 8,# Number of iterations for the first restart\n",
    "    #                                         T_mult = 1, # A factor increases TiTi after a restart\n",
    "    #                                         eta_min = learning_rate*0.1)\n",
    "    # scheduler = None\n",
    "    \n",
    "    sdsn_model = training_pytorch(model=sdsn_model,\n",
    "                                  train_loader=train,\n",
    "                                  criterion=criterion,\n",
    "                                  optimizer=optimizer,\n",
    "                                  num_epochs=10000,\n",
    "                                  scheduler=scheduler,\n",
    "                                  valid_loader=valid,\n",
    "                                  early_stopping=True,\n",
    "                                  early_stopping_metric=\"f1\",\n",
    "                                  patience=100,\n",
    "                                  verbose=True,\n",
    "                                  verbose_epoch=100,\n",
    "                                  seed=seed)\n",
    "\n",
    "    pred, label = testing_pytorch(sdsn_model, test, criterion)\n",
    "    print(f\"proportion of labels in prediction: {[sum(pred==i)/len(pred) for i in label_to_id.values()]}\")\n",
    "    print(f\"proportion of labels in data: {[sum(label==i)/len(label) for i in label_to_id.values()]}\")\n",
    "    \n",
    "    f1_scores = metrics.f1_score(label, pred, average=None)\n",
    "    print(f\"- f1: {f1_scores}\")\n",
    "    print(f\"- f1 (average): {sum(f1_scores)/len(f1_scores)}\")\n",
    "    print(f\"- accuracy: {sum(pred==label)/len(pred)}\")\n",
    "    \n",
    "    return sdsn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "58e319ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_hidden_dim_trial = [[8,8], [12,12,8], [12,12,12,8]]\n",
    "ffn_hidden_dim_trial = [[100]*i for i in range(2, 6)]\n",
    "sig_depth = 3\n",
    "output_channels = 10\n",
    "BiLSTM = True\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ec0e9c",
   "metadata": {},
   "source": [
    "## SBERT 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "7667376a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c5048617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import signatory\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class StackedDeepSigNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Stacked Deep Signature Neural Network for classification.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels: int,\n",
    "        output_channels: int,\n",
    "        num_time_features: int,\n",
    "        embedding_dim: int,\n",
    "        sig_depth: int,\n",
    "        hidden_dim_lstm: list[int] | int,\n",
    "        hidden_dim_ffn: list[int] | int,\n",
    "        output_dim: int,\n",
    "        dropout_rate: float,\n",
    "        augmentation_type: str = \"Conv1d\",\n",
    "        augmentation_args: dict | None = None,\n",
    "        hidden_dim_aug: list[int] | int | None = None,\n",
    "        BiLSTM: bool = False,\n",
    "        comb_method: str = \"gated_addition\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Stacked Deep Signature Neural Network for classification.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_channels : int\n",
    "            Dimension of the embeddings that will be passed in.\n",
    "        output_channels : int\n",
    "            Requested dimension of the embeddings after convolution layer.\n",
    "        num_time_features : int\n",
    "            Number of time features to add to FFN input. If none, set to zero.\n",
    "        embedding_dim : int\n",
    "            Dimension of embedding to add to FFN input. If none, set to zero.\n",
    "        sig_depth : int\n",
    "            The depth to truncate the path signature at.\n",
    "        hidden_dim_lstm : list[int] | int\n",
    "            Dimensions of the hidden layers in the LSTM blocks.\n",
    "        hidden_dim_ffn : list[int] | int\n",
    "            Dimension of the hidden layers in the FFN.\n",
    "        output_dim : int\n",
    "            Dimension of the output layer in the FFN.\n",
    "        dropout_rate : float\n",
    "            Dropout rate in the FFN.\n",
    "        augmentation_type : str, optional\n",
    "            Method of augmenting the path, by default \"Conv1d\".\n",
    "            Options are:\n",
    "            - \"Conv1d\": passes path through 1D convolution layer.\n",
    "            - \"signatory\": passes path through `Augment` layer from `signatory` package.\n",
    "        augmentation_args : dict | None, optional\n",
    "            Arguments to pass into `torch.Conv1d` or `signatory.Augment`, by default None.\n",
    "            If None, by default will set `kernel_size=3`, `stride=1`, `padding=0`.\n",
    "        hidden_dim_aug : list[int] | int | None\n",
    "            Dimensions of the hidden layers in the augmentation layer.\n",
    "            Passed into `Augment` class from `signatory` package if\n",
    "            `augmentation_type='signatory'`, by default None.\n",
    "        BiLSTM : bool, optional\n",
    "            Whether or not a birectional LSTM is used,\n",
    "            by default False (unidirectional LSTM is used in this case).\n",
    "        comb_method : str, optional\n",
    "            Determines how to combine the path signature and embeddings,\n",
    "            by default \"gated_addition\".\n",
    "            Options are:\n",
    "            - concatenation: concatenation of path signature and embedding vector\n",
    "            - gated_addition: element-wise addition of path signature and embedding vector\n",
    "        \"\"\"\n",
    "        super(StackedDeepSigNet, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        \n",
    "        if isinstance(hidden_dim_lstm, int):\n",
    "            hidden_dim_lstm = [hidden_dim_lstm]\n",
    "        if isinstance(hidden_dim_ffn, int):\n",
    "            hidden_dim_ffn = [hidden_dim_ffn]\n",
    "        self.hidden_dim_lstm = hidden_dim_lstm\n",
    "        self.hidden_dim_ffn = hidden_dim_ffn\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_time_features = num_time_features\n",
    "        if comb_method not in [\"concatenation\", \"gated_addition\"]:\n",
    "            raise ValueError(\n",
    "                \"`comb_method` must be either 'concatenation' or 'gated_addition'.\"\n",
    "            )\n",
    "        self.comb_method = comb_method\n",
    "        if augmentation_type not in [\"Conv1d\", \"signatory\"]:\n",
    "            raise ValueError(\"`augmentation_type` must be 'Conv1d' or 'signatory'.\")\n",
    "        \n",
    "        self.augmentation_type = augmentation_type\n",
    "        if isinstance(hidden_dim_aug, int):\n",
    "            hidden_dim_aug = [hidden_dim_aug]\n",
    "        elif hidden_dim_aug is None:\n",
    "            hidden_dim_aug = []\n",
    "        self.hidden_dim_aug = hidden_dim_aug\n",
    "        if augmentation_args is None:\n",
    "            augmentation_args = {\"kernel_size\": 3,\n",
    "                                 \"stride\": 1,\n",
    "                                 \"padding\": 1}\n",
    "        # convolution\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=input_channels,\n",
    "            out_channels=output_channels,\n",
    "            **augmentation_args,\n",
    "        )\n",
    "        self.augment = signatory.Augment(\n",
    "            in_channels=input_channels,\n",
    "            layer_sizes=self.hidden_dim_aug + [output_channels],\n",
    "            include_original=False,\n",
    "            include_time=False,\n",
    "            **augmentation_args,\n",
    "        )\n",
    "        # non-linearity\n",
    "        self.tanh1 = nn.Tanh()\n",
    "\n",
    "        self.signature_layers = []\n",
    "        self.lstm_layers = []\n",
    "        for l in range(len(self.hidden_dim_lstm)):\n",
    "            self.signature_layers.append(signatory.LogSignature(depth=sig_depth, stream=True))\n",
    "            if l == 0:\n",
    "                input_dim_lstm = signatory.logsignature_channels(output_channels, sig_depth)\n",
    "            else:\n",
    "                input_dim_lstm = signatory.logsignature_channels(self.hidden_dim_lstm[l-1], sig_depth)\n",
    "            self.lstm_layers.append(nn.LSTM(\n",
    "                input_size=input_dim_lstm,\n",
    "                hidden_size=self.hidden_dim_lstm[l],\n",
    "                num_layers=1,\n",
    "                batch_first=True,\n",
    "                bidirectional=False if l!=(len(self.hidden_dim_lstm)-1) else BiLSTM,\n",
    "            ))\n",
    "        \n",
    "        self.signature_layers = nn.ModuleList(self.signature_layers)\n",
    "        self.lstm_layers = nn.ModuleList(self.lstm_layers)\n",
    "\n",
    "        # signature without lift (for passing into FFN)\n",
    "        mult = 2 if BiLSTM else 1\n",
    "        self.signature2 = signatory.LogSignature(depth=sig_depth, stream=False)\n",
    "\n",
    "        # find dimension of features to pass through FFN\n",
    "        if self.comb_method == \"concatenation\":\n",
    "            input_dim = (\n",
    "                signatory.logsignature_channels(\n",
    "                    in_channels=mult * self.hidden_dim_lstm[-1], depth=sig_depth\n",
    "                )\n",
    "                + self.num_time_features\n",
    "                + self.embedding_dim\n",
    "            )\n",
    "        elif self.comb_method == \"gated_addition\":\n",
    "            input_dim = self.embedding_dim\n",
    "            input_gated_linear = (\n",
    "                signatory.logsignature_channels(\n",
    "                    in_channels=mult * self.hidden_dim_lstm[-1], depth=sig_depth\n",
    "                )\n",
    "                + self.num_time_features\n",
    "            )\n",
    "            if self.embedding_dim > 0:\n",
    "                self.fc_scale = nn.Linear(input_gated_linear, self.embedding_dim)\n",
    "                self.scaler = torch.nn.Parameter(torch.zeros(1, self.embedding_dim))\n",
    "            else:\n",
    "                self.fc_scale = nn.Linear(input_gated_linear, input_gated_linear)\n",
    "                self.scaler = torch.nn.Parameter(torch.zeros(1, input_gated_linear))\n",
    "            # non-linearity\n",
    "            self.tanh2 = nn.Tanh()\n",
    "\n",
    "        # FFN: input layer\n",
    "        self.ffn_input_layer = nn.Linear(input_dim, self.hidden_dim_ffn[0])\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        input_dim = self.hidden_dim_ffn[0]\n",
    "        \n",
    "        # FFN: hidden layers\n",
    "        self.ffn_linear_layers = []\n",
    "        self.ffn_non_linear_layers = []\n",
    "        self.dropout_layers = []\n",
    "        for l in range(len(self.hidden_dim_ffn)):\n",
    "            self.ffn_linear_layers.append(nn.Linear(input_dim, self.hidden_dim_ffn[l]))\n",
    "            self.ffn_non_linear_layers.append(nn.ReLU())\n",
    "            self.dropout_layers.append(nn.Dropout(dropout_rate))\n",
    "            input_dim = self.hidden_dim_ffn[l]\n",
    "        \n",
    "        self.ffn_linear_layers = nn.ModuleList(self.ffn_linear_layers)\n",
    "        self.ffn_non_linear_layers = nn.ModuleList(self.ffn_non_linear_layers)\n",
    "        self.dropout_layers = nn.ModuleList(self.dropout_layers)\n",
    "        \n",
    "        # FFN: readout\n",
    "        self.ffn_final_layer = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x has dimensions [batch, length of signal, channels]\n",
    "        \n",
    "        print(f\"input size: {x.shape}\")\n",
    "        \n",
    "        # convolution\n",
    "        if self.augmentation_type == \"Conv1d\":\n",
    "            # input has dimensions [batch, length of signal, channels]\n",
    "            # swap dimensions to get [batch, channels, length of signal]\n",
    "            # (nn.Conv1d expects this)\n",
    "            out = torch.transpose(x, 1, 2)\n",
    "            # get only the path information\n",
    "            out = self.conv(out[:, : self.input_channels, :])\n",
    "            out = self.tanh1(out)\n",
    "            # make output have dimensions [batch, length of signal, channels]\n",
    "            out = torch.transpose(out, 1, 2)\n",
    "        elif self.augmentation_type == \"signatory\":\n",
    "            # input has dimensions [batch, length of signal, channels]\n",
    "            # (signatory.Augment expects this)\n",
    "            # and get only the path information\n",
    "            # output has dimensions [batch, length of signal, channels]\n",
    "            out = self.augment(x[:, :, : self.input_channels])\n",
    "\n",
    "        print(f\"after conv: {out.shape}\")\n",
    "        \n",
    "        # take signature lifts and lstm\n",
    "        for l in range(len(self.hidden_dim_lstm)):\n",
    "            out = self.signature_layers[l](out)\n",
    "            print(f\"after signature: {out.shape}\")\n",
    "            out, _ = self.lstm_layers[l](out)\n",
    "            print(f\"after lstm: {out.shape}\")\n",
    "        \n",
    "        print(f\"after snwu: {out.shape}\")\n",
    "        # signature\n",
    "        out = self.signature2(out)\n",
    "        print(f\"after last signature: {out.shape}\")\n",
    "\n",
    "        # combine last post embedding\n",
    "        if x.shape[2] > self.input_channels:\n",
    "            # we have things to concatenate to the path\n",
    "            if self.comb_method == \"concatenation\":\n",
    "                if self.num_time_features > 0:\n",
    "                    # concatenate any time features\n",
    "                    # take the maximum for the latest time\n",
    "                    out = torch.cat(\n",
    "                        (\n",
    "                            out,\n",
    "                            x[\n",
    "                                :,\n",
    "                                :,\n",
    "                                self.input_channels : (\n",
    "                                    self.input_channels + self.num_time_features\n",
    "                                ),\n",
    "                            ].max(1)[0],\n",
    "                        ),\n",
    "                        dim=1,\n",
    "                    )\n",
    "                if x.shape[2] > self.input_channels + self.num_time_features:\n",
    "                    # concatenate current post embedding if provided\n",
    "                    out = torch.cat(\n",
    "                        (\n",
    "                            out,\n",
    "                            x[:, 0, (self.input_channels + self.num_time_features) :],\n",
    "                        ),\n",
    "                        dim=1,\n",
    "                    )\n",
    "            elif self.comb_method == \"gated_addition\":\n",
    "                if self.num_time_features > 0:\n",
    "                    # concatenate any time features\n",
    "                    out_gated = torch.cat(\n",
    "                        (\n",
    "                            out,\n",
    "                            x[\n",
    "                                :,\n",
    "                                :,\n",
    "                                self.input_channels : (\n",
    "                                    self.input_channels + self.num_time_features\n",
    "                                ),\n",
    "                            ].max(1)[0],\n",
    "                        ),\n",
    "                        dim=1,\n",
    "                    )\n",
    "                else:\n",
    "                    out_gated = out\n",
    "                out_gated = self.fc_scale(out_gated.float())\n",
    "                out_gated = self.tanh2(out_gated)\n",
    "                out_gated = torch.mul(self.scaler, out_gated)\n",
    "                if x.shape[2] > self.input_channels + self.num_time_features:\n",
    "                    # concatenate current post embedding if provided\n",
    "                    out = (\n",
    "                        out_gated\n",
    "                        + x[:, 0, (self.input_channels + self.num_time_features) :],\n",
    "                    )\n",
    "                else:\n",
    "                    out = out_gated\n",
    "\n",
    "        # FFN: input layer\n",
    "        out = self.ffn_input_layer(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # FFN: hidden layers    \n",
    "        for l in range(len(self.hidden_dim_ffn)):\n",
    "            out = self.ffn_linear_layers[l](out)\n",
    "            out = self.ffn_non_linear_layers[l](out)\n",
    "            out = self.dropout_layers[l](out)\n",
    "\n",
    "        # FFN: readout\n",
    "        out = self.ffn_final_layer(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "2e327ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import signatory\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class StackedDeepSigNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Stacked Deep Signature Neural Network for classification.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels: int,\n",
    "        output_channels: int,\n",
    "        num_time_features: int,\n",
    "        embedding_dim: int,\n",
    "        sig_depth: int,\n",
    "        hidden_dim_lstm: list[int] | int,\n",
    "        hidden_dim_ffn: list[int] | int,\n",
    "        output_dim: int,\n",
    "        dropout_rate: float,\n",
    "        augmentation_type: str = \"Conv1d\",\n",
    "        augmentation_args: dict | None = None,\n",
    "        hidden_dim_aug: list[int] | int | None = None,\n",
    "        BiLSTM: bool = False,\n",
    "        comb_method: str = \"gated_addition\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Stacked Deep Signature Neural Network for classification.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_channels : int\n",
    "            Dimension of the embeddings that will be passed in.\n",
    "        output_channels : int\n",
    "            Requested dimension of the embeddings after convolution layer.\n",
    "        num_time_features : int\n",
    "            Number of time features to add to FFN input. If none, set to zero.\n",
    "        embedding_dim : int\n",
    "            Dimension of embedding to add to FFN input. If none, set to zero.\n",
    "        sig_depth : int\n",
    "            The depth to truncate the path signature at.\n",
    "        hidden_dim_lstm : list[int] | int\n",
    "            Dimensions of the hidden layers in the LSTM blocks.\n",
    "        hidden_dim_ffn : list[int] | int\n",
    "            Dimension of the hidden layers in the FFN.\n",
    "        output_dim : int\n",
    "            Dimension of the output layer in the FFN.\n",
    "        dropout_rate : float\n",
    "            Dropout rate in the FFN.\n",
    "        augmentation_type : str, optional\n",
    "            Method of augmenting the path, by default \"Conv1d\".\n",
    "            Options are:\n",
    "            - \"Conv1d\": passes path through 1D convolution layer.\n",
    "            - \"signatory\": passes path through `Augment` layer from `signatory` package.\n",
    "        augmentation_args : dict | None, optional\n",
    "            Arguments to pass into `torch.Conv1d` or `signatory.Augment`, by default None.\n",
    "            If None, by default will set `kernel_size=3`, `stride=1`, `padding=0`.\n",
    "        hidden_dim_aug : list[int] | int | None\n",
    "            Dimensions of the hidden layers in the augmentation layer.\n",
    "            Passed into `Augment` class from `signatory` package if\n",
    "            `augmentation_type='signatory'`, by default None.\n",
    "        BiLSTM : bool, optional\n",
    "            Whether or not a birectional LSTM is used,\n",
    "            by default False (unidirectional LSTM is used in this case).\n",
    "        comb_method : str, optional\n",
    "            Determines how to combine the path signature and embeddings,\n",
    "            by default \"gated_addition\".\n",
    "            Options are:\n",
    "            - concatenation: concatenation of path signature and embedding vector\n",
    "            - gated_addition: element-wise addition of path signature and embedding vector\n",
    "        \"\"\"\n",
    "        super(StackedDeepSigNet, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        \n",
    "        if isinstance(hidden_dim_lstm, int):\n",
    "            hidden_dim_lstm = [hidden_dim_lstm]\n",
    "        if isinstance(hidden_dim_ffn, int):\n",
    "            hidden_dim_ffn = [hidden_dim_ffn]\n",
    "        self.hidden_dim_lstm = hidden_dim_lstm\n",
    "        self.hidden_dim_ffn = hidden_dim_ffn\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_time_features = num_time_features\n",
    "        if comb_method not in [\"concatenation\", \"gated_addition\"]:\n",
    "            raise ValueError(\n",
    "                \"`comb_method` must be either 'concatenation' or 'gated_addition'.\"\n",
    "            )\n",
    "        self.comb_method = comb_method\n",
    "        if augmentation_type not in [\"Conv1d\", \"signatory\"]:\n",
    "            raise ValueError(\"`augmentation_type` must be 'Conv1d' or 'signatory'.\")\n",
    "        \n",
    "        self.augmentation_type = augmentation_type\n",
    "        if isinstance(hidden_dim_aug, int):\n",
    "            hidden_dim_aug = [hidden_dim_aug]\n",
    "        elif hidden_dim_aug is None:\n",
    "            hidden_dim_aug = []\n",
    "        self.hidden_dim_aug = hidden_dim_aug\n",
    "        if augmentation_args is None:\n",
    "            augmentation_args = {\"kernel_size\": 3,\n",
    "                                 \"stride\": 1,\n",
    "                                 \"padding\": 1}\n",
    "        # convolution\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=input_channels,\n",
    "            out_channels=output_channels,\n",
    "            **augmentation_args,\n",
    "        )\n",
    "        self.augment = signatory.Augment(\n",
    "            in_channels=input_channels,\n",
    "            layer_sizes=self.hidden_dim_aug + [output_channels],\n",
    "            include_original=False,\n",
    "            include_time=False,\n",
    "            **augmentation_args,\n",
    "        )\n",
    "        # non-linearity\n",
    "        self.tanh1 = nn.Tanh()\n",
    "\n",
    "        self.signature_layers = []\n",
    "        self.lstm_layers = []\n",
    "        for l in range(len(self.hidden_dim_lstm)):\n",
    "            self.signature_layers.append(signatory.LogSignature(depth=sig_depth, stream=True))\n",
    "            if l == 0:\n",
    "                input_dim_lstm = signatory.logsignature_channels(output_channels, sig_depth)\n",
    "            else:\n",
    "                input_dim_lstm = signatory.logsignature_channels(self.hidden_dim_lstm[l-1], sig_depth)\n",
    "            self.lstm_layers.append(nn.LSTM(\n",
    "                input_size=input_dim_lstm,\n",
    "                hidden_size=self.hidden_dim_lstm[l],\n",
    "                num_layers=1,\n",
    "                batch_first=True,\n",
    "                bidirectional=False if l!=(len(self.hidden_dim_lstm)-1) else BiLSTM,\n",
    "            ))\n",
    "        \n",
    "        self.signature_layers = nn.ModuleList(self.signature_layers)\n",
    "        self.lstm_layers = nn.ModuleList(self.lstm_layers)\n",
    "\n",
    "        # signature without lift (for passing into FFN)\n",
    "        mult = 2 if BiLSTM else 1\n",
    "        self.signature2 = signatory.LogSignature(depth=sig_depth, stream=False)\n",
    "\n",
    "        # find dimension of features to pass through FFN\n",
    "        if self.comb_method == \"concatenation\":\n",
    "            input_dim = (\n",
    "                signatory.logsignature_channels(\n",
    "                    in_channels=mult * self.hidden_dim_lstm[-1], depth=sig_depth\n",
    "                )\n",
    "                + self.num_time_features\n",
    "                + self.embedding_dim\n",
    "            )\n",
    "        elif self.comb_method == \"gated_addition\":\n",
    "            input_dim = self.embedding_dim\n",
    "            input_gated_linear = (\n",
    "                signatory.logsignature_channels(\n",
    "                    in_channels=mult * self.hidden_dim_lstm[-1], depth=sig_depth\n",
    "                )\n",
    "                + self.num_time_features\n",
    "            )\n",
    "            if self.embedding_dim > 0:\n",
    "                self.fc_scale = nn.Linear(input_gated_linear, self.embedding_dim)\n",
    "                self.scaler = torch.nn.Parameter(torch.zeros(1, self.embedding_dim))\n",
    "            else:\n",
    "                self.fc_scale = nn.Linear(input_gated_linear, input_gated_linear)\n",
    "                self.scaler = torch.nn.Parameter(torch.zeros(1, input_gated_linear))\n",
    "            # non-linearity\n",
    "            self.tanh2 = nn.Tanh()\n",
    "\n",
    "        # FFN: input layer\n",
    "        self.ffn_input_layer = nn.Linear(input_dim, self.hidden_dim_ffn[0])\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        input_dim = self.hidden_dim_ffn[0]\n",
    "        \n",
    "        # FFN: hidden layers\n",
    "        self.ffn_linear_layers = []\n",
    "        self.ffn_non_linear_layers = []\n",
    "        self.dropout_layers = []\n",
    "        for l in range(len(self.hidden_dim_ffn)):\n",
    "            self.ffn_linear_layers.append(nn.Linear(input_dim, self.hidden_dim_ffn[l]))\n",
    "            self.ffn_non_linear_layers.append(nn.ReLU())\n",
    "            self.dropout_layers.append(nn.Dropout(dropout_rate))\n",
    "            input_dim = self.hidden_dim_ffn[l]\n",
    "        \n",
    "        self.ffn_linear_layers = nn.ModuleList(self.ffn_linear_layers)\n",
    "        self.ffn_non_linear_layers = nn.ModuleList(self.ffn_non_linear_layers)\n",
    "        self.dropout_layers = nn.ModuleList(self.dropout_layers)\n",
    "        \n",
    "        # FFN: readout\n",
    "        self.ffn_final_layer = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x has dimensions [batch, length of signal, channels]\n",
    "\n",
    "        # convolution\n",
    "        if self.augmentation_type == \"Conv1d\":\n",
    "            # input has dimensions [batch, length of signal, channels]\n",
    "            # swap dimensions to get [batch, channels, length of signal]\n",
    "            # (nn.Conv1d expects this)\n",
    "            out = torch.transpose(x, 1, 2)\n",
    "            # get only the path information\n",
    "            out = self.conv(out[:, : self.input_channels, :])\n",
    "            out = self.tanh1(out)\n",
    "            # make output have dimensions [batch, length of signal, channels]\n",
    "            out = torch.transpose(out, 1, 2)\n",
    "        elif self.augmentation_type == \"signatory\":\n",
    "            # input has dimensions [batch, length of signal, channels]\n",
    "            # (signatory.Augment expects this)\n",
    "            # and get only the path information\n",
    "            # output has dimensions [batch, length of signal, channels]\n",
    "            out = self.augment(x[:, :, : self.input_channels])\n",
    "\n",
    "        # take signature lifts and lstm\n",
    "        for l in range(len(self.hidden_dim_lstm)):\n",
    "            out = self.signature_layers[l](out)\n",
    "            out, _ = self.lstm_layers[l](out)\n",
    "        \n",
    "        # signature\n",
    "        out = self.signature2(out)\n",
    "\n",
    "        # combine last post embedding\n",
    "        if x.shape[2] > self.input_channels:\n",
    "            # we have things to concatenate to the path\n",
    "            if self.comb_method == \"concatenation\":\n",
    "                if self.num_time_features > 0:\n",
    "                    # concatenate any time features\n",
    "                    # take the maximum for the latest time\n",
    "                    out = torch.cat(\n",
    "                        (\n",
    "                            out,\n",
    "                            x[\n",
    "                                :,\n",
    "                                :,\n",
    "                                self.input_channels : (\n",
    "                                    self.input_channels + self.num_time_features\n",
    "                                ),\n",
    "                            ].max(1)[0],\n",
    "                        ),\n",
    "                        dim=1,\n",
    "                    )\n",
    "                if x.shape[2] > self.input_channels + self.num_time_features:\n",
    "                    # concatenate current post embedding if provided\n",
    "                    out = torch.cat(\n",
    "                        (\n",
    "                            out,\n",
    "                            x[:, 0, (self.input_channels + self.num_time_features) :],\n",
    "                        ),\n",
    "                        dim=1,\n",
    "                    )\n",
    "            elif self.comb_method == \"gated_addition\":\n",
    "                if self.num_time_features > 0:\n",
    "                    # concatenate any time features\n",
    "                    out_gated = torch.cat(\n",
    "                        (\n",
    "                            out,\n",
    "                            x[\n",
    "                                :,\n",
    "                                :,\n",
    "                                self.input_channels : (\n",
    "                                    self.input_channels + self.num_time_features\n",
    "                                ),\n",
    "                            ].max(1)[0],\n",
    "                        ),\n",
    "                        dim=1,\n",
    "                    )\n",
    "                else:\n",
    "                    out_gated = out\n",
    "                out_gated = self.fc_scale(out_gated.float())\n",
    "                out_gated = self.tanh2(out_gated)\n",
    "                out_gated = torch.mul(self.scaler, out_gated)\n",
    "                if x.shape[2] > self.input_channels + self.num_time_features:\n",
    "                    # concatenate current post embedding if provided\n",
    "                    out = (\n",
    "                        out_gated\n",
    "                        + x[:, 0, (self.input_channels + self.num_time_features) :],\n",
    "                    )\n",
    "                else:\n",
    "                    out = out_gated\n",
    "\n",
    "        # FFN: input layer\n",
    "        out = self.ffn_input_layer(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # FFN: hidden layers    \n",
    "        for l in range(len(self.hidden_dim_ffn)):\n",
    "            out = self.ffn_linear_layers[l](out)\n",
    "            out = self.ffn_non_linear_layers[l](out)\n",
    "            out = self.dropout_layers[l](out)\n",
    "\n",
    "        # FFN: readout\n",
    "        out = self.ffn_final_layer(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "57cabd51",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] - columns beginning with 'd' denote the dimension reduced embeddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Adding 'time_encoding' and feature...\n",
      "[INFO] Adding 'time_diff' and feature...\n",
      "[INFO] Adding 'timeline_index' feature...\n",
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6608ba95581643c3ac13b49ead977498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] The path was created for each item in the dataframe, by looking at its history, so to include embeddings in the FFN input, we concatenate the embeddings for each sentence / text.\n",
      "\n",
      "********** lstm_hidden_dim: [8, 8] || ffnhidden_dim: [100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f632d409835c4adfa1144c8366371461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.265817642211914\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0676361322402954\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9867492589083585 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.721148669719696\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.2635515034198761\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.6812250993468545 || Accuracy: 0.7043090462684631 || F1-score: 0.5708052355774097\n",
      "Early stopping at epoch 143!\n",
      "Accuracy on dataset of size 672: 70.98213958740234 %.\n",
      "Average loss: 0.7221233248710632\n",
      "proportion of labels in prediction: [tensor(0.7411), tensor(0.1696), tensor(0.0893)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.81570338 0.52272727 0.41717791]\n",
      "- f1 (average): 0.5852028558088597\n",
      "- accuracy: 0.7098214030265808\n",
      "\n",
      "********** lstm_hidden_dim: [8, 8] || ffnhidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c17726238e394a45827a9a53b0cb670a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.088348150253296\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0833232402801514\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0117262439294294 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.7248660922050476\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.12598766386508942\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.6911819739775225 || Accuracy: 0.7087666988372803 || F1-score: 0.5776151579459678\n",
      "Early stopping at epoch 146!\n",
      "Accuracy on dataset of size 672: 72.17262268066406 %.\n",
      "Average loss: 0.7211203033273871\n",
      "proportion of labels in prediction: [tensor(0.7351), tensor(0.1592), tensor(0.1057)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.82146769 0.52918288 0.48275862]\n",
      "- f1 (average): 0.6111363963348851\n",
      "- accuracy: 0.7217261791229248\n",
      "\n",
      "********** lstm_hidden_dim: [8, 8] || ffnhidden_dim: [100, 100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdfe7c9773ca45abacabe966a2d232d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0947375297546387\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0724949836730957\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0100325345993042 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.6952031850814819\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.0778966173529625\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7145196145231073 || Accuracy: 0.710252583026886 || F1-score: 0.5707901727575452\n",
      "Early stopping at epoch 149!\n",
      "Accuracy on dataset of size 672: 70.83333587646484 %.\n",
      "Average loss: 0.7661464701999318\n",
      "proportion of labels in prediction: [tensor(0.7351), tensor(0.1562), tensor(0.1086)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.81270537 0.50980392 0.45454545]\n",
      "- f1 (average): 0.5923515810121054\n",
      "- accuracy: 0.7083333134651184\n",
      "\n",
      "********** lstm_hidden_dim: [8, 8] || ffnhidden_dim: [100, 100, 100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91efa1b9a4194b30839cab675116d1f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.092410922050476\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0839177370071411\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9791089675643228 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.5057088732719421\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.7203328609466553\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7227513627572493 || Accuracy: 0.7072808146476746 || F1-score: 0.5747369384667627\n",
      "Early stopping at epoch 154!\n",
      "Accuracy on dataset of size 672: 72.02381134033203 %.\n",
      "Average loss: 0.7385878400369124\n",
      "proportion of labels in prediction: [tensor(0.7500), tensor(0.1518), tensor(0.0982)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.82340195 0.52380952 0.44970414]\n",
      "- f1 (average): 0.5989718719946239\n",
      "- accuracy: 0.7202380895614624\n",
      "\n",
      "********** lstm_hidden_dim: [12, 12, 8] || ffnhidden_dim: [100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3298b951d9b48e7b2ed3b4ef652fe87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.107893466949463\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0447618961334229\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9967642751607028 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.6416645646095276\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.23477709293365479\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.6964212222532793 || Accuracy: 0.7057949304580688 || F1-score: 0.57327064568848\n",
      "Early stopping at epoch 162!\n",
      "Accuracy on dataset of size 672: 72.32142639160156 %.\n",
      "Average loss: 0.733739587393674\n",
      "proportion of labels in prediction: [tensor(0.7321), tensor(0.1696), tensor(0.0982)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.82107574 0.54545455 0.47337278]\n",
      "- f1 (average): 0.6133010224878839\n",
      "- accuracy: 0.7232142686843872\n",
      "\n",
      "********** lstm_hidden_dim: [12, 12, 8] || ffnhidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ff1f21c56104626bcea72690bba5e5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.080686092376709\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0380885601043701\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9953892230987549 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.6616314649581909\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.2370094209909439\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7219040610573508 || Accuracy: 0.7087666988372803 || F1-score: 0.5653091474687719\n",
      "Early stopping at epoch 146!\n",
      "Accuracy on dataset of size 672: 70.38690185546875 %.\n",
      "Average loss: 0.7294310277158563\n",
      "proportion of labels in prediction: [tensor(0.7173), tensor(0.1801), tensor(0.1027)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.81243063 0.51660517 0.43023256]\n",
      "- f1 (average): 0.5864227856072021\n",
      "- accuracy: 0.7038690447807312\n",
      "\n",
      "********** lstm_hidden_dim: [12, 12, 8] || ffnhidden_dim: [100, 100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7899970f013c408796991eba08249e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1386523246765137\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.054408073425293\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0090482343326916 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.7550916075706482\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.07384757697582245\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7554692409255288 || Accuracy: 0.6968796253204346 || F1-score: 0.550924250528957\n",
      "Early stopping at epoch 183!\n",
      "Accuracy on dataset of size 672: 69.94047546386719 %.\n",
      "Average loss: 0.749945645982569\n",
      "proportion of labels in prediction: [tensor(0.7515), tensor(0.1533), tensor(0.0952)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.81385281 0.4743083  0.40718563]\n",
      "- f1 (average): 0.5651155809968619\n",
      "- accuracy: 0.699404776096344\n",
      "\n",
      "********** lstm_hidden_dim: [12, 12, 8] || ffnhidden_dim: [100, 100, 100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8d8a9fb549a49958e74956abca03a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0972087383270264\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0582512617111206\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9941164092584089 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.632483959197998\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.9149890542030334\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7416840249841864 || Accuracy: 0.6745913624763489 || F1-score: 0.42293048962640895\n",
      "Early stopping at epoch 132!\n",
      "Accuracy on dataset of size 672: 67.11309814453125 %.\n",
      "Average loss: 0.7763690244067799\n",
      "proportion of labels in prediction: [tensor(0.7738), tensor(0.2247), tensor(0.0015)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.80724175 0.4717608  0.01923077]\n",
      "- f1 (average): 0.4327444377039444\n",
      "- accuracy: 0.6711309552192688\n",
      "\n",
      "********** lstm_hidden_dim: [12, 12, 12, 8] || ffnhidden_dim: [100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_51169/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d81c85b3cbd4f5796b967c12559393d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Argument 'path' must have stream dimension of size at least 2. (Need at least this many points to define a path.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[180], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ffn_hidden_dim \u001b[38;5;129;01min\u001b[39;00m ffn_hidden_dim_trial:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m********** lstm_hidden_dim: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlstm_hidden_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|| ffnhidden_dim: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mffn_hidden_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mimplement_sdsn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                   \u001b[49m\u001b[43my_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                   \u001b[49m\u001b[43msig_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msig_depth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                   \u001b[49m\u001b[43minput_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                   \u001b[49m\u001b[43moutput_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mlstm_hidden_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlstm_hidden_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mffn_hidden_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mffn_hidden_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mBiLSTM\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBiLSTM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[175], line 56\u001b[0m, in \u001b[0;36mimplement_sdsn\u001b[0;34m(x_data, y_data, sig_depth, input_channels, output_channels, lstm_hidden_dim, ffn_hidden_dim, BiLSTM, learning_rate, loss, gamma)\u001b[0m\n\u001b[1;32m     48\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m ReduceLROnPlateau(optimizer, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# scheduler = StepLR(optimizer, step_size = 10, gamma = 0.5)\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# scheduler = CosineAnnealingWarmRestarts(optimizer, \u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m#                                         T_0 = 8,# Number of iterations for the first restart\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m#                                         T_mult = 1, # A factor increases TiTi after a restart\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m#                                         eta_min = learning_rate*0.1)\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# scheduler = None\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m sdsn_model \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_pytorch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msdsn_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m                              \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mearly_stopping_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mverbose_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m pred, label \u001b[38;5;241m=\u001b[39m testing_pytorch(sdsn_model, test, criterion)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproportion of labels in prediction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[\u001b[38;5;28msum\u001b[39m(pred\u001b[38;5;241m==\u001b[39mi)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(pred)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mi\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mlabel_to_id\u001b[38;5;241m.\u001b[39mvalues()]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-TheAlanTuringInstitute/rough_paths/nlpsig-networks/nlpsig_networks/pytorch_utils.py:185\u001b[0m, in \u001b[0;36mtraining_pytorch\u001b[0;34m(model, train_loader, criterion, optimizer, num_epochs, scheduler, valid_loader, seed, early_stopping, early_stopping_metric, patience, verbose, verbose_epoch, verbose_item)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# perform training by performing forward and backward passes\u001b[39;00m\n\u001b[1;32m    184\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 185\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43memb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m    187\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/nlpsig-networks/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[179], line 219\u001b[0m, in \u001b[0;36mStackedDeepSigNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    216\u001b[0m     out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm_layers[l](out)\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# signature\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;66;03m# combine last post embedding\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels:\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m# we have things to concatenate to the path\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/nlpsig-networks/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/nlpsig-networks/lib/python3.8/site-packages/signatory/logsignature_module.py:332\u001b[0m, in \u001b[0;36mLogSignature.forward\u001b[0;34m(self, path, basepoint)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: torch\u001b[38;5;241m.\u001b[39mTensor, basepoint: Union[\u001b[38;5;28mbool\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"The forward operation.\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \n\u001b[1;32m    323\u001b[0m \u001b[38;5;124;03m    Arguments:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;124;03m        As :func:`signatory.logsignature`.\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 332\u001b[0m     signature \u001b[38;5;241m=\u001b[39m \u001b[43msmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasepoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbasepoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43minverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_signature_to_logsignature_instance(path\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))(signature)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/nlpsig-networks/lib/python3.8/site-packages/signatory/signature_module.py:248\u001b[0m, in \u001b[0;36msignature\u001b[0;34m(path, depth, stream, basepoint, inverse, initial, scalar_term)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m initial \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m basepoint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    242\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minitial\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has been set but argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbasepoint\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has not. This is almost certainly a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistake. Argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbasepoint\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m should be set to the final value of the path whose signature is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    244\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minitial\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. See the documentation at\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    245\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    https://signatory.readthedocs.io/en/latest/pages/examples/online.html\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor more information.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 248\u001b[0m \u001b[43m_signature_checkargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasepoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscalar_term\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m result \u001b[38;5;241m=\u001b[39m _signature_batch_trick(path, depth, stream, basepoint, inverse, initial, scalar_term)\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# Either because we disabled use of the batch trick, or because the batch trick doesn't apply\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/nlpsig-networks/lib/python3.8/site-packages/signatory/signature_module.py:99\u001b[0m, in \u001b[0;36m_signature_checkargs\u001b[0;34m(path, depth, basepoint, initial, scalar_term)\u001b[0m\n\u001b[1;32m     97\u001b[0m basepoint, basepoint_value \u001b[38;5;241m=\u001b[39m interpret_basepoint(basepoint, path\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m), path\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), path\u001b[38;5;241m.\u001b[39mdtype, path\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     98\u001b[0m initial, initial_value \u001b[38;5;241m=\u001b[39m interpret_initial(initial)\n\u001b[0;32m---> 99\u001b[0m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature_checkargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasepoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasepoint_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscalar_term\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/nlpsig-networks/lib/python3.8/site-packages/signatory/impl.py:36\u001b[0m, in \u001b[0;36m_wrap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(e))\n",
      "\u001b[0;31mValueError\u001b[0m: Argument 'path' must have stream dimension of size at least 2. (Need at least this many points to define a path.)"
     ]
    }
   ],
   "source": [
    "x_data, input_channels = obtain_SDSN_input(sbert_768_embeddings, path_specifics)\n",
    "for lstm_hidden_dim in lstm_hidden_dim_trial:\n",
    "    for ffn_hidden_dim in ffn_hidden_dim_trial:\n",
    "        print(f\"\\n********** lstm_hidden_dim: {lstm_hidden_dim} \"\n",
    "              f\"|| ffnhidden_dim: {ffn_hidden_dim}\")\n",
    "        implement_sdsn(x_data=x_data,\n",
    "                       y_data=y_data,\n",
    "                       sig_depth=sig_depth,\n",
    "                       input_channels=input_channels,\n",
    "                       output_channels=output_channels,\n",
    "                       lstm_hidden_dim=lstm_hidden_dim,\n",
    "                       ffn_hidden_dim=ffn_hidden_dim,\n",
    "                       BiLSTM=BiLSTM,\n",
    "                       learning_rate=learning_rate,\n",
    "                       loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed6447e",
   "metadata": {},
   "source": [
    "## SBERT 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09df952b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_data, input_channels = obtain_SDSN_input(sbert_384_embeddings, path_specifics)\n",
    "for lstm_hidden_dim in lstm_hidden_dim_trial:\n",
    "    for ffn_hidden_dim in ffn_hidden_dim_trial:\n",
    "        print(f\"\\n********** lstm_hidden_dim: {lstm_hidden_dim} \"\n",
    "              f\"|| ffnhidden_dim: {ffn_hidden_dim}\")\n",
    "        implement_sdsn(x_data=x_data,\n",
    "                       y_data=y_data,\n",
    "                       sig_depth=sig_depth,\n",
    "                       input_channels=input_channels,\n",
    "                       output_channels=output_channels,\n",
    "                       lstm_hidden_dim=lstm_hidden_dim,\n",
    "                       ffn_hidden_dim=ffn_hidden_dim,\n",
    "                       BiLSTM=BiLSTM,\n",
    "                       learning_rate=learning_rate,\n",
    "                       loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64189f7",
   "metadata": {},
   "source": [
    "## Pretrained BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c43285a",
   "metadata": {},
   "source": [
    "### Mean pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7cd61e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_data, input_channels = obtain_SDSN_input(pooled_mean_pretrained, path_specifics)\n",
    "for lstm_hidden_dim in lstm_hidden_dim_trial:\n",
    "    for ffn_hidden_dim in ffn_hidden_dim_trial:\n",
    "        print(f\"\\n********** lstm_hidden_dim: {lstm_hidden_dim} \"\n",
    "              f\"|| ffnhidden_dim: {ffn_hidden_dim}\")\n",
    "        implement_sdsn(x_data=x_data,\n",
    "                       y_data=y_data,\n",
    "                       sig_depth=sig_depth,\n",
    "                       input_channels=input_channels,\n",
    "                       output_channels=output_channels,\n",
    "                       lstm_hidden_dim=lstm_hidden_dim,\n",
    "                       ffn_hidden_dim=ffn_hidden_dim,\n",
    "                       BiLSTM=BiLSTM,\n",
    "                       learning_rate=learning_rate,\n",
    "                       loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2270af66",
   "metadata": {},
   "source": [
    "### Max pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f400f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, input_channels = obtain_SDSN_input(pooled_max_pretrained, path_specifics)\n",
    "for lstm_hidden_dim in lstm_hidden_dim_trial:\n",
    "    for ffn_hidden_dim in ffn_hidden_dim_trial:\n",
    "        print(f\"\\n********** lstm_hidden_dim: {lstm_hidden_dim} \"\n",
    "              f\"|| ffnhidden_dim: {ffn_hidden_dim}\")\n",
    "        implement_sdsn(x_data=x_data,\n",
    "                       y_data=y_data,\n",
    "                       sig_depth=sig_depth,\n",
    "                       input_channels=input_channels,\n",
    "                       output_channels=output_channels,\n",
    "                       lstm_hidden_dim=lstm_hidden_dim,\n",
    "                       ffn_hidden_dim=ffn_hidden_dim,\n",
    "                       BiLSTM=BiLSTM,\n",
    "                       learning_rate=learning_rate,\n",
    "                       loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d65235a",
   "metadata": {},
   "source": [
    "### Sum pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a9591c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_data, input_channels = obtain_SDSN_input(pooled_sum_pretrained, path_specifics)\n",
    "for lstm_hidden_dim in lstm_hidden_dim_trial:\n",
    "    for ffn_hidden_dim in ffn_hidden_dim_trial:\n",
    "        print(f\"\\n********** lstm_hidden_dim: {lstm_hidden_dim} \"\n",
    "              f\"|| ffnhidden_dim: {ffn_hidden_dim}\")\n",
    "        implement_sdsn(x_data=x_data,\n",
    "                       y_data=y_data,\n",
    "                       sig_depth=sig_depth,\n",
    "                       input_channels=input_channels,\n",
    "                       output_channels=output_channels,\n",
    "                       lstm_hidden_dim=lstm_hidden_dim,\n",
    "                       ffn_hidden_dim=ffn_hidden_dim,\n",
    "                       BiLSTM=BiLSTM,\n",
    "                       learning_rate=learning_rate,\n",
    "                       loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23580589",
   "metadata": {},
   "source": [
    "### CLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68725b9b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_data, input_channels = obtain_SDSN_input(pooled_cls_pretrained, path_specifics)\n",
    "for lstm_hidden_dim in lstm_hidden_dim_trial:\n",
    "    for ffn_hidden_dim in ffn_hidden_dim_trial:\n",
    "        print(f\"\\n********** lstm_hidden_dim: {lstm_hidden_dim} \"\n",
    "              f\"|| ffnhidden_dim: {ffn_hidden_dim}\")\n",
    "        implement_sdsn(x_data=x_data,\n",
    "                       y_data=y_data,\n",
    "                       sig_depth=sig_depth,\n",
    "                       input_channels=input_channels,\n",
    "                       output_channels=output_channels,\n",
    "                       lstm_hidden_dim=lstm_hidden_dim,\n",
    "                       ffn_hidden_dim=ffn_hidden_dim,\n",
    "                       BiLSTM=BiLSTM,\n",
    "                       learning_rate=learning_rate,\n",
    "                       loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c31b98",
   "metadata": {},
   "source": [
    "## Fine-tuned BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181c0bc6",
   "metadata": {},
   "source": [
    "### Mean pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0bc651",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, input_channels = obtain_SDSN_input(pooled_mean, path_specifics)\n",
    "for lstm_hidden_dim in lstm_hidden_dim_trial:\n",
    "    for ffn_hidden_dim in ffn_hidden_dim_trial:\n",
    "        print(f\"\\n********** lstm_hidden_dim: {lstm_hidden_dim} \"\n",
    "              f\"|| ffnhidden_dim: {ffn_hidden_dim}\")\n",
    "        implement_sdsn(x_data=x_data,\n",
    "                       y_data=y_data,\n",
    "                       sig_depth=sig_depth,\n",
    "                       input_channels=input_channels,\n",
    "                       output_channels=output_channels,\n",
    "                       lstm_hidden_dim=lstm_hidden_dim,\n",
    "                       ffn_hidden_dim=ffn_hidden_dim,\n",
    "                       BiLSTM=BiLSTM,\n",
    "                       learning_rate=learning_rate,\n",
    "                       loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3535a4e2",
   "metadata": {},
   "source": [
    "### Max pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072a0e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, input_channels = obtain_SDSN_input(pooled_max, path_specifics)\n",
    "for lstm_hidden_dim in lstm_hidden_dim_trial:\n",
    "    for ffn_hidden_dim in ffn_hidden_dim_trial:\n",
    "        print(f\"\\n********** lstm_hidden_dim: {lstm_hidden_dim} \"\n",
    "              f\"|| ffnhidden_dim: {ffn_hidden_dim}\")\n",
    "        implement_sdsn(x_data=x_data,\n",
    "                       y_data=y_data,\n",
    "                       sig_depth=sig_depth,\n",
    "                       input_channels=input_channels,\n",
    "                       output_channels=output_channels,\n",
    "                       lstm_hidden_dim=lstm_hidden_dim,\n",
    "                       ffn_hidden_dim=ffn_hidden_dim,\n",
    "                       BiLSTM=BiLSTM,\n",
    "                       learning_rate=learning_rate,\n",
    "                       loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e9659e",
   "metadata": {},
   "source": [
    "### Sum pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52568870",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, input_channels = obtain_SDSN_input(pooled_sum, path_specifics)\n",
    "for lstm_hidden_dim in lstm_hidden_dim_trial:\n",
    "    for ffn_hidden_dim in ffn_hidden_dim_trial:\n",
    "        print(f\"\\n********** lstm_hidden_dim: {lstm_hidden_dim} \"\n",
    "              f\"|| ffnhidden_dim: {ffn_hidden_dim}\")\n",
    "        implement_sdsn(x_data=x_data,\n",
    "                       y_data=y_data,\n",
    "                       sig_depth=sig_depth,\n",
    "                       input_channels=input_channels,\n",
    "                       output_channels=output_channels,\n",
    "                       lstm_hidden_dim=lstm_hidden_dim,\n",
    "                       ffn_hidden_dim=ffn_hidden_dim,\n",
    "                       BiLSTM=BiLSTM,\n",
    "                       learning_rate=learning_rate,\n",
    "                       loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37820ff",
   "metadata": {},
   "source": [
    "### CLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0393550d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, input_channels = obtain_SDSN_input(pooled_cls, path_specifics)\n",
    "for lstm_hidden_dim in lstm_hidden_dim_trial:\n",
    "    for ffn_hidden_dim in ffn_hidden_dim_trial:\n",
    "        print(f\"\\n********** lstm_hidden_dim: {lstm_hidden_dim} \"\n",
    "              f\"|| ffnhidden_dim: {ffn_hidden_dim}\")\n",
    "        implement_sdsn(x_data=x_data,\n",
    "                       y_data=y_data,\n",
    "                       sig_depth=sig_depth,\n",
    "                       input_channels=input_channels,\n",
    "                       output_channels=output_channels,\n",
    "                       lstm_hidden_dim=lstm_hidden_dim,\n",
    "                       ffn_hidden_dim=ffn_hidden_dim,\n",
    "                       BiLSTM=BiLSTM,\n",
    "                       learning_rate=learning_rate,\n",
    "                       loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63ffede",
   "metadata": {},
   "source": [
    "Baselines:\n",
    "   - just looking at the sentence embeddings (encodes nothing about the history on the post)\n",
    "       - highlights importance of looking at the sequence\n",
    "   - averaging history\n",
    "   - comparing the cosine similarity between previous post and current post to see if switch\n",
    "   \n",
    "Test for:\n",
    "- How many posts do you need to look back?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpsig-networks",
   "language": "python",
   "name": "nlpsig-networks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
