{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a983a5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import nlpsig\n",
    "import nlpsig_networks\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "from nlpsig.classification_utils import split_dataset\n",
    "from nlpsig_networks.pytorch_utils import training_pytorch, testing_pytorch, set_seed\n",
    "from nlpsig_networks.ffn import FeedforwardNeuralNetModel\n",
    "from nlpsig_networks.deepsignet import StackedDeepSigNet\n",
    "from nlpsig_networks.focal_loss import FocalLoss, ClassBalanced_FocalLoss\n",
    "from sklearn import metrics\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "seed = 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e8fdb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import signatory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7409a03",
   "metadata": {},
   "source": [
    "## AnnoMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f00bb922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mi_quality</th>\n",
       "      <th>transcript_id</th>\n",
       "      <th>topic</th>\n",
       "      <th>utterance_id</th>\n",
       "      <th>interlocutor</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>utterance_text</th>\n",
       "      <th>annotator_id</th>\n",
       "      <th>therapist_input_exists</th>\n",
       "      <th>therapist_input_subtype</th>\n",
       "      <th>reflection_exists</th>\n",
       "      <th>reflection_subtype</th>\n",
       "      <th>question_exists</th>\n",
       "      <th>question_subtype</th>\n",
       "      <th>main_therapist_behaviour</th>\n",
       "      <th>client_talk_type</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>high</td>\n",
       "      <td>0</td>\n",
       "      <td>reducing alcohol consumption</td>\n",
       "      <td>0</td>\n",
       "      <td>therapist</td>\n",
       "      <td>00:00:13</td>\n",
       "      <td>Thanks for filling it out. We give this form t...</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>open</td>\n",
       "      <td>question</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-05-05 00:00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>high</td>\n",
       "      <td>0</td>\n",
       "      <td>reducing alcohol consumption</td>\n",
       "      <td>1</td>\n",
       "      <td>client</td>\n",
       "      <td>00:00:24</td>\n",
       "      <td>Sure.</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2023-05-05 00:00:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>high</td>\n",
       "      <td>0</td>\n",
       "      <td>reducing alcohol consumption</td>\n",
       "      <td>2</td>\n",
       "      <td>therapist</td>\n",
       "      <td>00:00:25</td>\n",
       "      <td>So, let's see. It looks that you put-- You dri...</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>information</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>therapist_input</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-05-05 00:00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>high</td>\n",
       "      <td>0</td>\n",
       "      <td>reducing alcohol consumption</td>\n",
       "      <td>3</td>\n",
       "      <td>client</td>\n",
       "      <td>00:00:34</td>\n",
       "      <td>Mm-hmm.</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2023-05-05 00:00:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>high</td>\n",
       "      <td>0</td>\n",
       "      <td>reducing alcohol consumption</td>\n",
       "      <td>4</td>\n",
       "      <td>therapist</td>\n",
       "      <td>00:00:34</td>\n",
       "      <td>-and you usually have three to four drinks whe...</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>information</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>therapist_input</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-05-05 00:00:34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mi_quality  transcript_id                         topic  utterance_id  \\\n",
       "0       high              0  reducing alcohol consumption             0   \n",
       "1       high              0  reducing alcohol consumption             1   \n",
       "2       high              0  reducing alcohol consumption             2   \n",
       "3       high              0  reducing alcohol consumption             3   \n",
       "4       high              0  reducing alcohol consumption             4   \n",
       "\n",
       "  interlocutor timestamp                                     utterance_text  \\\n",
       "0    therapist  00:00:13  Thanks for filling it out. We give this form t...   \n",
       "1       client  00:00:24                                              Sure.   \n",
       "2    therapist  00:00:25  So, let's see. It looks that you put-- You dri...   \n",
       "3       client  00:00:34                                            Mm-hmm.   \n",
       "4    therapist  00:00:34  -and you usually have three to four drinks whe...   \n",
       "\n",
       "   annotator_id therapist_input_exists therapist_input_subtype  \\\n",
       "0             3                  False                     NaN   \n",
       "1             3                    NaN                     NaN   \n",
       "2             3                   True             information   \n",
       "3             3                    NaN                     NaN   \n",
       "4             3                   True             information   \n",
       "\n",
       "  reflection_exists reflection_subtype question_exists question_subtype  \\\n",
       "0             False                NaN            True             open   \n",
       "1               NaN                NaN             NaN              NaN   \n",
       "2             False                NaN           False              NaN   \n",
       "3               NaN                NaN             NaN              NaN   \n",
       "4             False                NaN           False              NaN   \n",
       "\n",
       "  main_therapist_behaviour client_talk_type            datetime  \n",
       "0                 question              NaN 2023-05-05 00:00:13  \n",
       "1                      NaN          neutral 2023-05-05 00:00:24  \n",
       "2          therapist_input              NaN 2023-05-05 00:00:25  \n",
       "3                      NaN          neutral 2023-05-05 00:00:34  \n",
       "4          therapist_input              NaN 2023-05-05 00:00:34  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anno_mi = pd.read_csv(\"AnnoMI-full.csv\")\n",
    "anno_mi[\"datetime\"] = pd.to_datetime(anno_mi[\"timestamp\"])\n",
    "anno_mi = anno_mi.drop(columns=[\"video_title\", \"video_url\"])\n",
    "anno_mi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cea8099b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13551"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(anno_mi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44217a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral    0.627063\n",
       "change     0.248030\n",
       "sustain    0.124907\n",
       "Name: client_talk_type, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anno_mi[\"client_talk_type\"].value_counts() / anno_mi[\"interlocutor\"].value_counts()[\"client\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "367137a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "therapist    6826\n",
       "client       6725\n",
       "Name: interlocutor, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anno_mi[\"interlocutor\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a5a319f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reducing alcohol consumption                                                          2326\n",
       "more exercise / increasing activity                                                   2034\n",
       "reducing recidivism                                                                   1303\n",
       "reducing drug use                                                                     1104\n",
       "diabetes management                                                                    948\n",
       "smoking cessation                                                                      923\n",
       "smoking cessation                                                                      541\n",
       "taking medicine / following medical procedure                                          448\n",
       "asthma management                                                                      431\n",
       "avoiding DOI                                                                           394\n",
       "changing approach to disease                                                           315\n",
       "reducing gambling                                                                      297\n",
       "weight loss                                                                            294\n",
       "unidentifiable                                                                         287\n",
       "smoking cessation; reducing alcohol consumption                                        254\n",
       "overcoming issues at school                                                            167\n",
       "compliance with rules                                                                  146\n",
       "supporting client to live in more alignment with her values                            133\n",
       "increasing activity; taking medicine / following medical procedure                     126\n",
       "better oral health                                                                      97\n",
       "managing life                                                                           86\n",
       "anxiety management                                                                      79\n",
       "more exercise / increasing activity; weight loss                                        66\n",
       "Being assertive with flatmate about moving out                                          62\n",
       "reducing drug use; following medical procedure                                          59\n",
       "taking steps towards getting help with day-care                                         57\n",
       "reducing alcohol consumption; safe sex                                                  55\n",
       "reducing self-harm                                                                      54\n",
       "reducing coffee consumption                                                             51\n",
       "increasing self-confidence                                                              50\n",
       "completion of community service                                                         46\n",
       "reducing violence                                                                       41\n",
       "diet; reducing alcohol consumption; diabetes management                                 40\n",
       "weight loss; diet                                                                       36\n",
       "birth control                                                                           36\n",
       "reducing alcohol consumption; compliance with rules                                     31\n",
       "charging battery                                                                        29\n",
       "diagnosis                                                                               21\n",
       "providing information on medicines                                                      19\n",
       "engaging in community activities                                                        17\n",
       "problem recognition                                                                     16\n",
       "opening up                                                                              15\n",
       "recognising success                                                                     11\n",
       "not getting into a car with someone who is under the influence of drugs or alcohol       6\n",
       "Name: topic, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anno_mi[\"topic\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "507f8a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(anno_mi[\"transcript_id\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ca239c",
   "metadata": {},
   "source": [
    "## Only considering client for now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3eccadc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6725"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_index = [isinstance(x, str) for x in anno_mi[\"client_talk_type\"]]\n",
    "sum(client_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c9d0cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6725,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data = anno_mi[\"client_talk_type\"][client_index]\n",
    "y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a91e02e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     neutral\n",
       "3     neutral\n",
       "5     neutral\n",
       "7     neutral\n",
       "9     neutral\n",
       "11    neutral\n",
       "13    neutral\n",
       "15    neutral\n",
       "17    neutral\n",
       "19    neutral\n",
       "21    neutral\n",
       "23    neutral\n",
       "25    neutral\n",
       "27    neutral\n",
       "29    neutral\n",
       "31    neutral\n",
       "33    neutral\n",
       "35     change\n",
       "37     change\n",
       "39     change\n",
       "Name: client_talk_type, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10b944c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_id = {y_data.unique()[i]: i for i in range(len(y_data.unique()))}\n",
    "id_to_label = {v: k for k, v in label_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e226bfaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neutral': 0, 'change': 1, 'sustain': 2}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3319a9e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'neutral', 1: 'change', 2: 'sustain'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c919a378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data = [label_to_id[x] for x in y_data]\n",
    "y_data[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a35e43",
   "metadata": {},
   "source": [
    "## Obtaining SBERT Embeddings\n",
    "\n",
    "We can use the `SentenceEncoder` class within `nlpsig` to obtain sentence embeddings from a model. This class uses the [`sentence-transformer`](https://www.sbert.net/docs/package_reference/SentenceTransformer.html) package and here, we have use the pre-trained `all-mpnet-base-v2` model by passing this name as a string to the class - alternative models can be found [here](https://www.sbert.net/docs/pretrained_models.html).\n",
    "\n",
    "We can pass these into the constructor of the class to initialise our text encoder as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62808593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sbert_768_embeddings = np.load(\"anno_mi_sentence_embeddings_768.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07fe57d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the Text Encoder\n",
    "sbert_model_768 = \"all-mpnet-base-v2\"\n",
    "text_encoder_sbert_768 = nlpsig.SentenceEncoder(df=anno_mi,\n",
    "                                                feature_name=\"utterance_text\",\n",
    "                                                model_name=sbert_model_768)\n",
    "text_encoder_sbert_768.load_pretrained_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798b0890",
   "metadata": {},
   "source": [
    "The class has a `.encode_sentence_transformer()` method which first loads in the model (using the `model_name` and `model_args` attributes) and then obtains an embedding for each sentence. These sentence embeddings are then stored in the `embeddings_sentence` attribute of the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b984327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] number of sentences to encode: 13551\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a3a67e3c8e4ba29e300e29d5b8c509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_encoder_sbert_768.obtain_embeddings()\n",
    "sbert_768_embeddings = text_encoder_sbert_768.sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "913791f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"anno_mi_sentence_embeddings_768\", sbert_768_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50b23d6",
   "metadata": {},
   "source": [
    "## SBERT with 384 dimension vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a7dd27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sbert_384_embeddings = np.load(\"anno_mi_sentence_embeddings_384.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5474b3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the Text Encoder\n",
    "sbert_model_384 = \"all-MiniLM-L12-v2\"\n",
    "text_encoder_sbert_384 = nlpsig.SentenceEncoder(df=anno_mi,\n",
    "                                                feature_name=\"utterance_text\",\n",
    "                                                model_name=sbert_model_384)\n",
    "text_encoder_sbert_384.load_pretrained_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c1c89a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] number of sentences to encode: 13551\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dda6f6b4ccb14589ad271ae010fc6d13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_encoder_sbert_384.obtain_embeddings()\n",
    "sbert_384_embeddings = text_encoder_sbert_384.sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e3907f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"anno_mi_sentence_embeddings_384\", sbert_384_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04755f1",
   "metadata": {},
   "source": [
    "## Pretrained BERT and pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f865bde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pooled_mean_pretrained = np.load(\"anno_mi_pretrained_BERT_mean.npy\")\n",
    "# pooled_max_pretrained = np.load(\"anno_mi_pretrained_BERT_max.npy\")\n",
    "# pooled_sum_pretrained = np.load(\"anno_mi_pretrained_BERT_sum.npy\")\n",
    "# pooled_cls_pretrained = np.load(\"anno_mi_pretrained_BERT_cls.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5fe82ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "495a5d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "text_encoder_pretrained_BERT = nlpsig.TextEncoder(df=anno_mi,\n",
    "                                                  feature_name=\"utterance_text\",\n",
    "                                                  model_name=bert_model)\n",
    "text_encoder_pretrained_BERT.load_pretrained_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8207677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Setting return_special_tokens_mask=True\n",
      "[INFO] Tokenizing the dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13551 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving the tokenized text for each sentence into `.df['tokens']`...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13551 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating tokenized dataframe and setting in `.tokenized_df` attribute...\n",
      "[INFO] Note: 'text_id' is the column name for denoting the corresponding text id\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['mi_quality', 'transcript_id', 'topic', 'utterance_id', 'interlocutor', 'timestamp', 'utterance_text', 'annotator_id', 'therapist_input_exists', 'therapist_input_subtype', 'reflection_exists', 'reflection_subtype', 'question_exists', 'question_subtype', 'main_therapist_behaviour', 'client_talk_type', 'datetime', 'input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask', 'tokens'],\n",
       "    num_rows: 13551\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder_pretrained_BERT.tokenize_text(skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5aa2519b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27e0296f8c2244c0bd37a79b2ed773f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/136 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "token_embeddings_pretrained = text_encoder_pretrained_BERT.obtain_embeddings(method=\"hidden_layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fbe9a924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af56fde9d7974848b3ee2f426e5312d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a5e2741f2cd4db3a1428eabfde89e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "020c2619b18f4c348df9add3d7e62a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2abc947b4dde41aeb72d417dc46e4681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pooled_mean_pretrained = text_encoder_pretrained_BERT.pool_token_embeddings()\n",
    "pooled_max_pretrained = text_encoder_pretrained_BERT.pool_token_embeddings(method=\"max\")\n",
    "pooled_sum_pretrained = text_encoder_pretrained_BERT.pool_token_embeddings(method=\"sum\") \n",
    "pooled_cls_pretrained = text_encoder_pretrained_BERT.pool_token_embeddings(method=\"cls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db9ce6c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13551, 768)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_mean_pretrained.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "58d47bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13551, 768)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_max_pretrained.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "24c1a7ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13551, 768)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_sum_pretrained.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b57b4a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13551, 768)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_cls_pretrained.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fdec472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"anno_mi_pretrained_BERT_mean\", pooled_mean_pretrained)\n",
    "np.save(\"anno_mi_pretrained_BERT_max\", pooled_max_pretrained)\n",
    "np.save(\"anno_mi_pretrained_BERT_sum\", pooled_sum_pretrained)\n",
    "np.save(\"anno_mi_pretrained_BERT_cls\", pooled_cls_pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f49f72",
   "metadata": {},
   "source": [
    "## Fine-tuning BERT and pooling\n",
    "\n",
    "### (Ignoring this part for now while, but will run this on GPU cluster soon...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8928ddaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pooled_mean = np.load(\"anno_mi_BERT_mean.npy\")\n",
    "# pooled_max = np.load(\"anno_mi_BERT_max.npy\")\n",
    "# pooled_sum = np.load(\"anno_mi_BERT_sum.npy\")\n",
    "# pooled_cls = np.load(\"anno_mi_BERT_cls.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "426cf1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(bert_model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_model)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4c279d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder_BERT = nlpsig.TextEncoder(df=anno_mi,\n",
    "                                       feature_name=\"utterance_text\",\n",
    "                                       model=model,\n",
    "                                       tokenizer=tokenizer,\n",
    "                                       data_collator=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0d410ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Setting return_special_tokens_mask=True\n",
      "[INFO] Tokenizing the dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13551 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving the tokenized text for each sentence into `.df['tokens']`...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13551 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating tokenized dataframe and setting in `.tokenized_df` attribute...\n",
      "[INFO] Note: 'text_id' is the column name for denoting the corresponding text id\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['mi_quality', 'transcript_id', 'topic', 'utterance_id', 'interlocutor', 'timestamp', 'utterance_text', 'annotator_id', 'therapist_input_exists', 'therapist_input_subtype', 'reflection_exists', 'reflection_subtype', 'question_exists', 'question_subtype', 'main_therapist_behaviour', 'client_talk_type', 'datetime', 'tokens', 'input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "    num_rows: 13551\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder_BERT.tokenize_text(skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4e7040",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fcf20f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up data_collator for language modelling (has dynamic padding)\n",
    "data_collator_for_LM = DataCollatorForLanguageModeling(tokenizer=tokenizer,\n",
    "                                                       mlm=True,\n",
    "                                                       mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7703b1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Splitting up dataset into train / validation / test sets, and saving to `.dataset_split`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['mi_quality', 'transcript_id', 'topic', 'utterance_id', 'interlocutor', 'timestamp', 'utterance_text', 'annotator_id', 'therapist_input_exists', 'therapist_input_subtype', 'reflection_exists', 'reflection_subtype', 'question_exists', 'question_subtype', 'main_therapist_behaviour', 'client_talk_type', 'datetime', 'tokens', 'input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 10840\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['mi_quality', 'transcript_id', 'topic', 'utterance_id', 'interlocutor', 'timestamp', 'utterance_text', 'annotator_id', 'therapist_input_exists', 'therapist_input_subtype', 'reflection_exists', 'reflection_subtype', 'question_exists', 'question_subtype', 'main_therapist_behaviour', 'client_talk_type', 'datetime', 'tokens', 'input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 1356\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['mi_quality', 'transcript_id', 'topic', 'utterance_id', 'interlocutor', 'timestamp', 'utterance_text', 'annotator_id', 'therapist_input_exists', 'therapist_input_subtype', 'reflection_exists', 'reflection_subtype', 'question_exists', 'question_subtype', 'main_therapist_behaviour', 'client_talk_type', 'datetime', 'tokens', 'input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 1355\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder_BERT.split_dataset(seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fe25d8fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text_encoder_BERT.dataset_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "771ff5ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Setting up TrainingArguments object and saving to `.training_args`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=0,\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "auto_find_batch_size=False,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "data_seed=None,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_pin_memory=True,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "ddp_timeout=1800,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "do_eval=True,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_delay=0,\n",
       "eval_steps=None,\n",
       "evaluation_strategy=epoch,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "fsdp=[],\n",
       "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
       "fsdp_min_num_params=0,\n",
       "fsdp_transformer_layer_cls_to_wrap=None,\n",
       "full_determinism=False,\n",
       "gradient_accumulation_steps=1,\n",
       "gradient_checkpointing=False,\n",
       "greater_is_better=None,\n",
       "group_by_length=False,\n",
       "half_precision_backend=auto,\n",
       "hub_model_id=None,\n",
       "hub_private_repo=False,\n",
       "hub_strategy=every_save,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "include_inputs_for_metrics=False,\n",
       "jit_mode_eval=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=5e-05,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=False,\n",
       "local_rank=-1,\n",
       "log_level=passive,\n",
       "log_level_replica=warning,\n",
       "log_on_each_node=True,\n",
       "logging_dir=bert-base-uncased-anno-mi/runs/May05_13-23-13_MAC-ATI1167,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=500,\n",
       "logging_strategy=steps,\n",
       "lr_scheduler_type=linear,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=None,\n",
       "mp_parameters=,\n",
       "no_cuda=False,\n",
       "num_train_epochs=600,\n",
       "optim=adamw_hf,\n",
       "optim_args=None,\n",
       "output_dir=bert-base-uncased-anno-mi,\n",
       "overwrite_output_dir=False,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=8,\n",
       "per_device_train_batch_size=128,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "ray_scope=last,\n",
       "remove_unused_columns=True,\n",
       "report_to=[],\n",
       "resume_from_checkpoint=None,\n",
       "run_name=bert-base-uncased-anno-mi,\n",
       "save_on_each_node=False,\n",
       "save_steps=10000,\n",
       "save_strategy=steps,\n",
       "save_total_limit=None,\n",
       "seed=2023,\n",
       "sharded_ddp=[],\n",
       "skip_memory_metrics=True,\n",
       "tf32=None,\n",
       "torch_compile=False,\n",
       "torch_compile_backend=None,\n",
       "torch_compile_mode=None,\n",
       "torchdynamo=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_ipex=False,\n",
       "use_legacy_prediction_loop=False,\n",
       "use_mps_device=False,\n",
       "warmup_ratio=0.0,\n",
       "warmup_steps=0,\n",
       "weight_decay=0.0,\n",
       "xpu_backend=None,\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased-anno-mi\"\n",
    "text_encoder_BERT.set_up_training_args(output_dir=model_name,\n",
    "                                  num_train_epochs=600,\n",
    "                                  per_device_train_batch_size=128,\n",
    "                                  disable_tqdm=False,\n",
    "                                  save_strategy=\"steps\",\n",
    "                                  save_steps=10000,\n",
    "                                  seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9a986768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.training_args.TrainingArguments"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text_encoder_BERT.training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "25992969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Setting up Trainer object, and saving to `.trainer`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<transformers.trainer.Trainer at 0x2b04c1e80>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder_BERT.set_up_trainer(data_collator=data_collator_for_LM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "86d8ffc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.trainer.Trainer"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text_encoder_BERT.trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "43e4b27f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "02ea1a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9db03070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to only report errors to avoid excessing logging\n",
    "transformers.utils.logging.set_verbosity(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d24afca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training model with 109514298 parameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rchan/opt/miniconda3/envs/nlpsig-networks/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='51000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    6/51000 01:31 < 323:48:12, 0.04 it/s, Epoch 0.06/600]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_encoder_BERT.fit_transformer_with_trainer_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17001c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder_BERT.trainer.save_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21a3d80",
   "metadata": {},
   "source": [
    "### Evaluating model on masked language modelling task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14947a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder_BERT.tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c0e1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_masked_character_accuracy(fill_mask, words):\n",
    "    was_correct = []\n",
    "    print(f\"Evaluating with {len(words)} words\")\n",
    "    for word in tqdm(words):\n",
    "        masked_strings = [word[:i] + '<mask>' + word[i+1:] for i in range(len(word))]\n",
    "        predictions = [fill_mask(word)[0]['sequence'] for word in masked_strings]\n",
    "        was_correct += [pred == word for pred in predictions]\n",
    "    \n",
    "    acc = np.sum(was_correct) / len(was_correct)\n",
    "    print(f\"Accuracy: {acc}\")\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19683fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask = pipeline(\"fill-mask\",\n",
    "                     model=model_name,\n",
    "                     tokenizer=model_name)\n",
    "\n",
    "compute_masked_character_accuracy(fill_mask, text_encoder_BERT.dataset_split[\"test\"][\"word\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0437704",
   "metadata": {},
   "source": [
    "### Obtain embeddings from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfbad0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the model to CPU (might not be always necessary to run this)\n",
    "text_encoder_BERT.model.to('cpu')\n",
    "token_embeddings = text_encoder_BERT.obtain_embeddings(method=\"hidden_layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee038d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e1dad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_mean = text_encoder_BERT.pool_token_embeddings()\n",
    "pooled_max = text_encoder_BERT.pool_token_embeddings(method=\"max\")\n",
    "pooled_sum = text_encoder_BERT.pool_token_embeddings(method=\"sum\")\n",
    "pooled_cls = text_encoder_BERT.pool_token_embeddings(method=\"cls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b6ec40",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41ce5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_max.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bf8a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_sum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b30b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_cls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66a2e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"anno_mi_BERT_mean\", pooled_mean)\n",
    "np.save(\"anno_mi_BERT_max\", pooled_max)\n",
    "np.save(\"anno_mi_BERT_sum\", pooled_sum)\n",
    "np.save(\"anno_mi_BERT_cls\", pooled_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89170fce",
   "metadata": {},
   "source": [
    "# Baseline 1: FFN baseline\n",
    "\n",
    "Using the embeddings for the sentences directly in a FFN.\n",
    "\n",
    "Below is a function that takes in some inputs x_data, y_data and fits a FFN. Will do early stopping if the F1 score continually does not improve (patience is a bit high since we actually count if the F1 isn't changed in an epoch, so we make it a bit higher to allow it to stay at the same F1 performance for a few epochs basically)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b91437c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def implement_ffn(x_data,\n",
    "                  y_data,\n",
    "                  hidden_dim,\n",
    "                  learning_rate,\n",
    "                  loss,\n",
    "                  gamma=0):\n",
    "    # set seed\n",
    "    set_seed(seed)\n",
    "    \n",
    "    # initialise FFN\n",
    "    ffn_model = FeedforwardNeuralNetModel(input_dim=x_data.shape[1],\n",
    "                                          hidden_dim=hidden_dim,\n",
    "                                          output_dim=len(label_to_id),\n",
    "                                          dropout_rate=0.1)\n",
    "    \n",
    "    # split dataset\n",
    "    train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n",
    "                                       y_data=torch.tensor(y_data),\n",
    "                                       train_size=0.8,\n",
    "                                       valid_size=0.5,\n",
    "                                       shuffle=True,\n",
    "                                       as_DataLoader=True,\n",
    "                                       seed=seed)\n",
    "\n",
    "    # define loss\n",
    "    if loss == \"focal\":\n",
    "        criterion = FocalLoss(gamma = gamma)\n",
    "    elif loss == \"cross_entropy\":\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # define optimizer\n",
    "    optimizer = torch.optim.Adam(ffn_model.parameters(), lr=learning_rate)\n",
    "    # define scheduler for adjusting the learning rate\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "    # scheduler = StepLR(optimizer, step_size = 4, gamma = 0.5)\n",
    "    # scheduler = None\n",
    "    \n",
    "    ffn_model = training_pytorch(model=ffn_model,\n",
    "                                 train_loader=train,\n",
    "                                 criterion=criterion,\n",
    "                                 optimizer=optimizer,\n",
    "                                 num_epochs=10000,\n",
    "                                 scheduler=scheduler,\n",
    "                                 valid_loader=valid,\n",
    "                                 seed=seed,\n",
    "                                 early_stopping=True,\n",
    "                                 early_stopping_metric=\"f1\",\n",
    "                                 patience=100,\n",
    "                                 verbose=True,\n",
    "                                 verbose_epoch=100)\n",
    "\n",
    "    pred, label = testing_pytorch(ffn_model, test, criterion)\n",
    "    print(f\"proportion of labels in prediction: {[sum(pred==i)/len(pred) for i in label_to_id.values()]}\")\n",
    "    print(f\"proportion of labels in data: {[sum(label==i)/len(label) for i in label_to_id.values()]}\")\n",
    "    \n",
    "    f1_scores = metrics.f1_score(label, pred, average=None)\n",
    "    print(f\"- f1: {f1_scores}\")\n",
    "    print(f\"- f1 (average): {sum(f1_scores)/len(f1_scores)}\")\n",
    "    print(f\"- accuracy: {sum(pred==label)/len(pred)}\")\n",
    "    \n",
    "    return ffn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6223d974",
   "metadata": {},
   "source": [
    "Going to try out some variations (1 hidden layer, 2 hidden layers and 3 hidden layers - all of size 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6bc9f4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim_trials = [100, [100,100], [100,100,100]]\n",
    "learning_rate = 2e-5\n",
    "loss = \"cross_entropy\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177602bd",
   "metadata": {},
   "source": [
    "## SBERT 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "490e6e55",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** hidden_dim: 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91e3e0e9001d4f92b1f079995d6d9d64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0885392427444458\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0795083045959473\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0757082592357288 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.5589850544929504\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.9038074016571045\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.6840004433285106 || Accuracy: 0.6953937411308289 || F1-score: 0.532593092563333\n",
      "Epoch: 201/10000 || Item: 0/85 || Loss: 0.7341877222061157\n",
      "--------------------------------------------------\n",
      "##### Epoch: 201/10000 || Loss: 1.368276834487915\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 201 || Loss: 0.6921473199670966 || Accuracy: 0.7013372778892517 || F1-score: 0.5449254096356076\n",
      "Early stopping at epoch 211!\n",
      "Accuracy on dataset of size 672: 69.3452377319336 %.\n",
      "Average loss: 0.7180577083067461\n",
      "proportion of labels in prediction: [tensor(0.7560), tensor(0.1786), tensor(0.0655)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.80690399 0.51111111 0.31292517]\n",
      "- f1 (average): 0.5436467575163831\n",
      "- accuracy: 0.6934523582458496\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b852d8617ff14c04acab0285f0cf83c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.116580843925476\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.118660807609558\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.1014031171798706 || Accuracy: 0.12184249609708786 || F1-score: 0.07240618101545253\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.7407606244087219\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.4813379645347595\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.685649649663405 || Accuracy: 0.6939078569412231 || F1-score: 0.521815389563948\n",
      "Epoch: 201/10000 || Item: 0/85 || Loss: 0.7610892653465271\n",
      "--------------------------------------------------\n",
      "##### Epoch: 201/10000 || Loss: 1.2291091680526733\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 201 || Loss: 0.6832592649893328 || Accuracy: 0.7028231620788574 || F1-score: 0.5480318366095817\n",
      "Early stopping at epoch 227!\n",
      "Accuracy on dataset of size 672: 70.08928680419922 %.\n",
      "Average loss: 0.730528782714497\n",
      "proportion of labels in prediction: [tensor(0.7455), tensor(0.1875), tensor(0.0670)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.81086957 0.52173913 0.35135135]\n",
      "- f1 (average): 0.5613200156678417\n",
      "- accuracy: 0.7008928656578064\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77b893d31356409f982fae23135a3ea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0993850231170654\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.084618330001831\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0871503244746814 || Accuracy: 0.37147101759910583 || F1-score: 0.2602133749674733\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.6977670192718506\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.1788264811038971\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7017621939832513 || Accuracy: 0.7013372778892517 || F1-score: 0.4817348905283425\n",
      "Epoch: 201/10000 || Item: 0/85 || Loss: 0.6531517505645752\n",
      "--------------------------------------------------\n",
      "##### Epoch: 201/10000 || Loss: 0.25363898277282715\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 201 || Loss: 0.6932478276166049 || Accuracy: 0.7028231620788574 || F1-score: 0.5252829038733572\n",
      "Early stopping at epoch 217!\n",
      "Accuracy on dataset of size 672: 69.3452377319336 %.\n",
      "Average loss: 0.7214958667755127\n",
      "proportion of labels in prediction: [tensor(0.7307), tensor(0.2232), tensor(0.0461)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.81538462 0.52       0.25373134]\n",
      "- f1 (average): 0.5297053195560658\n",
      "- accuracy: 0.6934523582458496\n"
     ]
    }
   ],
   "source": [
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=sbert_768_embeddings[client_index],\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfb93a2",
   "metadata": {},
   "source": [
    "## SBERT 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "176acf75",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** hidden_dim: 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f331b81d2f0244669b3a5ef18266d9d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0692368745803833\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0597608089447021\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0575233806263318 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.5714445114135742\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.993977427482605\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7121773145415566 || Accuracy: 0.6775631308555603 || F1-score: 0.4852881287212332\n",
      "Early stopping at epoch 193!\n",
      "Accuracy on dataset of size 672: 69.49404907226562 %.\n",
      "Average loss: 0.7372136386958036\n",
      "proportion of labels in prediction: [tensor(0.7693), tensor(0.1786), tensor(0.0521)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.80982906 0.51111111 0.27536232]\n",
      "- f1 (average): 0.5321008299269169\n",
      "- accuracy: 0.694940447807312\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb4b8db3d10d4c8b8389f64cfd0b9e6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1142284870147705\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0898610353469849\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.1067617481405085 || Accuracy: 0.25557205080986023 || F1-score: 0.13570019723865875\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.7563970685005188\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.24487124383449554\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7194767540151422 || Accuracy: 0.6864784359931946 || F1-score: 0.48013654513009985\n",
      "Epoch: 201/10000 || Item: 0/85 || Loss: 0.7156612277030945\n",
      "--------------------------------------------------\n",
      "##### Epoch: 201/10000 || Loss: 1.055163860321045\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 201 || Loss: 0.7143634991212324 || Accuracy: 0.6924219727516174 || F1-score: 0.5152181344117231\n",
      "Early stopping at epoch 228!\n",
      "Accuracy on dataset of size 672: 68.75 %.\n",
      "Average loss: 0.7355566729198803\n",
      "proportion of labels in prediction: [tensor(0.7500), tensor(0.1979), tensor(0.0521)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.81040087 0.48763251 0.27536232]\n",
      "- f1 (average): 0.5244652314711323\n",
      "- accuracy: 0.6875\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df5d0225518d4077a5679a992ce44622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0598180294036865\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0403138399124146\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.051291660829024 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.8030747175216675\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.3111073076725006\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7629680037498474 || Accuracy: 0.6627042889595032 || F1-score: 0.41140123154470193\n",
      "Early stopping at epoch 174!\n",
      "Accuracy on dataset of size 672: 66.51786041259766 %.\n",
      "Average loss: 0.7649482705376365\n",
      "proportion of labels in prediction: [tensor(0.7321), tensor(0.2679), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.82107574 0.44242424 0.        ]\n",
      "- f1 (average): 0.4211666611227533\n",
      "- accuracy: 0.6651785969734192\n"
     ]
    }
   ],
   "source": [
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=sbert_384_embeddings[client_index],\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f83174",
   "metadata": {},
   "source": [
    "## Pretrained BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f68ab2",
   "metadata": {},
   "source": [
    "### Mean pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b4d5d457",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** hidden_dim: 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94321e7306ac4397bd6906c90d8bd810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0839786529541016\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0357847213745117\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0042308189652183 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.5391806364059448\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.6543185114860535\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7084653865207325 || Accuracy: 0.6879643201828003 || F1-score: 0.5493394831286408\n",
      "Early stopping at epoch 186!\n",
      "Accuracy on dataset of size 672: 69.04762268066406 %.\n",
      "Average loss: 0.7226935570890253\n",
      "proportion of labels in prediction: [tensor(0.7396), tensor(0.2009), tensor(0.0595)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.80567686 0.51929825 0.29370629]\n",
      "- f1 (average): 0.5395604650718417\n",
      "- accuracy: 0.6904761791229248\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c1ac8217a2d4ac2a99b371fc20b6f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1131868362426758\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0881928205490112\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.070029453797774 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.7127247452735901\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.4683959186077118\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.6954461498693987 || Accuracy: 0.7028231620788574 || F1-score: 0.5731369952368576\n",
      "Early stopping at epoch 194!\n",
      "Accuracy on dataset of size 672: 68.75 %.\n",
      "Average loss: 0.7084680687297474\n",
      "proportion of labels in prediction: [tensor(0.7277), tensor(0.2024), tensor(0.0699)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.80396476 0.51748252 0.30666667]\n",
      "- f1 (average): 0.5427046472861451\n",
      "- accuracy: 0.6875\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1deb9d1e45554ba599dffa2bf842d623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.098788857460022\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0711716413497925\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0712726332924583 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.6781942248344421\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.12910042703151703\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.6977984309196472 || Accuracy: 0.6939078569412231 || F1-score: 0.5608619843873767\n",
      "Early stopping at epoch 180!\n",
      "Accuracy on dataset of size 672: 69.19642639160156 %.\n",
      "Average loss: 0.7155460823665966\n",
      "proportion of labels in prediction: [tensor(0.7277), tensor(0.1979), tensor(0.0744)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.81057269 0.50883392 0.32679739]\n",
      "- f1 (average): 0.5487346650356896\n",
      "- accuracy: 0.6919642686843872\n"
     ]
    }
   ],
   "source": [
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=pooled_mean_pretrained[client_index],\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0dab54",
   "metadata": {},
   "source": [
    "### Max pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d8c64311",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** hidden_dim: 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a26142f6b69b4179b7e679be65f0cd21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0957067012786865\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0682802200317383\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.969028727574782 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.571304440498352\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.8043275475502014\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7314741611480713 || Accuracy: 0.6716195940971375 || F1-score: 0.506992275294162\n",
      "Early stopping at epoch 178!\n",
      "Accuracy on dataset of size 672: 68.00595092773438 %.\n",
      "Average loss: 0.7330376018177379\n",
      "proportion of labels in prediction: [tensor(0.7619), tensor(0.2068), tensor(0.0312)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.80558539 0.47750865 0.20967742]\n",
      "- f1 (average): 0.4975904873084758\n",
      "- accuracy: 0.680059552192688\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf927f6097ea4629a0942f7baa76004a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1076050996780396\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0695494413375854\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0264393470504067 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.8345953226089478\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.3302067220211029\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.721508036960255 || Accuracy: 0.6775631308555603 || F1-score: 0.5272523330049675\n",
      "Early stopping at epoch 193!\n",
      "Accuracy on dataset of size 672: 69.3452377319336 %.\n",
      "Average loss: 0.7208894978870045\n",
      "proportion of labels in prediction: [tensor(0.7500), tensor(0.2009), tensor(0.0491)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.80823402 0.5122807  0.29411765]\n",
      "- f1 (average): 0.5382107894382783\n",
      "- accuracy: 0.6934523582458496\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a413bb6ba51472aae39b882857423b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0997354984283447\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0507149696350098\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0337201248515735 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.7388246655464172\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.18091349303722382\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7373271042650397 || Accuracy: 0.6760772466659546 || F1-score: 0.5229165624322477\n",
      "Epoch: 201/10000 || Item: 0/85 || Loss: 0.7790274620056152\n",
      "--------------------------------------------------\n",
      "##### Epoch: 201/10000 || Loss: 0.18720248341560364\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 201 || Loss: 0.7315309806303545 || Accuracy: 0.6805348992347717 || F1-score: 0.5289228692454498\n",
      "Early stopping at epoch 206!\n",
      "Accuracy on dataset of size 672: 69.19642639160156 %.\n",
      "Average loss: 0.7459257245063782\n",
      "proportion of labels in prediction: [tensor(0.7440), tensor(0.2143), tensor(0.0417)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.80957563 0.5170068  0.25954198]\n",
      "- f1 (average): 0.5287081377113333\n",
      "- accuracy: 0.6919642686843872\n"
     ]
    }
   ],
   "source": [
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=pooled_max_pretrained[client_index],\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428b672c",
   "metadata": {},
   "source": [
    "### Sum pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ec6afd4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** hidden_dim: 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1319f2c0de4348a489db8629e22b9a71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 2.0271031856536865\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0212513208389282\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0024839531291614 || Accuracy: 0.6225854158401489 || F1-score: 0.3232245025425277\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.4351794719696045\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.2922021746635437\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.72612741318616 || Accuracy: 0.6909360885620117 || F1-score: 0.565352009350549\n",
      "Early stopping at epoch 136!\n",
      "Accuracy on dataset of size 672: 70.68452453613281 %.\n",
      "Average loss: 0.7379533269188621\n",
      "proportion of labels in prediction: [tensor(0.7485), tensor(0.1786), tensor(0.0729)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.80477223 0.54074074 0.40789474]\n",
      "- f1 (average): 0.5844692372853882\n",
      "- accuracy: 0.706845223903656\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fec02be0a914ae9ac22a86b2ba9dd67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.114878535270691\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0557730197906494\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.054190386425365 || Accuracy: 0.6151559948921204 || F1-score: 0.34358449687048626\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.736968994140625\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.3847038149833679\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7197055708278309 || Accuracy: 0.7028231620788574 || F1-score: 0.5803708684031409\n",
      "Early stopping at epoch 151!\n",
      "Accuracy on dataset of size 672: 71.57737731933594 %.\n",
      "Average loss: 0.7132322219285098\n",
      "proportion of labels in prediction: [tensor(0.7426), tensor(0.1860), tensor(0.0714)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.81263617 0.56       0.41059603]\n",
      "- f1 (average): 0.5944107306891361\n",
      "- accuracy: 0.7157738208770752\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba635c77250947fa9752757e96de0cba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.111946940422058\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.1200898885726929\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0467318513176658 || Accuracy: 0.6344724893569946 || F1-score: 0.293682004525378\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.6803026795387268\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.07645384967327118\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7378906770185991 || Accuracy: 0.7072808146476746 || F1-score: 0.5981767162832544\n",
      "Early stopping at epoch 139!\n",
      "Accuracy on dataset of size 672: 71.13095092773438 %.\n",
      "Average loss: 0.736527681350708\n",
      "proportion of labels in prediction: [tensor(0.7232), tensor(0.1860), tensor(0.0908)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.81104972 0.53818182 0.45121951]\n",
      "- f1 (average): 0.6001503513779487\n",
      "- accuracy: 0.711309552192688\n"
     ]
    }
   ],
   "source": [
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=pooled_sum_pretrained[client_index],\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c50d145",
   "metadata": {},
   "source": [
    "### CLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d1e85b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** hidden_dim: 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c25c918db914936bd1154bd590f37a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.072586178779602\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.054619312286377\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9783457517623901 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.5552260279655457\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.6392856240272522\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7163692333481528 || Accuracy: 0.6849925518035889 || F1-score: 0.5467744526948758\n",
      "Early stopping at epoch 186!\n",
      "Accuracy on dataset of size 672: 71.13095092773438 %.\n",
      "Average loss: 0.7055106813257391\n",
      "proportion of labels in prediction: [tensor(0.7515), tensor(0.1801), tensor(0.0685)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.81818182 0.51660517 0.40268456]\n",
      "- f1 (average): 0.579157182663956\n",
      "- accuracy: 0.711309552192688\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a28edf828444abd905945fe9a60551a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1158097982406616\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0941648483276367\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0671783143823796 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.8176752924919128\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.4519164562225342\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7216152494603937 || Accuracy: 0.6805348992347717 || F1-score: 0.5284876279863481\n",
      "Early stopping at epoch 149!\n",
      "Accuracy on dataset of size 672: 69.3452377319336 %.\n",
      "Average loss: 0.7150283401662653\n",
      "proportion of labels in prediction: [tensor(0.7560), tensor(0.1696), tensor(0.0744)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.80690399 0.46969697 0.39215686]\n",
      "- f1 (average): 0.5562526079373594\n",
      "- accuracy: 0.6934523582458496\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b535f2b1dee4844a4e6ea4a315aa585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.099726676940918\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0698367357254028\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.062767429785295 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.7204791903495789\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.13758376240730286\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7134658856825395 || Accuracy: 0.689450204372406 || F1-score: 0.56131034880422\n",
      "Early stopping at epoch 182!\n",
      "Accuracy on dataset of size 672: 70.68452453613281 %.\n",
      "Average loss: 0.6856141903183677\n",
      "proportion of labels in prediction: [tensor(0.7336), tensor(0.1830), tensor(0.0833)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.81578947 0.51282051 0.41509434]\n",
      "- f1 (average): 0.5812347753757884\n",
      "- accuracy: 0.706845223903656\n"
     ]
    }
   ],
   "source": [
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=pooled_cls_pretrained[client_index],\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e37432",
   "metadata": {},
   "source": [
    "## Fine-tuned BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a62a929",
   "metadata": {},
   "source": [
    "### Mean pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1490307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=pooled_mean_pretrained[client_index],\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=1e-5,\n",
    "                  loss=\"cross_entropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f53af7",
   "metadata": {},
   "source": [
    "### Max pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435194ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=pooled_max_pretrained[client_index],\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88d5227",
   "metadata": {},
   "source": [
    "### Sum pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e627a372",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=pooled_sum_pretrained[client_index],\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff82ab6",
   "metadata": {},
   "source": [
    "### CLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1237106",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=pooled_sum_pretrained[client_index],\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9603d72a",
   "metadata": {},
   "source": [
    "# Baseline 2: Averaging history and use FFN\n",
    "\n",
    "Here, we will use `nlpsig` to construct some paths of embeddings which we will average and use those in a FFN.\n",
    "\n",
    "First, we define the arguments for how we want to construct our path. As we're going to just do a simple average of embeddings, I'll set zero padding as false, and construct the path by looking at the last `k` posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4cc74983",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_specifics = {\"pad_by\": \"history\",\n",
    "                  \"zero_padding\": False,\n",
    "                  \"method\": \"k_last\",\n",
    "                  \"k\": 5,\n",
    "                  \"time_feature\": None,\n",
    "                  \"embeddings\": \"full\",\n",
    "                  \"include_current_embedding\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "bffcbba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_mean_history(embeddings, path_specifics):\n",
    "    paths = nlpsig.PrepareData(anno_mi,\n",
    "                               id_column=\"transcript_id\",\n",
    "                               label_column=\"client_talk_type\",\n",
    "                               embeddings=embeddings)\n",
    "    path = paths.pad(**path_specifics)\n",
    "    # remove last two columns (which contains the id and the label)\n",
    "    path = path[client_index][:,:,:-2]\n",
    "    # average in the first dimension\n",
    "    return path.mean(1).astype(\"float\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8290e3e9",
   "metadata": {},
   "source": [
    "## SBERT 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "8cbe4f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Adding 'time_encoding' and feature...\n",
      "[INFO] Adding 'time_diff' and feature...\n",
      "[INFO] Adding 'timeline_index' feature...\n",
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1a068c42b3647e3a9b0209da7c87617",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** hidden_dim: 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efcd9f965eed4bbfaf2a804723f94a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.088150978088379\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0829229354858398\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.075862082568082 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.831689178943634\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.9813627004623413\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.8894803578203375 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 101!\n",
      "Accuracy on dataset of size 672: 62.35118865966797 %.\n",
      "Average loss: 0.9005365100773898\n",
      "proportion of labels in prediction: [tensor(1.), tensor(0.), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.76810266 0.         0.        ]\n",
      "- f1 (average): 0.25603421937060805\n",
      "- accuracy: 0.6235119104385376\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d3e7f71bcb4f4fa4f5bdc7945243fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1165460348129272\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.1194690465927124\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.1018761504780163 || Accuracy: 0.12184249609708786 || F1-score: 0.07240618101545253\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.9759571552276611\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.4108854830265045\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.8946103345264088 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 102!\n",
      "Accuracy on dataset of size 672: 62.35118865966797 %.\n",
      "Average loss: 0.9107744693756104\n",
      "proportion of labels in prediction: [tensor(1.), tensor(0.), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.76810266 0.         0.        ]\n",
      "- f1 (average): 0.25603421937060805\n",
      "- accuracy: 0.6235119104385376\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "416b19baae1940ad8cecefd4a4271b93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0994921922683716\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0858962535858154\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0874490196054631 || Accuracy: 0.2615155875682831 || F1-score: 0.14264979681024412\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.9241882562637329\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.6933719515800476\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.8937450593168085 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 102!\n",
      "Accuracy on dataset of size 672: 62.35118865966797 %.\n",
      "Average loss: 0.9194471185857599\n",
      "proportion of labels in prediction: [tensor(1.), tensor(0.), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.76810266 0.         0.        ]\n",
      "- f1 (average): 0.25603421937060805\n",
      "- accuracy: 0.6235119104385376\n"
     ]
    }
   ],
   "source": [
    "path_history = obtain_mean_history(sbert_768_embeddings, path_specifics)\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=path_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eb9bb8",
   "metadata": {},
   "source": [
    "## SBERT 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "0c545933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Adding 'time_encoding' and feature...\n",
      "[INFO] Adding 'time_diff' and feature...\n",
      "[INFO] Adding 'timeline_index' feature...\n",
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02178708deda4f708517855a93c9c6ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** hidden_dim: 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adfa7e325bd64a43aaa5000f85defeb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0689338445663452\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.059541940689087\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.05829148942774 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.8537594079971313\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 1.0656259059906006\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.896614210172133 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 101!\n",
      "Accuracy on dataset of size 672: 62.35118865966797 %.\n",
      "Average loss: 0.912129510532726\n",
      "proportion of labels in prediction: [tensor(1.), tensor(0.), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.76810266 0.         0.        ]\n",
      "- f1 (average): 0.25603421937060805\n",
      "- accuracy: 0.6235119104385376\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80355f1a7f16414f9326e2d63f57d532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.114090919494629\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0903785228729248\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.106756405396895 || Accuracy: 0.25557205080986023 || F1-score: 0.13570019723865875\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.9462771415710449\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.4005976617336273\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.8934355920011346 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 103!\n",
      "Accuracy on dataset of size 672: 62.35118865966797 %.\n",
      "Average loss: 0.9252154231071472\n",
      "proportion of labels in prediction: [tensor(1.), tensor(0.), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.76810266 0.         0.        ]\n",
      "- f1 (average): 0.25603421937060805\n",
      "- accuracy: 0.6235119104385376\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca121d4cbe0a41c89a87cdac50757cb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0598030090332031\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0398383140563965\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0513845031911677 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.9368603229522705\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.695285439491272\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.893760393966328 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 101!\n",
      "Accuracy on dataset of size 672: 62.35118865966797 %.\n",
      "Average loss: 0.9282568964091215\n",
      "proportion of labels in prediction: [tensor(1.), tensor(0.), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.76810266 0.         0.        ]\n",
      "- f1 (average): 0.25603421937060805\n",
      "- accuracy: 0.6235119104385376\n"
     ]
    }
   ],
   "source": [
    "path_history = obtain_mean_history(sbert_384_embeddings, path_specifics)\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=path_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c884a19",
   "metadata": {},
   "source": [
    "## Pretrained BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7818828",
   "metadata": {},
   "source": [
    "### Mean pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "5f7a77f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Adding 'time_encoding' and feature...\n",
      "[INFO] Adding 'time_diff' and feature...\n",
      "[INFO] Adding 'timeline_index' feature...\n",
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb7b98b9c0c3470ba8a810ee0307568f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** hidden_dim: 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53f2930634974e2ea8d4a41d90a4dcc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0827751159667969\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.067988395690918\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.00626910274679 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.8487571477890015\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 1.1112945079803467\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.886727511882782 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 101!\n",
      "Accuracy on dataset of size 672: 62.35118865966797 %.\n",
      "Average loss: 0.9075098742138256\n",
      "proportion of labels in prediction: [tensor(1.), tensor(0.), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.76810266 0.         0.        ]\n",
      "- f1 (average): 0.25603421937060805\n",
      "- accuracy: 0.6235119104385376\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf34451f3cdb4325928c7c3f6333c781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1143455505371094\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0963679552078247\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0698934359983965 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.976948618888855\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.4426864683628082\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.8920709815892306 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 101!\n",
      "Accuracy on dataset of size 672: 62.35118865966797 %.\n",
      "Average loss: 0.9157748547467318\n",
      "proportion of labels in prediction: [tensor(1.), tensor(0.), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.76810266 0.         0.        ]\n",
      "- f1 (average): 0.25603421937060805\n",
      "- accuracy: 0.6235119104385376\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8662b9293852497ab199d3fe4b4f671e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0997600555419922\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0753464698791504\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0727270191366023 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.9181236028671265\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.721269428730011\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.8867454582994635 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 101!\n",
      "Accuracy on dataset of size 672: 62.35118865966797 %.\n",
      "Average loss: 0.9253213839097456\n",
      "proportion of labels in prediction: [tensor(1.), tensor(0.), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.76810266 0.         0.        ]\n",
      "- f1 (average): 0.25603421937060805\n",
      "- accuracy: 0.6235119104385376\n"
     ]
    }
   ],
   "source": [
    "path_history = obtain_mean_history(pooled_mean_pretrained, path_specifics)\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=path_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab28615",
   "metadata": {},
   "source": [
    "### Max pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "2d3e718d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Adding 'time_encoding' and feature...\n",
      "[INFO] Adding 'time_diff' and feature...\n",
      "[INFO] Adding 'timeline_index' feature...\n",
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd5690e6fce4eb288f3a2c67b0f3ac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** hidden_dim: 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fbb0909954b4b8b860287d15211dcfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0947773456573486\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.159409999847412\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9376135522669012 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.8620471954345703\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 1.1281884908676147\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.8903347687287764 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 101!\n",
      "Accuracy on dataset of size 672: 62.35118865966797 %.\n",
      "Average loss: 0.9101777943697843\n",
      "proportion of labels in prediction: [tensor(1.), tensor(0.), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.76810266 0.         0.        ]\n",
      "- f1 (average): 0.25603421937060805\n",
      "- accuracy: 0.6235119104385376\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "663eff80dd074488afe2767736436ac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1085178852081299\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0852488279342651\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0026371316476301 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.9753608107566833\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.4552372694015503\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.8919325145808134 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 101!\n",
      "Accuracy on dataset of size 672: 62.35118865966797 %.\n",
      "Average loss: 0.916386983611367\n",
      "proportion of labels in prediction: [tensor(1.), tensor(0.), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.76810266 0.         0.        ]\n",
      "- f1 (average): 0.25603421937060805\n",
      "- accuracy: 0.6235119104385376\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8c775810c0c4f43b673d3655454cda8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.099276065826416\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0503199100494385\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0202285701578313 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.926359236240387\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.7107961177825928\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.8936019160530784 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 101!\n",
      "Accuracy on dataset of size 672: 62.35118865966797 %.\n",
      "Average loss: 0.9287005175243724\n",
      "proportion of labels in prediction: [tensor(1.), tensor(0.), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.76810266 0.         0.        ]\n",
      "- f1 (average): 0.25603421937060805\n",
      "- accuracy: 0.6235119104385376\n"
     ]
    }
   ],
   "source": [
    "path_history = obtain_mean_history(pooled_max_pretrained, path_specifics)\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=path_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157ff50d",
   "metadata": {},
   "source": [
    "### Sum pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "5d319f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Adding 'time_encoding' and feature...\n",
      "[INFO] Adding 'time_diff' and feature...\n",
      "[INFO] Adding 'timeline_index' feature...\n",
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dc9e75ad6004090a24267c9aecf864a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** hidden_dim: 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db81225808de4a17b617b490353fbe99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.5255906581878662\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.2541639804840088\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9284746050834656 || Accuracy: 0.6240713000297546 || F1-score: 0.26351130351130353\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.5938670039176941\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.7251362800598145\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.8142873265526511 || Accuracy: 0.6493313312530518 || F1-score: 0.41329564888064\n",
      "Early stopping at epoch 171!\n",
      "Accuracy on dataset of size 672: 65.32737731933594 %.\n",
      "Average loss: 0.8633222579956055\n",
      "proportion of labels in prediction: [tensor(0.8795), tensor(0.1042), tensor(0.0164)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.78019802 0.31818182 0.1754386 ]\n",
      "- f1 (average): 0.42460614482500886\n",
      "- accuracy: 0.6532738208770752\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a900a4b88686431e964d72357f3eb7b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0804401636123657\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.2311005592346191\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.953440628268502 || Accuracy: 0.6210995316505432 || F1-score: 0.25542315918117936\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.8102289438247681\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.4154810905456543\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.8209891265088861 || Accuracy: 0.6419019103050232 || F1-score: 0.4145659757894998\n",
      "Early stopping at epoch 184!\n",
      "Accuracy on dataset of size 672: 65.77381134033203 %.\n",
      "Average loss: 0.8609610958532854\n",
      "proportion of labels in prediction: [tensor(0.8557), tensor(0.1220), tensor(0.0223)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.7806841  0.36206897 0.20338983]\n",
      "- f1 (average): 0.4487143002178275\n",
      "- accuracy: 0.6577380895614624\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0b4346057d04034bcd0923704588b53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0874210596084595\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.1935654878616333\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9624836119738492 || Accuracy: 0.6210995316505432 || F1-score: 0.25542315918117936\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.7057756781578064\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.6206573247909546\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.8404642180962996 || Accuracy: 0.6389301419258118 || F1-score: 0.44752010597029046\n",
      "Early stopping at epoch 172!\n",
      "Accuracy on dataset of size 672: 64.73213958740234 %.\n",
      "Average loss: 0.8940324891697277\n",
      "proportion of labels in prediction: [tensor(0.8259), tensor(0.1414), tensor(0.0327)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.76796715 0.3755102  0.24      ]\n",
      "- f1 (average): 0.46115911662406234\n",
      "- accuracy: 0.6473214030265808\n"
     ]
    }
   ],
   "source": [
    "path_history = obtain_mean_history(pooled_sum_pretrained, path_specifics)\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=path_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478b3eb8",
   "metadata": {},
   "source": [
    "### CLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "cf1b1ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Adding 'time_encoding' and feature...\n",
      "[INFO] Adding 'time_diff' and feature...\n",
      "[INFO] Adding 'timeline_index' feature...\n",
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e967de6059c4c1abfb7b67932a4bf14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** hidden_dim: 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9df466def1ad41d0abbef761ad492c62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0739248991012573\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0877466201782227\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9801731163805182 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.8488182425498962\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 1.0061792135238647\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.8912230188196356 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 101!\n",
      "Accuracy on dataset of size 672: 62.35118865966797 %.\n",
      "Average loss: 0.9096886840733615\n",
      "proportion of labels in prediction: [tensor(1.), tensor(0.), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.76810266 0.         0.        ]\n",
      "- f1 (average): 0.25603421937060805\n",
      "- accuracy: 0.6235119104385376\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "965f6df35bfd4b60a77c59927a564e7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.116005539894104\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0968785285949707\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0649296695535833 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.9889340400695801\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.43216484785079956\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.8960904695770957 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 101!\n",
      "Accuracy on dataset of size 672: 62.35118865966797 %.\n",
      "Average loss: 0.9185672185637734\n",
      "proportion of labels in prediction: [tensor(1.), tensor(0.), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.76810266 0.         0.        ]\n",
      "- f1 (average): 0.25603421937060805\n",
      "- accuracy: 0.6235119104385376\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11e019bacb18424389e41dc2247a3359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.100395917892456\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0688506364822388\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0630166205492886 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.9245232343673706\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.6860083341598511\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.8921460184183988 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 101!\n",
      "Accuracy on dataset of size 672: 62.35118865966797 %.\n",
      "Average loss: 0.9267005866224115\n",
      "proportion of labels in prediction: [tensor(1.), tensor(0.), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.76810266 0.         0.        ]\n",
      "- f1 (average): 0.25603421937060805\n",
      "- accuracy: 0.6235119104385376\n"
     ]
    }
   ],
   "source": [
    "path_history = obtain_mean_history(pooled_cls_pretrained, path_specifics)\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=path_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301db045",
   "metadata": {},
   "source": [
    "## Fine-tuned BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1ef4e0",
   "metadata": {},
   "source": [
    "### Mean pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "cca3c6a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pooled_mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[175], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m path_history \u001b[38;5;241m=\u001b[39m obtain_mean_history(\u001b[43mpooled_mean\u001b[49m, path_specifics)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hidden_dim \u001b[38;5;129;01min\u001b[39;00m hidden_dim_trials:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m********** hidden_dim: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhidden_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pooled_mean' is not defined"
     ]
    }
   ],
   "source": [
    "path_history = obtain_mean_history(pooled_mean, path_specifics)\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=path_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b440e280",
   "metadata": {},
   "source": [
    "### Max pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04571fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_history = obtain_mean_history(pooled_max, path_specifics)\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=path_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e4d944",
   "metadata": {},
   "source": [
    "### Sum pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f49d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_history = obtain_mean_history(pooled_sum, path_specifics)\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=path_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7defa49",
   "metadata": {},
   "source": [
    "### CLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c60f8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_history = obtain_mean_history(pooled_cls, path_specifics)\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=path_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e021216",
   "metadata": {},
   "source": [
    "# Baseline 3: LSTM classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b60aa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5269ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b082eb04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b4dafc3",
   "metadata": {},
   "source": [
    "# Baseline 4: FFN using signatures\n",
    "\n",
    "First, we dimension reduce these and then take signatures. We use the path signature as input to the FFN for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5b32cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_specifics = {\"pad_by\": \"history\",\n",
    "                  \"zero_padding\": False,\n",
    "                  \"method\": \"k_last\",\n",
    "                  \"k\": 5,\n",
    "                  \"time_feature\": None,\n",
    "                  \"embeddings\": \"dim_reduced\",\n",
    "                  \"include_current_embedding\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4992104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_signatures_history(embeddings, path_specifics, dimension, sig_depth):\n",
    "    # dimension reduce\n",
    "    reduction = nlpsig.DimReduce(method=\"gaussian_random_projection\", n_components=dimension)\n",
    "    embeddings_reduced = reduction.fit_transform(embeddings, random_state=seed)\n",
    "    \n",
    "    paths = nlpsig.PrepareData(anno_mi,\n",
    "                               id_column=\"transcript_id\",\n",
    "                               label_column=\"client_talk_type\",\n",
    "                               embeddings=embeddings,\n",
    "                               embeddings_reduced=embeddings_reduced)\n",
    "    path = paths.pad(**path_specifics)\n",
    "    # remove last two columns (which contains the id and the label)\n",
    "    path = path[client_index][:,:,:-2].astype(\"float\")\n",
    "    \n",
    "    # convert to torch tensor to compute signature using signatory\n",
    "    path = torch.from_numpy(path).float()\n",
    "    return signatory.signature(path, sig_depth).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d86f84c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = 5\n",
    "sig_depth = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5db86f",
   "metadata": {},
   "source": [
    "## SBERT 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e615c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] - columns beginning with 'd' denote the dimension reduced embeddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Adding 'time_encoding' and feature...\n",
      "[INFO] Adding 'time_diff' and feature...\n",
      "[INFO] Adding 'timeline_index' feature...\n",
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d946aa8053db4ed1a9b7588d43fc6359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signature_history.shape = torch.Size([6725, 780])\n",
      "\n",
      "********** hidden_dim: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2844181214.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "313ab2b1617a4f248a4f75ac3fbba6c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1296181678771973\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.1276369094848633\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.1229537291960283 || Accuracy: 0.12184249609708786 || F1-score: 0.07240618101545253\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.8314493894577026\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.9525586366653442\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.9158739393407648 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 106!\n",
      "Accuracy on dataset of size 672: 62.35118865966797 %.\n",
      "Average loss: 0.9345286271788857\n",
      "proportion of labels in prediction: [tensor(1.), tensor(0.), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.76810266 0.         0.        ]\n",
      "- f1 (average): 0.25603421937060805\n",
      "- accuracy: 0.6235119104385376\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2844181214.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1507f087b7fc4d8c84959d6a849ff7ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1389586925506592\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.1151443719863892\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.1336447325619785 || Accuracy: 0.1367013305425644 || F1-score: 0.11381436427262122\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.9477335810661316\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.45682698488235474\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.9138921065763994 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 104!\n",
      "Accuracy on dataset of size 672: 62.35118865966797 %.\n",
      "Average loss: 0.931088463826613\n",
      "proportion of labels in prediction: [tensor(1.), tensor(0.), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.76810266 0.         0.        ]\n",
      "- f1 (average): 0.25603421937060805\n",
      "- accuracy: 0.6235119104385376\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2844181214.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a33007a95b9c4d4eb81548edd356ddad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0805878639221191\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0740684270858765\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0679300915111194 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.936995804309845\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.6773010492324829\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.9045545350421559 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 101!\n",
      "Accuracy on dataset of size 672: 62.35118865966797 %.\n",
      "Average loss: 0.9375779899683866\n",
      "proportion of labels in prediction: [tensor(1.), tensor(0.), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.76810266 0.         0.        ]\n",
      "- f1 (average): 0.25603421937060805\n",
      "- accuracy: 0.6235119104385376\n"
     ]
    }
   ],
   "source": [
    "signature_history = obtain_signatures_history(sbert_768_embeddings, path_specifics, dimension, sig_depth)\n",
    "print(f\"signature_history.shape = {signature_history.shape}\")\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=signature_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3784c7",
   "metadata": {},
   "source": [
    "## SBERT 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a31f58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] - columns beginning with 'd' denote the dimension reduced embeddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Adding 'time_encoding' and feature...\n",
      "[INFO] Adding 'time_diff' and feature...\n",
      "[INFO] Adding 'timeline_index' feature...\n",
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3dbe4976bf944ef9a260d9d6e6a2189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signature_history.shape = torch.Size([6725, 780])\n",
      "\n",
      "********** hidden_dim: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2844181214.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16fe8aa20b684238859d3f9737ec96cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1300742626190186\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.1269279718399048\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.1223173466595737 || Accuracy: 0.12184249609708786 || F1-score: 0.07240618101545253\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.805180549621582\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 1.0327091217041016\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.9099448052319613 || Accuracy: 0.6240713000297546 || F1-score: 0.2598877839178142\n",
      "Early stopping at epoch 105!\n",
      "Accuracy on dataset of size 672: 62.35118865966797 %.\n",
      "Average loss: 0.9492884224111383\n",
      "proportion of labels in prediction: [tensor(1.), tensor(0.), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.76810266 0.         0.        ]\n",
      "- f1 (average): 0.25603421937060805\n",
      "- accuracy: 0.6235119104385376\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2844181214.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ac23b6813284fcd949d9812ca96084a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1388251781463623\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.1151952743530273\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.133223761211742 || Accuracy: 0.12927190959453583 || F1-score: 0.10275403608736942\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.9606556296348572\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.45526576042175293\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.9071839939464222 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 104!\n",
      "Accuracy on dataset of size 672: 62.35118865966797 %.\n",
      "Average loss: 0.9430014870383523\n",
      "proportion of labels in prediction: [tensor(1.), tensor(0.), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.76810266 0.         0.        ]\n",
      "- f1 (average): 0.25603421937060805\n",
      "- accuracy: 0.6235119104385376\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2844181214.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf6a721b545142f6a66f2f8e48b95f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0802912712097168\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0748323202133179\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0678671490062366 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.9114990234375\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.6415907740592957\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.9043075767430392 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Early stopping at epoch 101!\n",
      "Accuracy on dataset of size 672: 62.35118865966797 %.\n",
      "Average loss: 0.9514352083206177\n",
      "proportion of labels in prediction: [tensor(1.), tensor(0.), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.76810266 0.         0.        ]\n",
      "- f1 (average): 0.25603421937060805\n",
      "- accuracy: 0.6235119104385376\n"
     ]
    }
   ],
   "source": [
    "signature_history = obtain_signatures_history(sbert_384_embeddings, path_specifics, dimension, sig_depth)\n",
    "print(f\"signature_history.shape = {signature_history.shape}\")\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=signature_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ef39e5",
   "metadata": {},
   "source": [
    "## Pretrained BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c5f89e",
   "metadata": {},
   "source": [
    "### Mean pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a4da6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] - columns beginning with 'd' denote the dimension reduced embeddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Adding 'time_encoding' and feature...\n",
      "[INFO] Adding 'time_diff' and feature...\n",
      "[INFO] Adding 'timeline_index' feature...\n",
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc56957996f54098a41b74dadc69d561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signature_history.shape = torch.Size([6725, 780])\n",
      "\n",
      "********** hidden_dim: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2844181214.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e44a50b001e4428e9e45977c21cf98dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 3.7441978454589844\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 6.150984287261963\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 3.259506008841775 || Accuracy: 0.3789004385471344 || F1-score: 0.2955251625410387\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.5173958539962769\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.5215308666229248\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 1.6734585491093723 || Accuracy: 0.5378900170326233 || F1-score: 0.30862251887560127\n",
      "Early stopping at epoch 163!\n",
      "Accuracy on dataset of size 672: 55.654762268066406 %.\n",
      "Average loss: 1.819061886180531\n",
      "proportion of labels in prediction: [tensor(0.8452), tensor(0.1101), tensor(0.0446)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.72745694 0.11607143 0.03007519]\n",
      "- f1 (average): 0.2912011855880837\n",
      "- accuracy: 0.5565476417541504\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2844181214.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aa0422b3fd64c319957ba26e6de1849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 2.1577088832855225\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.8434455394744873\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.41989220272411 || Accuracy: 0.4041604697704315 || F1-score: 0.3026431397979094\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.7993342876434326\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.29983827471733093\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 1.125967020338232 || Accuracy: 0.5854383111000061 || F1-score: 0.2882955502925611\n",
      "Early stopping at epoch 111!\n",
      "Accuracy on dataset of size 672: 58.779762268066406 %.\n",
      "Average loss: 1.2293318726799705\n",
      "proportion of labels in prediction: [tensor(0.9062), tensor(0.0759), tensor(0.0179)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.74513619 0.10945274 0.0173913 ]\n",
      "- f1 (average): 0.29066007581222064\n",
      "- accuracy: 0.5877976417541504\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2844181214.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2a7b634d61f45bf9dc30ef678171f63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.4558333158493042\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.6769936084747314\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.1019763621416958 || Accuracy: 0.5898959636688232 || F1-score: 0.3101851851851852\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.8751983046531677\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.7145705223083496\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.9868716489184987 || Accuracy: 0.6166418790817261 || F1-score: 0.30221593665319496\n",
      "Early stopping at epoch 104!\n",
      "Accuracy on dataset of size 672: 61.904762268066406 %.\n",
      "Average loss: 1.0826639424670825\n",
      "proportion of labels in prediction: [tensor(0.9613), tensor(0.0342), tensor(0.0045)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.76431925 0.09248555 0.01886792]\n",
      "- f1 (average): 0.29189090749584695\n",
      "- accuracy: 0.6190476417541504\n"
     ]
    }
   ],
   "source": [
    "signature_history = obtain_signatures_history(pooled_mean_pretrained, path_specifics, dimension, sig_depth)\n",
    "print(f\"signature_history.shape = {signature_history.shape}\")\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=signature_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817155b3",
   "metadata": {},
   "source": [
    "### Max pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a50fc0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] - columns beginning with 'd' denote the dimension reduced embeddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Adding 'time_encoding' and feature...\n",
      "[INFO] Adding 'time_diff' and feature...\n",
      "[INFO] Adding 'timeline_index' feature...\n",
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "897a2963cfe144788c4402f259dc544b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signature_history.shape = torch.Size([6725, 780])\n",
      "\n",
      "********** hidden_dim: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2844181214.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd8a58e3bdad40778b566156a469a3b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 21.10601234436035\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 12.224977493286133\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 10.66905671899969 || Accuracy: 0.3789004385471344 || F1-score: 0.2881857171991343\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.7429050803184509\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.7301334738731384\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 3.5669758319854736 || Accuracy: 0.518573522567749 || F1-score: 0.30701727642276416\n",
      "Early stopping at epoch 166!\n",
      "Accuracy on dataset of size 672: 54.761905670166016 %.\n",
      "Average loss: 3.691923423246904\n",
      "proportion of labels in prediction: [tensor(0.7708), tensor(0.1920), tensor(0.0372)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.69797225 0.23655914 0.125     ]\n",
      "- f1 (average): 0.3531771305508696\n",
      "- accuracy: 0.5476190447807312\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2844181214.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8643c8fb01b4be19aaa735c93495e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 8.153376579284668\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 5.994278907775879\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 3.633267944509333 || Accuracy: 0.4160475432872772 || F1-score: 0.3395383192400591\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.9116073250770569\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.30709612369537354\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 1.4584283774549311 || Accuracy: 0.528974711894989 || F1-score: 0.2895468472826963\n",
      "Early stopping at epoch 101!\n",
      "Accuracy on dataset of size 672: 55.20833206176758 %.\n",
      "Average loss: 1.5717109658501365\n",
      "proportion of labels in prediction: [tensor(0.8393), tensor(0.1250), tensor(0.0357)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.7121058  0.14529915 0.06299213]\n",
      "- f1 (average): 0.3067990232863952\n",
      "- accuracy: 0.5520833134651184\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2844181214.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61559c0ad65b45dba2f294924c1748c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 3.3812642097473145\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.7356096506118774\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.7038150375539607 || Accuracy: 0.5572065114974976 || F1-score: 0.31691976419027806\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 1.0301419496536255\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.7248117923736572\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 1.0454745401035657 || Accuracy: 0.5958395004272461 || F1-score: 0.2886131204789224\n",
      "Early stopping at epoch 104!\n",
      "Accuracy on dataset of size 672: 58.18452453613281 %.\n",
      "Average loss: 1.1007776260375977\n",
      "proportion of labels in prediction: [tensor(0.9226), tensor(0.0625), tensor(0.0149)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.73724735 0.08333333 0.        ]\n",
      "- f1 (average): 0.27352689551919585\n",
      "- accuracy: 0.581845223903656\n"
     ]
    }
   ],
   "source": [
    "signature_history = obtain_signatures_history(pooled_max_pretrained, path_specifics, dimension, sig_depth)\n",
    "print(f\"signature_history.shape = {signature_history.shape}\")\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=signature_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5d6cf5",
   "metadata": {},
   "source": [
    "### Sum pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b8b665fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] - columns beginning with 'd' denote the dimension reduced embeddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Adding 'time_encoding' and feature...\n",
      "[INFO] Adding 'time_diff' and feature...\n",
      "[INFO] Adding 'timeline_index' feature...\n",
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8744eea3754484b92deaf9ac8502f8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signature_history.shape = torch.Size([6725, 780])\n",
      "\n",
      "********** hidden_dim: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2844181214.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9395eb1fb2e425f837e635d0829c341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 24631632.0\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 4711557.0\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 5138354.7727272725 || Accuracy: 0.30312034487724304 || F1-score: 0.24780222971615964\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 438294.0625\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 14429.716796875\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 1813593.4460227273 || Accuracy: 0.4918276369571686 || F1-score: 0.32324284347418575\n",
      "Early stopping at epoch 157!\n",
      "Accuracy on dataset of size 672: 53.27381134033203 %.\n",
      "Average loss: 2374243.1136363638\n",
      "proportion of labels in prediction: [tensor(0.7500), tensor(0.1815), tensor(0.0685)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.69122427 0.23529412 0.09395973]\n",
      "- f1 (average): 0.3401593726265802\n",
      "- accuracy: 0.5327380895614624\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2844181214.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd4fac2b5a749a3abae8a3a7b28ea1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 4012826.75\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1845360.5\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1519848.6420454546 || Accuracy: 0.3893016278743744 || F1-score: 0.33526926716779265\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 152850.84375\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 116427.6015625\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 528084.4169034091 || Accuracy: 0.5408617854118347 || F1-score: 0.3710585187801719\n",
      "Epoch: 201/10000 || Item: 0/85 || Loss: 57562.52734375\n",
      "--------------------------------------------------\n",
      "##### Epoch: 201/10000 || Loss: 136120.421875\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 201 || Loss: 382654.3352272727 || Accuracy: 0.5334323644638062 || F1-score: 0.37047981318734907\n",
      "Early stopping at epoch 254!\n",
      "Accuracy on dataset of size 672: 51.78571319580078 %.\n",
      "Average loss: 472817.9943181818\n",
      "proportion of labels in prediction: [tensor(0.7485), tensor(0.1607), tensor(0.0908)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.68112798 0.18604651 0.12195122]\n",
      "- f1 (average): 0.3297085712621743\n",
      "- accuracy: 0.5178571343421936\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2844181214.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a714af48ea784e4098d04389172bf910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 4776060.5\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 588908.25\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 848951.8991477273 || Accuracy: 0.49479940533638 || F1-score: 0.3280075672438099\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 81126.34375\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 75532.5703125\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 243149.96661931818 || Accuracy: 0.5126299858093262 || F1-score: 0.34342909084755435\n",
      "Early stopping at epoch 119!\n",
      "Accuracy on dataset of size 672: 51.636905670166016 %.\n",
      "Average loss: 230526.19850852274\n",
      "proportion of labels in prediction: [tensor(0.7143), tensor(0.2158), tensor(0.0699)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.67408231 0.23050847 0.13333333]\n",
      "- f1 (average): 0.3459747071971577\n",
      "- accuracy: 0.5163690447807312\n"
     ]
    }
   ],
   "source": [
    "signature_history = obtain_signatures_history(pooled_sum_pretrained, path_specifics, dimension, sig_depth)\n",
    "print(f\"signature_history.shape = {signature_history.shape}\")\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=signature_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398d5f11",
   "metadata": {},
   "source": [
    "### CLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f036756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] - columns beginning with 'd' denote the dimension reduced embeddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Adding 'time_encoding' and feature...\n",
      "[INFO] Adding 'time_diff' and feature...\n",
      "[INFO] Adding 'timeline_index' feature...\n",
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b63679f0664490b98ea4c5de2414aa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signature_history.shape = torch.Size([6725, 780])\n",
      "\n",
      "********** hidden_dim: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2844181214.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26cfcbfb6c9247e598fbc70cf3fddd2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 3.4455976486206055\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 4.314119815826416\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 2.479116439819336 || Accuracy: 0.37444278597831726 || F1-score: 0.29591340220143464\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.5990457534790039\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.5958691835403442\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 1.4913460124622693 || Accuracy: 0.5646359324455261 || F1-score: 0.3106762934164993\n",
      "Early stopping at epoch 170!\n",
      "Accuracy on dataset of size 672: 56.69643020629883 %.\n",
      "Average loss: 1.5198925516822122\n",
      "proportion of labels in prediction: [tensor(0.8333), tensor(0.1295), tensor(0.0372)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.72318693 0.19409283 0.0625    ]\n",
      "- f1 (average): 0.32659325081277857\n",
      "- accuracy: 0.5669642686843872\n",
      "\n",
      "********** hidden_dim: [100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2844181214.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53aa36c380a949c09955c57d25f6050f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 2.1884889602661133\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.824597716331482\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.207720864902843 || Accuracy: 0.43982169032096863 || F1-score: 0.3415113871635611\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.7532700300216675\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.5469624996185303\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 1.0839504220268943 || Accuracy: 0.5973253846168518 || F1-score: 0.31075292640596736\n",
      "Early stopping at epoch 121!\n",
      "Accuracy on dataset of size 672: 59.0773811340332 %.\n",
      "Average loss: 1.1274788813157515\n",
      "proportion of labels in prediction: [tensor(0.9241), tensor(0.0625), tensor(0.0134)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.74230769 0.10416667 0.01785714]\n",
      "- f1 (average): 0.28811050061050064\n",
      "- accuracy: 0.5907738208770752\n",
      "\n",
      "********** hidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2844181214.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c660f802fddb4683a96654246537f63d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.3909130096435547\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 0.9869392514228821\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0498062426393682 || Accuracy: 0.5824665427207947 || F1-score: 0.2753911679371505\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.8995655179023743\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.7283216714859009\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.9988734071904962 || Accuracy: 0.6136701107025146 || F1-score: 0.2773923113307563\n",
      "Early stopping at epoch 103!\n",
      "Accuracy on dataset of size 672: 61.755950927734375 %.\n",
      "Average loss: 1.0209869037974963\n",
      "proportion of labels in prediction: [tensor(0.9658), tensor(0.0312), tensor(0.0030)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.76217228 0.09356725 0.        ]\n",
      "- f1 (average): 0.2852465120353943\n",
      "- accuracy: 0.617559552192688\n"
     ]
    }
   ],
   "source": [
    "signature_history = obtain_signatures_history(pooled_cls_pretrained, path_specifics, dimension, sig_depth)\n",
    "print(f\"signature_history.shape = {signature_history.shape}\")\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=signature_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25463b4d",
   "metadata": {},
   "source": [
    "### Fine-tuned BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8753a111",
   "metadata": {},
   "source": [
    "### Mean pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f167a9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "signature_history = obtain_signatures_history(pooled_mean, path_specifics, dimension, sig_depth)\n",
    "print(f\"signature_history.shape = {signature_history.shape}\")\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=signature_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d217fd98",
   "metadata": {},
   "source": [
    "### Max pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64642b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "signature_history = obtain_signatures_history(pooled_max, path_specifics, dimension, sig_depth)\n",
    "print(f\"signature_history.shape = {signature_history.shape}\")\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=signature_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82e618b",
   "metadata": {},
   "source": [
    "### Sum pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75851dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "signature_history = obtain_signatures_history(pooled_sum, path_specifics, dimension, sig_depth)\n",
    "print(f\"signature_history.shape = {signature_history.shape}\")\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=signature_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f78d509",
   "metadata": {},
   "source": [
    "### CLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9824f00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "signature_history = obtain_signatures_history(pooled_cls, path_specifics, dimension, sig_depth)\n",
    "print(f\"signature_history.shape = {signature_history.shape}\")\n",
    "for hidden_dim in hidden_dim_trials:\n",
    "    print(f\"\\n********** hidden_dim: {hidden_dim}\")\n",
    "    implement_ffn(x_data=signature_history,\n",
    "                  y_data=y_data,\n",
    "                  hidden_dim=hidden_dim,\n",
    "                  learning_rate=learning_rate,\n",
    "                  loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd426f0",
   "metadata": {},
   "source": [
    "# StackedDeepSigNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8fa92a",
   "metadata": {},
   "source": [
    "## Obtaining path by looking at post history\n",
    "\n",
    "We can obtain a path by looking at the history of each post. Here we look at the last 10 posts (and pad with vectors of zeros if there are less than 10 posts) including the current post.\n",
    "\n",
    "We only want to consider paths that correspond to a client's utterance as we want to model a change in mood at that time. Their history will still contain the therapist's utterances too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "224df773",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_features = [\"time_encoding\", \"timeline_index\"]\n",
    "path_specifics = {\"pad_by\": \"history\",\n",
    "                  \"zero_padding\": True,\n",
    "                  \"method\": \"k_last\",\n",
    "                  \"k\": 20,\n",
    "                  \"time_feature\": time_features,\n",
    "                  \"standardise_method\": [\"minmax\", None],\n",
    "                  \"embeddings\": \"dim_reduced\",\n",
    "                  \"include_current_embedding\": True,\n",
    "                  \"pad_from_below\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c06be6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_SDSN_input(embeddings, path_specifics):\n",
    "    reduction = nlpsig.DimReduce(method=\"gaussian_random_projection\", n_components=50)\n",
    "    embeddings_reduced = reduction.fit_transform(embeddings, random_state=seed)\n",
    "    \n",
    "    paths = nlpsig.PrepareData(anno_mi,\n",
    "                               id_column=\"transcript_id\",\n",
    "                               label_column=\"client_talk_type\",\n",
    "                               embeddings=embeddings,\n",
    "                               embeddings_reduced=embeddings_reduced)\n",
    "    \n",
    "    paths.pad(**path_specifics)\n",
    "    \n",
    "    paths.array_padded = paths.array_padded[client_index]\n",
    "    paths.embeddings = paths.embeddings[client_index]\n",
    "    paths.embeddings_reduced = paths.embeddings_reduced[client_index]\n",
    "    \n",
    "    return paths.get_torch_path_for_SDSN(\n",
    "        include_time_features_in_path=True,\n",
    "        include_time_features_in_input=True,\n",
    "        include_embedding_in_input=True,\n",
    "        reduced_embeddings=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f4feda92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def implement_sdsn(x_data,\n",
    "                   y_data,\n",
    "                   sig_depth,\n",
    "                   input_channels,\n",
    "                   output_channels,\n",
    "                   lstm_hidden_dim,\n",
    "                   ffn_hidden_dim,\n",
    "                   BiLSTM,\n",
    "                   learning_rate,\n",
    "                   loss,\n",
    "                   gamma = 0):\n",
    "    SDSN_args = {\n",
    "        \"input_channels\": input_channels,\n",
    "        \"output_channels\": output_channels,\n",
    "        \"num_time_features\": len(time_features),\n",
    "        \"embedding_dim\": x_data.shape[2]-input_channels-len(time_features),\n",
    "        \"sig_depth\": sig_depth,\n",
    "        \"hidden_dim_lstm\": lstm_hidden_dim,\n",
    "        \"hidden_dim_ffn\": ffn_hidden_dim,\n",
    "        \"output_dim\": len(label_to_id),\n",
    "        \"dropout_rate\": 0.1,\n",
    "        \"augmentation_type\": \"Conv1d\",\n",
    "        \"BiLSTM\": BiLSTM,\n",
    "        \"comb_method\": \"concatenation\"\n",
    "    }\n",
    "    \n",
    "    sdsn_model = StackedDeepSigNet(**SDSN_args)\n",
    "    # print(sdsn_model)\n",
    "    \n",
    "    # split dataset\n",
    "    train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n",
    "                                       y_data=torch.tensor(y_data),\n",
    "                                       train_size=0.8,\n",
    "                                       valid_size=0.5,\n",
    "                                       shuffle=True,\n",
    "                                       as_DataLoader=True,\n",
    "                                       seed=seed)\n",
    "    \n",
    "    # define loss\n",
    "    if loss == \"focal\":    \n",
    "        criterion = FocalLoss(gamma = gamma)\n",
    "    elif loss == \"cross_entropy\":\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # define optimizer\n",
    "    optimizer = torch.optim.Adam(sdsn_model.parameters(), lr=learning_rate)\n",
    "    # define scheduler for adjusting the learning rate\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "    # scheduler = StepLR(optimizer, step_size = 10, gamma = 0.5)\n",
    "    # scheduler = CosineAnnealingWarmRestarts(optimizer, \n",
    "    #                                         T_0 = 8,# Number of iterations for the first restart\n",
    "    #                                         T_mult = 1, # A factor increases TiTi after a restart\n",
    "    #                                         eta_min = learning_rate*0.1)\n",
    "    # scheduler = None\n",
    "    \n",
    "    sdsn_model = training_pytorch(model=sdsn_model,\n",
    "                                  train_loader=train,\n",
    "                                  criterion=criterion,\n",
    "                                  optimizer=optimizer,\n",
    "                                  num_epochs=10000,\n",
    "                                  scheduler=scheduler,\n",
    "                                  valid_loader=valid,\n",
    "                                  early_stopping=True,\n",
    "                                  early_stopping_metric=\"f1\",\n",
    "                                  patience=100,\n",
    "                                  verbose=True,\n",
    "                                  verbose_epoch=100,\n",
    "                                  seed=seed)\n",
    "\n",
    "    pred, label = testing_pytorch(sdsn_model, test, criterion)\n",
    "    print(f\"proportion of labels in prediction: {[sum(pred==i)/len(pred) for i in label_to_id.values()]}\")\n",
    "    print(f\"proportion of labels in data: {[sum(label==i)/len(label) for i in label_to_id.values()]}\")\n",
    "    \n",
    "    f1_scores = metrics.f1_score(label, pred, average=None)\n",
    "    print(f\"- f1: {f1_scores}\")\n",
    "    print(f\"- f1 (average): {sum(f1_scores)/len(f1_scores)}\")\n",
    "    print(f\"- accuracy: {sum(pred==label)/len(pred)}\")\n",
    "    \n",
    "    return sdsn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3ecea63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_hidden_dim_trial = [[8,8], [12,12,8]]\n",
    "ffn_hidden_dim_trial = [[100,100], [100,100,100], [100,100,100,100]]\n",
    "sig_depth = 3\n",
    "output_channels = 10\n",
    "BiLSTM = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ec0e9c",
   "metadata": {},
   "source": [
    "## SBERT 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "57cabd51",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] - columns beginning with 'd' denote the dimension reduced embeddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Adding 'time_encoding' and feature...\n",
      "[INFO] Adding 'time_diff' and feature...\n",
      "[INFO] Adding 'timeline_index' feature...\n",
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe9d1423e1a04e94b686f39f4fc9e8c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] The path was created for each item in the dataframe, by looking at its history, so to include embeddings in the FFN input, we concatenate the embeddings for each sentence / text.\n",
      "\n",
      "********** lstm_hidden_dim: [8, 8] || ffnhidden_dim: [100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_74263/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f6e08ecd99e43689b4145a61fbbc1f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0969207286834717\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0233066082000732\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.023309436711398 || Accuracy: 0.46359583735466003 || F1-score: 0.3083223136970524\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.8304529190063477\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.41528797149658203\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7428785833445463 || Accuracy: 0.6745913624763489 || F1-score: 0.4143467517817599\n",
      "Epoch: 201/10000 || Item: 0/85 || Loss: 0.782322347164154\n",
      "--------------------------------------------------\n",
      "##### Epoch: 201/10000 || Loss: 1.4106078147888184\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 201 || Loss: 0.7343586520715193 || Accuracy: 0.6745913624763489 || F1-score: 0.41507797124235485\n",
      "Early stopping at epoch 211!\n",
      "Accuracy on dataset of size 672: 68.45237731933594 %.\n",
      "Average loss: 0.7534834471615878\n",
      "proportion of labels in prediction: [tensor(0.7768), tensor(0.2232), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.82678002 0.47333333 0.        ]\n",
      "- f1 (average): 0.43337111819577284\n",
      "- accuracy: 0.6845238208770752\n",
      "\n",
      "********** lstm_hidden_dim: [8, 8] || ffnhidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_74263/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae8e2887669849b6850a4a6746619230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.12066650390625\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.067062497138977\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.054376938126304 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.8029945492744446\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.28940171003341675\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.749141666022214 || Accuracy: 0.6760772466659546 || F1-score: 0.41843096051133877\n",
      "Epoch: 201/10000 || Item: 0/85 || Loss: 0.7253010272979736\n",
      "--------------------------------------------------\n",
      "##### Epoch: 201/10000 || Loss: 0.31466588377952576\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 201 || Loss: 0.7473348433321173 || Accuracy: 0.6775631308555603 || F1-score: 0.41826697015446385\n",
      "Early stopping at epoch 201!\n",
      "Accuracy on dataset of size 672: 67.41071319580078 %.\n",
      "Average loss: 0.7641120823946866\n",
      "proportion of labels in prediction: [tensor(0.7798), tensor(0.2202), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.82078473 0.44295302 0.        ]\n",
      "- f1 (average): 0.4212459165735514\n",
      "- accuracy: 0.6741071343421936\n",
      "\n",
      "********** lstm_hidden_dim: [8, 8] || ffnhidden_dim: [100, 100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_74263/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c34606ebccf24c63817dfa19e2a8ea3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0944191217422485\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0965360403060913\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0433697700500488 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.9591290354728699\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.40191689133644104\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7622772238471291 || Accuracy: 0.6671619415283203 || F1-score: 0.4036286381697556\n",
      "Epoch: 201/10000 || Item: 0/85 || Loss: 0.6861003637313843\n",
      "--------------------------------------------------\n",
      "##### Epoch: 201/10000 || Loss: 1.047218680381775\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 201 || Loss: 0.7436184287071228 || Accuracy: 0.6760772466659546 || F1-score: 0.4207838982127052\n",
      "Early stopping at epoch 238!\n",
      "Accuracy on dataset of size 672: 67.26190185546875 %.\n",
      "Average loss: 0.7572173747149381\n",
      "proportion of labels in prediction: [tensor(0.7560), tensor(0.2440), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.81984898 0.45859873 0.        ]\n",
      "- f1 (average): 0.4261492337678103\n",
      "- accuracy: 0.6726190447807312\n",
      "\n",
      "********** lstm_hidden_dim: [12, 12, 8] || ffnhidden_dim: [100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_74263/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "891fcfe699a94cbebd3ac19cb78d4743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1812686920166016\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0823497772216797\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0679176612333818 || Accuracy: 0.39524516463279724 || F1-score: 0.2803170874828573\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.7879034876823425\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.43962371349334717\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7241770408370278 || Accuracy: 0.679049015045166 || F1-score: 0.4252490292550386\n",
      "Epoch: 201/10000 || Item: 0/85 || Loss: 0.7371249198913574\n",
      "--------------------------------------------------\n",
      "##### Epoch: 201/10000 || Loss: 1.613956332206726\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 201 || Loss: 0.7125839320096102 || Accuracy: 0.679049015045166 || F1-score: 0.43306966655334533\n",
      "Early stopping at epoch 224!\n",
      "Accuracy on dataset of size 672: 67.70833587646484 %.\n",
      "Average loss: 0.7561800696633079\n",
      "proportion of labels in prediction: [tensor(0.7827), tensor(0.2158), tensor(0.0015)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.81693122 0.46779661 0.        ]\n",
      "- f1 (average): 0.42824260903356953\n",
      "- accuracy: 0.6770833134651184\n",
      "\n",
      "********** lstm_hidden_dim: [12, 12, 8] || ffnhidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_74263/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cfdea44b45d47f080223059705b006e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0663707256317139\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0678725242614746\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0295921997590498 || Accuracy: 0.5542347431182861 || F1-score: 0.32125706214689265\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.8314594030380249\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.26384830474853516\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7574997706846758 || Accuracy: 0.6775631308555603 || F1-score: 0.41700782877253467\n",
      "Early stopping at epoch 199!\n",
      "Accuracy on dataset of size 672: 66.96428680419922 %.\n",
      "Average loss: 0.7706493789499457\n",
      "proportion of labels in prediction: [tensor(0.7976), tensor(0.2024), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.81675393 0.41958042 0.        ]\n",
      "- f1 (average): 0.4121114487606634\n",
      "- accuracy: 0.6696428656578064\n",
      "\n",
      "********** lstm_hidden_dim: [12, 12, 8] || ffnhidden_dim: [100, 100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_74263/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a84ed19d576f4a34bae7bf144ece189d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1014739274978638\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.110789179801941\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0646752227436413 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.9555978775024414\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.3831523060798645\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7607014016671614 || Accuracy: 0.668647825717926 || F1-score: 0.40675829701642635\n",
      "Epoch: 201/10000 || Item: 0/85 || Loss: 0.6827191114425659\n",
      "--------------------------------------------------\n",
      "##### Epoch: 201/10000 || Loss: 0.9079384207725525\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 201 || Loss: 0.7459247383204374 || Accuracy: 0.668647825717926 || F1-score: 0.40772953940552764\n",
      "Early stopping at epoch 215!\n",
      "Accuracy on dataset of size 672: 66.96428680419922 %.\n",
      "Average loss: 0.7774955847046592\n",
      "proportion of labels in prediction: [tensor(0.7738), tensor(0.2262), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.8200213  0.43046358 0.        ]\n",
      "- f1 (average): 0.4168282918044888\n",
      "- accuracy: 0.6696428656578064\n"
     ]
    }
   ],
   "source": [
    "x_data, input_channels = obtain_SDSN_input(sbert_768_embeddings, path_specifics)\n",
    "for lstm_hidden_dim in lstm_hidden_dim_trial:\n",
    "    for ffn_hidden_dim in ffn_hidden_dim_trial:\n",
    "        print(f\"\\n********** lstm_hidden_dim: {lstm_hidden_dim} \"\n",
    "              f\"|| ffnhidden_dim: {ffn_hidden_dim}\")\n",
    "        implement_sdsn(x_data=x_data,\n",
    "                       y_data=y_data,\n",
    "                       sig_depth=sig_depth,\n",
    "                       input_channels=input_channels,\n",
    "                       output_channels=output_channels,\n",
    "                       lstm_hidden_dim=lstm_hidden_dim,\n",
    "                       ffn_hidden_dim=ffn_hidden_dim,\n",
    "                       BiLSTM=BiLSTM,\n",
    "                       learning_rate=learning_rate,\n",
    "                       loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fdae63",
   "metadata": {},
   "source": [
    "## SBERT 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a8abadb7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] - columns beginning with 'd' denote the dimension reduced embeddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Adding 'time_encoding' and feature...\n",
      "[INFO] Adding 'time_diff' and feature...\n",
      "[INFO] Adding 'timeline_index' feature...\n",
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "804c2530ea8843d398cf0731a5542160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] The path was created for each item in the dataframe, by looking at its history, so to include embeddings in the FFN input, we concatenate the embeddings for each sentence / text.\n",
      "\n",
      "********** lstm_hidden_dim: [8, 8] || ffnhidden_dim: [100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1a5dbf5baee47d3981bffba3040d8a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0764864683151245\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0873043537139893\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.020038989457217 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.8260774612426758\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.23308005928993225\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7744331630793485 || Accuracy: 0.6552748680114746 || F1-score: 0.3925271304903637\n",
      "Early stopping at epoch 198!\n",
      "Accuracy on dataset of size 672: 65.32737731933594 %.\n",
      "Average loss: 0.7782417156479575\n",
      "proportion of labels in prediction: [tensor(0.7768), tensor(0.2232), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.80765143 0.39333333 0.        ]\n",
      "- f1 (average): 0.400328255992443\n",
      "- accuracy: 0.6532738208770752\n",
      "\n",
      "********** lstm_hidden_dim: [8, 8] || ffnhidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c06712cf300b487dbc6fae354aa84b24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.072562336921692\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.1152584552764893\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0209932814944873 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.8427107930183411\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.40092605352401733\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7744652357968417 || Accuracy: 0.6493313312530518 || F1-score: 0.37042501925131033\n",
      "Epoch: 201/10000 || Item: 0/85 || Loss: 0.7869120240211487\n",
      "--------------------------------------------------\n",
      "##### Epoch: 201/10000 || Loss: 0.3241257071495056\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 201 || Loss: 0.7755436572161588 || Accuracy: 0.6508172154426575 || F1-score: 0.3741355157413551\n",
      "Early stopping at epoch 207!\n",
      "Accuracy on dataset of size 672: 65.77381134033203 %.\n",
      "Average loss: 0.767627315087752\n",
      "proportion of labels in prediction: [tensor(0.8006), tensor(0.1994), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.81504702 0.36619718 0.        ]\n",
      "- f1 (average): 0.39374806834738846\n",
      "- accuracy: 0.6577380895614624\n",
      "\n",
      "********** lstm_hidden_dim: [8, 8] || ffnhidden_dim: [100, 100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9a3c6b95ac649b88b6f2fcce3dadbdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1326649188995361\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.1599905490875244\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0839721831408413 || Accuracy: 0.2956909239292145 || F1-score: 0.217697083376074\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.9524648189544678\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.573074221611023\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7800712422891096 || Accuracy: 0.6552748680114746 || F1-score: 0.3853948816005455\n",
      "Epoch: 201/10000 || Item: 0/85 || Loss: 0.7340773344039917\n",
      "--------------------------------------------------\n",
      "##### Epoch: 201/10000 || Loss: 0.9599273800849915\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 201 || Loss: 0.7671386328610507 || Accuracy: 0.6493313312530518 || F1-score: 0.39364179029182006\n",
      "Early stopping at epoch 221!\n",
      "Accuracy on dataset of size 672: 66.07142639160156 %.\n",
      "Average loss: 0.7596183256669478\n",
      "proportion of labels in prediction: [tensor(0.7455), tensor(0.2545), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.81956522 0.41744548 0.        ]\n",
      "- f1 (average): 0.41233690008578266\n",
      "- accuracy: 0.6607142686843872\n",
      "\n",
      "********** lstm_hidden_dim: [12, 12, 8] || ffnhidden_dim: [100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03da0cf80a534bf68c8bba3c69ba83e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0632113218307495\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0915249586105347\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0186090794476597 || Accuracy: 0.549777090549469 || F1-score: 0.3190642105043819\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.824112057685852\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.31476056575775146\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7664196220311251 || Accuracy: 0.6627042889595032 || F1-score: 0.39176008431541637\n",
      "Epoch: 201/10000 || Item: 0/85 || Loss: 0.7843072414398193\n",
      "--------------------------------------------------\n",
      "##### Epoch: 201/10000 || Loss: 1.452071189880371\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 201 || Loss: 0.7611909942193464 || Accuracy: 0.6671619415283203 || F1-score: 0.40130826825070187\n",
      "Early stopping at epoch 210!\n",
      "Accuracy on dataset of size 672: 66.51786041259766 %.\n",
      "Average loss: 0.766436679796739\n",
      "proportion of labels in prediction: [tensor(0.7708), tensor(0.2292), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.81750267 0.42105263 0.        ]\n",
      "- f1 (average): 0.4128517665561984\n",
      "- accuracy: 0.6651785969734192\n",
      "\n",
      "********** lstm_hidden_dim: [12, 12, 8] || ffnhidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0243462cc08747cfb873b57fc0e18d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1042710542678833\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0711721181869507\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0595148368315264 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.8390007019042969\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.36518001556396484\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7754646431316029 || Accuracy: 0.647845447063446 || F1-score: 0.38405269615876003\n",
      "Early stopping at epoch 194!\n",
      "Accuracy on dataset of size 672: 66.96428680419922 %.\n",
      "Average loss: 0.7810252796519886\n",
      "proportion of labels in prediction: [tensor(0.7560), tensor(0.2440), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.82200647 0.43949045 0.        ]\n",
      "- f1 (average): 0.4204989727839273\n",
      "- accuracy: 0.6696428656578064\n",
      "\n",
      "********** lstm_hidden_dim: [12, 12, 8] || ffnhidden_dim: [100, 100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99e3fd7436b4552b01b0a7a48c7d001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0992392301559448\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.1072020530700684\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0587024905464866 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.9701166749000549\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.5604714751243591\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7806987599893049 || Accuracy: 0.647845447063446 || F1-score: 0.38134144816374177\n",
      "Early stopping at epoch 187!\n",
      "Accuracy on dataset of size 672: 66.51786041259766 %.\n",
      "Average loss: 0.765200219371102\n",
      "proportion of labels in prediction: [tensor(0.7738), tensor(0.2262), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.8200213  0.41059603 0.        ]\n",
      "- f1 (average): 0.4102057752481974\n",
      "- accuracy: 0.6651785969734192\n"
     ]
    }
   ],
   "source": [
    "x_data, input_channels = obtain_SDSN_input(sbert_384_embeddings, path_specifics)\n",
    "for lstm_hidden_dim in lstm_hidden_dim_trial:\n",
    "    for ffn_hidden_dim in ffn_hidden_dim_trial:\n",
    "        print(f\"\\n********** lstm_hidden_dim: {lstm_hidden_dim} \"\n",
    "              f\"|| ffnhidden_dim: {ffn_hidden_dim}\")\n",
    "        implement_sdsn(x_data=x_data,\n",
    "                       y_data=y_data,\n",
    "                       sig_depth=sig_depth,\n",
    "                       input_channels=input_channels,\n",
    "                       output_channels=output_channels,\n",
    "                       lstm_hidden_dim=lstm_hidden_dim,\n",
    "                       ffn_hidden_dim=ffn_hidden_dim,\n",
    "                       BiLSTM=BiLSTM,\n",
    "                       learning_rate=learning_rate,\n",
    "                       loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8da1ce",
   "metadata": {},
   "source": [
    "## Pretrained BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350c5bf6",
   "metadata": {},
   "source": [
    "### Mean pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c3c0ccc3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] - columns beginning with 'd' denote the dimension reduced embeddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Adding 'time_encoding' and feature...\n",
      "[INFO] Adding 'time_diff' and feature...\n",
      "[INFO] Adding 'timeline_index' feature...\n",
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d68c1b440dc04fc69e866cfd8aa3a2b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] The path was created for each item in the dataframe, by looking at its history, so to include embeddings in the FFN input, we concatenate the embeddings for each sentence / text.\n",
      "\n",
      "********** lstm_hidden_dim: [8, 8] || ffnhidden_dim: [100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79bb5d7d4c7f42d4ae250a2fc4a79482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.092307686805725\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0484129190444946\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9850388440218839 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.7668509483337402\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.4241316318511963\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7366883158683777 || Accuracy: 0.6805348992347717 || F1-score: 0.4956525368198217\n",
      "Early stopping at epoch 167!\n",
      "Accuracy on dataset of size 672: 67.70833587646484 %.\n",
      "Average loss: 0.7894256440075961\n",
      "proportion of labels in prediction: [tensor(0.7604), tensor(0.1905), tensor(0.0491)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.79784946 0.47482014 0.26470588]\n",
      "- f1 (average): 0.5124584962011415\n",
      "- accuracy: 0.6770833134651184\n",
      "\n",
      "********** lstm_hidden_dim: [8, 8] || ffnhidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f588247657c46708f676b83c1615273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.086983561515808\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0657081604003906\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0310974771326238 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.704171895980835\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.16407299041748047\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7339336167682301 || Accuracy: 0.6775631308555603 || F1-score: 0.5232700427496731\n",
      "Early stopping at epoch 182!\n",
      "Accuracy on dataset of size 672: 68.1547622680664 %.\n",
      "Average loss: 0.7501686919819225\n",
      "proportion of labels in prediction: [tensor(0.7396), tensor(0.1860), tensor(0.0744)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.80786026 0.46545455 0.31372549]\n",
      "- f1 (average): 0.5290134325531192\n",
      "- accuracy: 0.6815476417541504\n",
      "\n",
      "********** lstm_hidden_dim: [8, 8] || ffnhidden_dim: [100, 100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5267cb76f9364845a3542d981b1654f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1314183473587036\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.1348956823349\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0881675698540427 || Accuracy: 0.31797918677330017 || F1-score: 0.23029510885582852\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.891852080821991\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.16898664832115173\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7633115215735002 || Accuracy: 0.6775631308555603 || F1-score: 0.49486527754501614\n",
      "Early stopping at epoch 191!\n",
      "Accuracy on dataset of size 672: 67.41071319580078 %.\n",
      "Average loss: 0.7805675376545299\n",
      "proportion of labels in prediction: [tensor(0.7232), tensor(0.2277), tensor(0.0491)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.80883978 0.4620462  0.25      ]\n",
      "- f1 (average): 0.5069619945419956\n",
      "- accuracy: 0.6741071343421936\n",
      "\n",
      "********** lstm_hidden_dim: [12, 12, 8] || ffnhidden_dim: [100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b688a327b54c509132ea113dc2f88b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.2139078378677368\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0814762115478516\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0501754392277112 || Accuracy: 0.48885586857795715 || F1-score: 0.32113079729025534\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.7437731623649597\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.43895959854125977\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7175708358938043 || Accuracy: 0.6835066676139832 || F1-score: 0.5386153975538449\n",
      "Epoch: 201/10000 || Item: 0/85 || Loss: 0.7395185232162476\n",
      "--------------------------------------------------\n",
      "##### Epoch: 201/10000 || Loss: 1.5451140403747559\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 201 || Loss: 0.7181831652467902 || Accuracy: 0.6835066676139832 || F1-score: 0.5474054215779308\n",
      "Early stopping at epoch 208!\n",
      "Accuracy on dataset of size 672: 68.30357360839844 %.\n",
      "Average loss: 0.7640142224051736\n",
      "proportion of labels in prediction: [tensor(0.7217), tensor(0.2024), tensor(0.0759)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.80088496 0.4965035  0.33766234]\n",
      "- f1 (average): 0.5450169299726821\n",
      "- accuracy: 0.6830357313156128\n",
      "\n",
      "********** lstm_hidden_dim: [12, 12, 8] || ffnhidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e966df9401c9496d9c3b64f062ae38b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.116066813468933\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.1265699863433838\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0330844088034197 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.7519819140434265\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.15710219740867615\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.738428451798179 || Accuracy: 0.689450204372406 || F1-score: 0.5121958121109225\n",
      "Early stopping at epoch 179!\n",
      "Accuracy on dataset of size 672: 68.1547622680664 %.\n",
      "Average loss: 0.7435725439678539\n",
      "proportion of labels in prediction: [tensor(0.7336), tensor(0.1994), tensor(0.0670)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.80921053 0.46478873 0.31081081]\n",
      "- f1 (average): 0.5282700231736555\n",
      "- accuracy: 0.6815476417541504\n",
      "\n",
      "********** lstm_hidden_dim: [12, 12, 8] || ffnhidden_dim: [100, 100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e92db6cfae44ee0abc5d0d6ac355a43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1306880712509155\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0889372825622559\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0812130624597722 || Accuracy: 0.365527480840683 || F1-score: 0.2590785171999675\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.9445035457611084\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.1216757744550705\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.732742808081887 || Accuracy: 0.6760772466659546 || F1-score: 0.497789699856748\n",
      "Epoch: 201/10000 || Item: 0/85 || Loss: 0.679328978061676\n",
      "--------------------------------------------------\n",
      "##### Epoch: 201/10000 || Loss: 0.5225780606269836\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 201 || Loss: 0.7117806483398784 || Accuracy: 0.6879643201828003 || F1-score: 0.5363720172205836\n",
      "Early stopping at epoch 214!\n",
      "Accuracy on dataset of size 672: 68.30357360839844 %.\n",
      "Average loss: 0.7561947703361511\n",
      "proportion of labels in prediction: [tensor(0.7307), tensor(0.1964), tensor(0.0729)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.81098901 0.4751773  0.30263158]\n",
      "- f1 (average): 0.529599298300306\n",
      "- accuracy: 0.6830357313156128\n"
     ]
    }
   ],
   "source": [
    "x_data, input_channels = obtain_SDSN_input(pooled_mean_pretrained, path_specifics)\n",
    "for lstm_hidden_dim in lstm_hidden_dim_trial:\n",
    "    for ffn_hidden_dim in ffn_hidden_dim_trial:\n",
    "        print(f\"\\n********** lstm_hidden_dim: {lstm_hidden_dim} \"\n",
    "              f\"|| ffnhidden_dim: {ffn_hidden_dim}\")\n",
    "        implement_sdsn(x_data=x_data,\n",
    "                       y_data=y_data,\n",
    "                       sig_depth=sig_depth,\n",
    "                       input_channels=input_channels,\n",
    "                       output_channels=output_channels,\n",
    "                       lstm_hidden_dim=lstm_hidden_dim,\n",
    "                       ffn_hidden_dim=ffn_hidden_dim,\n",
    "                       BiLSTM=BiLSTM,\n",
    "                       learning_rate=learning_rate,\n",
    "                       loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e3879b",
   "metadata": {},
   "source": [
    "### Max pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cd20dc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] - columns beginning with 'd' denote the dimension reduced embeddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Adding 'time_encoding' and feature...\n",
      "[INFO] Adding 'time_diff' and feature...\n",
      "[INFO] Adding 'timeline_index' feature...\n",
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a6d2da68ff4f20ae6ef1cb4444b156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] The path was created for each item in the dataframe, by looking at its history, so to include embeddings in the FFN input, we concatenate the embeddings for each sentence / text.\n",
      "\n",
      "********** lstm_hidden_dim: [8, 8] || ffnhidden_dim: [100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2211192c75e445386e6df52fc63619e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1773746013641357\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0755505561828613\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0181242064996199 || Accuracy: 0.6077265739440918 || F1-score: 0.2556888677370605\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.814765214920044\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.29490041732788086\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7640200203115289 || Accuracy: 0.6597325205802917 || F1-score: 0.4036568213783404\n",
      "Epoch: 201/10000 || Item: 0/85 || Loss: 0.7249236702919006\n",
      "--------------------------------------------------\n",
      "##### Epoch: 201/10000 || Loss: 1.3043550252914429\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 201 || Loss: 0.767925045707009 || Accuracy: 0.6627042889595032 || F1-score: 0.41408009700438103\n",
      "Early stopping at epoch 209!\n",
      "Accuracy on dataset of size 672: 64.58333587646484 %.\n",
      "Average loss: 0.7653376189145175\n",
      "proportion of labels in prediction: [tensor(0.7560), tensor(0.2440), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.798274   0.40764331 0.        ]\n",
      "- f1 (average): 0.4019724380864694\n",
      "- accuracy: 0.6458333134651184\n",
      "\n",
      "********** lstm_hidden_dim: [8, 8] || ffnhidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f172fc17575447abbaeef76a4533e9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1322405338287354\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0741022825241089\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0415133454582908 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.8184701204299927\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.3524358868598938\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7725036577744917 || Accuracy: 0.6508172154426575 || F1-score: 0.3933262711864407\n",
      "Early stopping at epoch 172!\n",
      "Accuracy on dataset of size 672: 65.0297622680664 %.\n",
      "Average loss: 0.8041848648678173\n",
      "proportion of labels in prediction: [tensor(0.7560), tensor(0.2440), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.802589   0.41401274 0.        ]\n",
      "- f1 (average): 0.4055339118724191\n",
      "- accuracy: 0.6502976417541504\n",
      "\n",
      "********** lstm_hidden_dim: [8, 8] || ffnhidden_dim: [100, 100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b46ff61bb284f43a2843dd1679f8cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.113893985748291\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.03090500831604\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0526592081243342 || Accuracy: 0.33729568123817444 || F1-score: 0.2380251394770451\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.9875040650367737\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.22272755205631256\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7775280638174578 || Accuracy: 0.6552748680114746 || F1-score: 0.39859559825315866\n",
      "Early stopping at epoch 183!\n",
      "Accuracy on dataset of size 672: 63.83928680419922 %.\n",
      "Average loss: 0.7828834273598411\n",
      "proportion of labels in prediction: [tensor(0.7485), tensor(0.2515), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.79609544 0.38871473 0.        ]\n",
      "- f1 (average): 0.39493672607592867\n",
      "- accuracy: 0.6383928656578064\n",
      "\n",
      "********** lstm_hidden_dim: [12, 12, 8] || ffnhidden_dim: [100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "296ea524f8864d73817d83126e5966c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.2054002285003662\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.063188910484314\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0061352957378735 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.8090469837188721\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.27597397565841675\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7800249023870989 || Accuracy: 0.6508172154426575 || F1-score: 0.39815406460773867\n",
      "Early stopping at epoch 166!\n",
      "Accuracy on dataset of size 672: 66.66666412353516 %.\n",
      "Average loss: 0.7648813724517822\n",
      "proportion of labels in prediction: [tensor(0.7738), tensor(0.2262), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.80937167 0.45033113 0.        ]\n",
      "- f1 (average): 0.41990093260643163\n",
      "- accuracy: 0.6666666865348816\n",
      "\n",
      "********** lstm_hidden_dim: [12, 12, 8] || ffnhidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d67ce41f5ac454185b14c928effac63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0991864204406738\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0555108785629272\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0443167686462402 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.8127844929695129\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.300264447927475\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7771428335796703 || Accuracy: 0.6523030996322632 || F1-score: 0.4005143704391825\n",
      "Early stopping at epoch 171!\n",
      "Accuracy on dataset of size 672: 64.73213958740234 %.\n",
      "Average loss: 0.8017999909140847\n",
      "proportion of labels in prediction: [tensor(0.7307), tensor(0.2693), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.8043956  0.41691843 0.        ]\n",
      "- f1 (average): 0.4071046777995419\n",
      "- accuracy: 0.6473214030265808\n",
      "\n",
      "********** lstm_hidden_dim: [12, 12, 8] || ffnhidden_dim: [100, 100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d42dcb517a94baf8f9dbe2706c20488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1318442821502686\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.10346519947052\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.083070993423462 || Accuracy: 0.3878157436847687 || F1-score: 0.2741996857199964\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.9358991384506226\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.35324254631996155\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.8188418366692283 || Accuracy: 0.637444257736206 || F1-score: 0.38353693495518376\n",
      "Early stopping at epoch 139!\n",
      "Accuracy on dataset of size 672: 61.30952453613281 %.\n",
      "Average loss: 0.8005170659585432\n",
      "proportion of labels in prediction: [tensor(0.7634), tensor(0.2366), tensor(0.)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.78111588 0.31067961 0.        ]\n",
      "- f1 (average): 0.36393183049293726\n",
      "- accuracy: 0.613095223903656\n"
     ]
    }
   ],
   "source": [
    "x_data, input_channels = obtain_SDSN_input(pooled_max_pretrained, path_specifics)\n",
    "for lstm_hidden_dim in lstm_hidden_dim_trial:\n",
    "    for ffn_hidden_dim in ffn_hidden_dim_trial:\n",
    "        print(f\"\\n********** lstm_hidden_dim: {lstm_hidden_dim} \"\n",
    "              f\"|| ffnhidden_dim: {ffn_hidden_dim}\")\n",
    "        implement_sdsn(x_data=x_data,\n",
    "                       y_data=y_data,\n",
    "                       sig_depth=sig_depth,\n",
    "                       input_channels=input_channels,\n",
    "                       output_channels=output_channels,\n",
    "                       lstm_hidden_dim=lstm_hidden_dim,\n",
    "                       ffn_hidden_dim=ffn_hidden_dim,\n",
    "                       BiLSTM=BiLSTM,\n",
    "                       learning_rate=learning_rate,\n",
    "                       loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c9c694",
   "metadata": {},
   "source": [
    "### Sum pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6b5f90bc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] - columns beginning with 'd' denote the dimension reduced embeddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Adding 'time_encoding' and feature...\n",
      "[INFO] Adding 'time_diff' and feature...\n",
      "[INFO] Adding 'timeline_index' feature...\n",
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c09902691c594090ad0cb32a4dc08112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] The path was created for each item in the dataframe, by looking at its history, so to include embeddings in the FFN input, we concatenate the embeddings for each sentence / text.\n",
      "\n",
      "********** lstm_hidden_dim: [8, 8] || ffnhidden_dim: [100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ebb9cd97b9499e9fd7969a01fa191c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.2181402444839478\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.066859245300293\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.98489126292142 || Accuracy: 0.6166418790817261 || F1-score: 0.2900937871846043\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.6420540809631348\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.24827660620212555\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.716571103442799 || Accuracy: 0.7191678881645203 || F1-score: 0.6066606145470689\n",
      "Early stopping at epoch 154!\n",
      "Accuracy on dataset of size 672: 70.68452453613281 %.\n",
      "Average loss: 0.7215899445793845\n",
      "proportion of labels in prediction: [tensor(0.7411), tensor(0.1830), tensor(0.0759)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.81134133 0.52014652 0.41558442]\n",
      "- f1 (average): 0.5823574220520785\n",
      "- accuracy: 0.706845223903656\n",
      "\n",
      "********** lstm_hidden_dim: [8, 8] || ffnhidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4374b2ac17945379d8599eef9a4135b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.139095664024353\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0661197900772095\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0196888284249739 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.6759999990463257\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.10273061692714691\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.743569168177518 || Accuracy: 0.7013372778892517 || F1-score: 0.5876951020676172\n",
      "Early stopping at epoch 149!\n",
      "Accuracy on dataset of size 672: 71.13095092773438 %.\n",
      "Average loss: 0.7319929653948004\n",
      "proportion of labels in prediction: [tensor(0.7351), tensor(0.1741), tensor(0.0908)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.81270537 0.53932584 0.42682927]\n",
      "- f1 (average): 0.5929534926371822\n",
      "- accuracy: 0.711309552192688\n",
      "\n",
      "********** lstm_hidden_dim: [8, 8] || ffnhidden_dim: [100, 100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae34eac6037d4cc8af67d39996a2e4bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1228188276290894\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0800007581710815\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0646044232628562 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.7415486574172974\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.23596514761447906\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.762010167945515 || Accuracy: 0.7043090462684631 || F1-score: 0.5941088590095213\n",
      "Early stopping at epoch 156!\n",
      "Accuracy on dataset of size 672: 71.875 %.\n",
      "Average loss: 0.7243921648372303\n",
      "proportion of labels in prediction: [tensor(0.7158), tensor(0.1801), tensor(0.1042)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.81777778 0.54612546 0.47398844]\n",
      "- f1 (average): 0.6126305594462497\n",
      "- accuracy: 0.71875\n",
      "\n",
      "********** lstm_hidden_dim: [12, 12, 8] || ffnhidden_dim: [100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "def97c7ae0024c1ba5484a9adcb33bca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.2287657260894775\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.1224294900894165\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0203510902144692 || Accuracy: 0.6300148367881775 || F1-score: 0.3143051308188923\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.6053046584129333\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.24841231107711792\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7670120217583396 || Accuracy: 0.6835066676139832 || F1-score: 0.561768259551016\n",
      "Early stopping at epoch 132!\n",
      "Accuracy on dataset of size 672: 69.49404907226562 %.\n",
      "Average loss: 0.7803263447501443\n",
      "proportion of labels in prediction: [tensor(0.7411), tensor(0.1801), tensor(0.0789)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.80043621 0.47232472 0.46153846]\n",
      "- f1 (average): 0.5780997966006839\n",
      "- accuracy: 0.694940447807312\n",
      "\n",
      "********** lstm_hidden_dim: [12, 12, 8] || ffnhidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11fdab27d6bd45caaa8310bac83a7217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.034242033958435\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0073752403259277\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9958898479288275 || Accuracy: 0.6225854158401489 || F1-score: 0.26099318277276984\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.5993335247039795\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.16584578156471252\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7585349841551348 || Accuracy: 0.7087666988372803 || F1-score: 0.5956471115321761\n",
      "Early stopping at epoch 153!\n",
      "Accuracy on dataset of size 672: 71.72618865966797 %.\n",
      "Average loss: 0.725405137647282\n",
      "proportion of labels in prediction: [tensor(0.7143), tensor(0.2024), tensor(0.0833)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.82313682 0.53846154 0.44025157]\n",
      "- f1 (average): 0.6006166431586709\n",
      "- accuracy: 0.7172619104385376\n",
      "\n",
      "********** lstm_hidden_dim: [12, 12, 8] || ffnhidden_dim: [100, 100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e715961459d4a33bad3ce3cb12437ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0629332065582275\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0641825199127197\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0240105227990584 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.6873794794082642\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.13534866273403168\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7940189025618813 || Accuracy: 0.6849925518035889 || F1-score: 0.5937051930568719\n",
      "Early stopping at epoch 138!\n",
      "Accuracy on dataset of size 672: 68.75 %.\n",
      "Average loss: 0.7797680172053251\n",
      "proportion of labels in prediction: [tensor(0.7039), tensor(0.2009), tensor(0.0952)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.79147982 0.49122807 0.46706587]\n",
      "- f1 (average): 0.5832579196889048\n",
      "- accuracy: 0.6875\n"
     ]
    }
   ],
   "source": [
    "x_data, input_channels = obtain_SDSN_input(pooled_sum_pretrained, path_specifics)\n",
    "for lstm_hidden_dim in lstm_hidden_dim_trial:\n",
    "    for ffn_hidden_dim in ffn_hidden_dim_trial:\n",
    "        print(f\"\\n********** lstm_hidden_dim: {lstm_hidden_dim} \"\n",
    "              f\"|| ffnhidden_dim: {ffn_hidden_dim}\")\n",
    "        implement_sdsn(x_data=x_data,\n",
    "                       y_data=y_data,\n",
    "                       sig_depth=sig_depth,\n",
    "                       input_channels=input_channels,\n",
    "                       output_channels=output_channels,\n",
    "                       lstm_hidden_dim=lstm_hidden_dim,\n",
    "                       ffn_hidden_dim=ffn_hidden_dim,\n",
    "                       BiLSTM=BiLSTM,\n",
    "                       learning_rate=learning_rate,\n",
    "                       loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc6e416",
   "metadata": {},
   "source": [
    "### CLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a0ac49ab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] - columns beginning with 'd' denote the dimension reduced embeddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Adding 'time_encoding' and feature...\n",
      "[INFO] Adding 'time_diff' and feature...\n",
      "[INFO] Adding 'timeline_index' feature...\n",
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b02fbbdb67480ebf7bac604d1f333b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] The path was created for each item in the dataframe, by looking at its history, so to include embeddings in the FFN input, we concatenate the embeddings for each sentence / text.\n",
      "\n",
      "********** lstm_hidden_dim: [8, 8] || ffnhidden_dim: [100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4a5b4c0bf3a473e8286ad4be285221f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1161494255065918\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0059633255004883\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 0.9993689656257629 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.7387819886207581\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.28736263513565063\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7172420187429949 || Accuracy: 0.6953937411308289 || F1-score: 0.5449024011323456\n",
      "Early stopping at epoch 190!\n",
      "Accuracy on dataset of size 672: 69.94047546386719 %.\n",
      "Average loss: 0.7094901095737111\n",
      "proportion of labels in prediction: [tensor(0.7381), tensor(0.1756), tensor(0.0863)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.81530055 0.48507463 0.39751553]\n",
      "- f1 (average): 0.5659635670880232\n",
      "- accuracy: 0.699404776096344\n",
      "\n",
      "********** lstm_hidden_dim: [8, 8] || ffnhidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc7916babe244eb3a1173f16e15c2ae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1231155395507812\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0709043741226196\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.043758901682767 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.7524192333221436\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.17940518260002136\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7213849696246061 || Accuracy: 0.6864784359931946 || F1-score: 0.5292301182466207\n",
      "Early stopping at epoch 188!\n",
      "Accuracy on dataset of size 672: 68.45237731933594 %.\n",
      "Average loss: 0.730201008644971\n",
      "proportion of labels in prediction: [tensor(0.7619), tensor(0.1652), tensor(0.0729)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.80988185 0.43678161 0.34210526]\n",
      "- f1 (average): 0.5295895732763766\n",
      "- accuracy: 0.6845238208770752\n",
      "\n",
      "********** lstm_hidden_dim: [8, 8] || ffnhidden_dim: [100, 100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d49ecc3eb64264b8dded8ebdc174f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1191399097442627\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0814623832702637\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0798902186480435 || Accuracy: 0.35066863894462585 || F1-score: 0.24751491973154996\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.8690895438194275\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.2705612778663635\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7456639625809409 || Accuracy: 0.6924219727516174 || F1-score: 0.5419908486575153\n",
      "Epoch: 201/10000 || Item: 0/85 || Loss: 0.6533145904541016\n",
      "--------------------------------------------------\n",
      "##### Epoch: 201/10000 || Loss: 0.6370545625686646\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 201 || Loss: 0.7173359231515364 || Accuracy: 0.7043090462684631 || F1-score: 0.5707119983017303\n",
      "Early stopping at epoch 219!\n",
      "Accuracy on dataset of size 672: 70.98213958740234 %.\n",
      "Average loss: 0.7263262326067145\n",
      "proportion of labels in prediction: [tensor(0.7455), tensor(0.1607), tensor(0.0938)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.8173913  0.48837209 0.45783133]\n",
      "- f1 (average): 0.5878649075574289\n",
      "- accuracy: 0.7098214030265808\n",
      "\n",
      "********** lstm_hidden_dim: [12, 12, 8] || ffnhidden_dim: [100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f5f7d7a5fb14c9ebf44b8ae443a4665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.0801074504852295\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.1135045289993286\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0021320906552402 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.7768341302871704\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.33637309074401855\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7227382118051703 || Accuracy: 0.6953937411308289 || F1-score: 0.552636199297143\n",
      "Early stopping at epoch 182!\n",
      "Accuracy on dataset of size 672: 68.60118865966797 %.\n",
      "Average loss: 0.7287239161404696\n",
      "proportion of labels in prediction: [tensor(0.7426), tensor(0.1726), tensor(0.0848)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.80392157 0.45864662 0.3875    ]\n",
      "- f1 (average): 0.5500227283896014\n",
      "- accuracy: 0.6860119104385376\n",
      "\n",
      "********** lstm_hidden_dim: [12, 12, 8] || ffnhidden_dim: [100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0bac80c05cc463293d3f21030b32a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1512285470962524\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0863783359527588\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.061882495880127 || Accuracy: 0.42050519585609436 || F1-score: 0.29243233165539567\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.7626064419746399\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.24707303941249847\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7200096087022261 || Accuracy: 0.6939078569412231 || F1-score: 0.5443017935602079\n",
      "Early stopping at epoch 182!\n",
      "Accuracy on dataset of size 672: 68.1547622680664 %.\n",
      "Average loss: 0.734047683802518\n",
      "proportion of labels in prediction: [tensor(0.7470), tensor(0.1711), tensor(0.0818)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.80564604 0.44528302 0.35443038]\n",
      "- f1 (average): 0.5351198118437184\n",
      "- accuracy: 0.6815476417541504\n",
      "\n",
      "********** lstm_hidden_dim: [12, 12, 8] || ffnhidden_dim: [100, 100, 100, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/bnn1j6bs3bqfskq2jlq55yh80000gr/T/ipykernel_90651/2493663396.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train, valid, test = split_dataset(x_data=torch.tensor(x_data).float(),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8091c770bf7d42d689483a4a064b2826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10000 || Item: 0/85 || Loss: 1.1070767641067505\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10000 || Loss: 1.0723456144332886\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 1 || Loss: 1.0553485263477673 || Accuracy: 0.6225854158401489 || F1-score: 0.25579975579975583\n",
      "Epoch: 101/10000 || Item: 0/85 || Loss: 0.9206050634384155\n",
      "--------------------------------------------------\n",
      "##### Epoch: 101/10000 || Loss: 0.18701200187206268\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 101 || Loss: 0.7487882592461326 || Accuracy: 0.6775631308555603 || F1-score: 0.4986432148426074\n",
      "Epoch: 201/10000 || Item: 0/85 || Loss: 0.6725772619247437\n",
      "--------------------------------------------------\n",
      "##### Epoch: 201/10000 || Loss: 0.6710101366043091\n",
      "--------------------------------------------------\n",
      "Validation || Epoch: 201 || Loss: 0.7234085418961265 || Accuracy: 0.689450204372406 || F1-score: 0.5484433300950712\n",
      "Early stopping at epoch 228!\n",
      "Accuracy on dataset of size 672: 69.49404907226562 %.\n",
      "Average loss: 0.8170809068463065\n",
      "proportion of labels in prediction: [tensor(0.7500), tensor(0.1562), tensor(0.0938)]\n",
      "proportion of labels in data: [tensor(0.6235), tensor(0.2232), tensor(0.1533)]\n",
      "- f1: [0.81040087 0.44705882 0.43373494]\n",
      "- f1 (average): 0.5637315433424476\n",
      "- accuracy: 0.694940447807312\n"
     ]
    }
   ],
   "source": [
    "x_data, input_channels = obtain_SDSN_input(pooled_cls_pretrained, path_specifics)\n",
    "for lstm_hidden_dim in lstm_hidden_dim_trial:\n",
    "    for ffn_hidden_dim in ffn_hidden_dim_trial:\n",
    "        print(f\"\\n********** lstm_hidden_dim: {lstm_hidden_dim} \"\n",
    "              f\"|| ffnhidden_dim: {ffn_hidden_dim}\")\n",
    "        implement_sdsn(x_data=x_data,\n",
    "                       y_data=y_data,\n",
    "                       sig_depth=sig_depth,\n",
    "                       input_channels=input_channels,\n",
    "                       output_channels=output_channels,\n",
    "                       lstm_hidden_dim=lstm_hidden_dim,\n",
    "                       ffn_hidden_dim=ffn_hidden_dim,\n",
    "                       BiLSTM=BiLSTM,\n",
    "                       learning_rate=learning_rate,\n",
    "                       loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc2d18b",
   "metadata": {},
   "source": [
    "## Fine-tuned BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f51eace",
   "metadata": {},
   "source": [
    "### Mean pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a0cead",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, input_channels = obtain_SDSN_input(pooled_mean, path_specifics)\n",
    "for lstm_hidden_dim in lstm_hidden_dim_trial:\n",
    "    for ffn_hidden_dim in ffn_hidden_dim_trial:\n",
    "        print(f\"\\n********** lstm_hidden_dim: {lstm_hidden_dim} \"\n",
    "              f\"|| ffnhidden_dim: {ffn_hidden_dim}\")\n",
    "        implement_sdsn(x_data=x_data,\n",
    "                       y_data=y_data,\n",
    "                       sig_depth=sig_depth,\n",
    "                       input_channels=input_channels,\n",
    "                       output_channels=output_channels,\n",
    "                       lstm_hidden_dim=lstm_hidden_dim,\n",
    "                       ffn_hidden_dim=ffn_hidden_dim,\n",
    "                       BiLSTM=BiLSTM,\n",
    "                       learning_rate=learning_rate,\n",
    "                       loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84f553a",
   "metadata": {},
   "source": [
    "### Max pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f2cd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, input_channels = obtain_SDSN_input(pooled_max, path_specifics)\n",
    "for lstm_hidden_dim in lstm_hidden_dim_trial:\n",
    "    for ffn_hidden_dim in ffn_hidden_dim_trial:\n",
    "        print(f\"\\n********** lstm_hidden_dim: {lstm_hidden_dim} \"\n",
    "              f\"|| ffnhidden_dim: {ffn_hidden_dim}\")\n",
    "        implement_sdsn(x_data=x_data,\n",
    "                       y_data=y_data,\n",
    "                       sig_depth=sig_depth,\n",
    "                       input_channels=input_channels,\n",
    "                       output_channels=output_channels,\n",
    "                       lstm_hidden_dim=lstm_hidden_dim,\n",
    "                       ffn_hidden_dim=ffn_hidden_dim,\n",
    "                       BiLSTM=BiLSTM,\n",
    "                       learning_rate=learning_rate,\n",
    "                       loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1892c538",
   "metadata": {},
   "source": [
    "### Sum pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2caf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, input_channels = obtain_SDSN_input(pooled_sum, path_specifics)\n",
    "for lstm_hidden_dim in lstm_hidden_dim_trial:\n",
    "    for ffn_hidden_dim in ffn_hidden_dim_trial:\n",
    "        print(f\"\\n********** lstm_hidden_dim: {lstm_hidden_dim} \"\n",
    "              f\"|| ffnhidden_dim: {ffn_hidden_dim}\")\n",
    "        implement_sdsn(x_data=x_data,\n",
    "                       y_data=y_data,\n",
    "                       sig_depth=sig_depth,\n",
    "                       input_channels=input_channels,\n",
    "                       output_channels=output_channels,\n",
    "                       lstm_hidden_dim=lstm_hidden_dim,\n",
    "                       ffn_hidden_dim=ffn_hidden_dim,\n",
    "                       BiLSTM=BiLSTM,\n",
    "                       learning_rate=learning_rate,\n",
    "                       loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3cfc31",
   "metadata": {},
   "source": [
    "### CLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57d21d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, input_channels = obtain_SDSN_input(pooled_cls, path_specifics)\n",
    "for lstm_hidden_dim in lstm_hidden_dim_trial:\n",
    "    for ffn_hidden_dim in ffn_hidden_dim_trial:\n",
    "        print(f\"\\n********** lstm_hidden_dim: {lstm_hidden_dim} \"\n",
    "              f\"|| ffnhidden_dim: {ffn_hidden_dim}\")\n",
    "        implement_sdsn(x_data=x_data,\n",
    "                       y_data=y_data,\n",
    "                       sig_depth=sig_depth,\n",
    "                       input_channels=input_channels,\n",
    "                       output_channels=output_channels,\n",
    "                       lstm_hidden_dim=lstm_hidden_dim,\n",
    "                       ffn_hidden_dim=ffn_hidden_dim,\n",
    "                       BiLSTM=BiLSTM,\n",
    "                       learning_rate=learning_rate,\n",
    "                       loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63ffede",
   "metadata": {},
   "source": [
    "Baselines:\n",
    "   - just looking at the sentence embeddings (encodes nothing about the history on the post)\n",
    "       - highlights importance of looking at the sequence\n",
    "   - averaging history\n",
    "   - comparing the cosine similarity between previous post and current post to see if switch\n",
    "   \n",
    "Test for:\n",
    "- How many posts do you need to look back?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpsig-networks",
   "language": "python",
   "name": "nlpsig-networks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
