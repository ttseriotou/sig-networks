{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a983a5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "seed = 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4e666cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlpsig_networks.scripts.swmhau_network_functions import (\n",
    "    swmhau_network_hyperparameter_search\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e918dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"client_talk_type_output\"\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7409a03",
   "metadata": {},
   "source": [
    "## AnnoMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f00bb922",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../load_anno_mi.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "720d820f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mi_quality</th>\n",
       "      <th>transcript_id</th>\n",
       "      <th>topic</th>\n",
       "      <th>utterance_id</th>\n",
       "      <th>interlocutor</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>utterance_text</th>\n",
       "      <th>annotator_id</th>\n",
       "      <th>therapist_input_exists</th>\n",
       "      <th>therapist_input_subtype</th>\n",
       "      <th>reflection_exists</th>\n",
       "      <th>reflection_subtype</th>\n",
       "      <th>question_exists</th>\n",
       "      <th>question_subtype</th>\n",
       "      <th>main_therapist_behaviour</th>\n",
       "      <th>client_talk_type</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>high</td>\n",
       "      <td>0</td>\n",
       "      <td>reducing alcohol consumption</td>\n",
       "      <td>0</td>\n",
       "      <td>therapist</td>\n",
       "      <td>00:00:13</td>\n",
       "      <td>Thanks for filling it out. We give this form t...</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>open</td>\n",
       "      <td>question</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-08-16 00:00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>high</td>\n",
       "      <td>0</td>\n",
       "      <td>reducing alcohol consumption</td>\n",
       "      <td>1</td>\n",
       "      <td>client</td>\n",
       "      <td>00:00:24</td>\n",
       "      <td>Sure.</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2023-08-16 00:00:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>high</td>\n",
       "      <td>0</td>\n",
       "      <td>reducing alcohol consumption</td>\n",
       "      <td>2</td>\n",
       "      <td>therapist</td>\n",
       "      <td>00:00:25</td>\n",
       "      <td>So, let's see. It looks that you put-- You dri...</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>information</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>therapist_input</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-08-16 00:00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>high</td>\n",
       "      <td>0</td>\n",
       "      <td>reducing alcohol consumption</td>\n",
       "      <td>3</td>\n",
       "      <td>client</td>\n",
       "      <td>00:00:34</td>\n",
       "      <td>Mm-hmm.</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2023-08-16 00:00:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>high</td>\n",
       "      <td>0</td>\n",
       "      <td>reducing alcohol consumption</td>\n",
       "      <td>4</td>\n",
       "      <td>therapist</td>\n",
       "      <td>00:00:34</td>\n",
       "      <td>-and you usually have three to four drinks whe...</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>information</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>therapist_input</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-08-16 00:00:34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mi_quality  transcript_id                         topic  utterance_id  \\\n",
       "0       high              0  reducing alcohol consumption             0   \n",
       "1       high              0  reducing alcohol consumption             1   \n",
       "2       high              0  reducing alcohol consumption             2   \n",
       "3       high              0  reducing alcohol consumption             3   \n",
       "4       high              0  reducing alcohol consumption             4   \n",
       "\n",
       "  interlocutor timestamp                                     utterance_text  \\\n",
       "0    therapist  00:00:13  Thanks for filling it out. We give this form t...   \n",
       "1       client  00:00:24                                              Sure.   \n",
       "2    therapist  00:00:25  So, let's see. It looks that you put-- You dri...   \n",
       "3       client  00:00:34                                            Mm-hmm.   \n",
       "4    therapist  00:00:34  -and you usually have three to four drinks whe...   \n",
       "\n",
       "   annotator_id therapist_input_exists therapist_input_subtype  \\\n",
       "0             3                  False                     NaN   \n",
       "1             3                    NaN                     NaN   \n",
       "2             3                   True             information   \n",
       "3             3                    NaN                     NaN   \n",
       "4             3                   True             information   \n",
       "\n",
       "  reflection_exists reflection_subtype question_exists question_subtype  \\\n",
       "0             False                NaN            True             open   \n",
       "1               NaN                NaN             NaN              NaN   \n",
       "2             False                NaN           False              NaN   \n",
       "3               NaN                NaN             NaN              NaN   \n",
       "4             False                NaN           False              NaN   \n",
       "\n",
       "  main_therapist_behaviour client_talk_type            datetime  \n",
       "0                 question              NaN 2023-08-16 00:00:13  \n",
       "1                      NaN          neutral 2023-08-16 00:00:24  \n",
       "2          therapist_input              NaN 2023-08-16 00:00:25  \n",
       "3                      NaN          neutral 2023-08-16 00:00:34  \n",
       "4          therapist_input              NaN 2023-08-16 00:00:34  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anno_mi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84d5594d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13551, 384)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../anno_mi_sbert.pkl\", \"rb\") as f:\n",
    "    sbert_embeddings = pickle.load(f)\n",
    "    \n",
    "sbert_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4ff92d",
   "metadata": {},
   "source": [
    "# swmhau Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ae1a72",
   "metadata": {},
   "source": [
    "## Obtaining path by looking at post history\n",
    "\n",
    "We can obtain a path by looking at the history of each post. Here we look at the last 10 posts (and pad with vectors of zeros if there are less than 10 posts) including the current post.\n",
    "\n",
    "We only want to consider paths that correspond to a client's utterance as we want to model a change in mood at that time. Their history will still contain the therapist's utterances too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90bfd87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_features = [\"time_encoding\", \"timeline_index\"]\n",
    "standardise_method = [\"minmax\", None]\n",
    "num_features = len(time_features)\n",
    "add_time_in_path = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e9bee3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "embedding_dim = 384\n",
    "dimensions = [15] # [50, 15]\n",
    "# define swmhau parameters: (output_channels, sig_depth, num_heads)\n",
    "swmhau_parameters = [(12, 3, 10)]\n",
    "num_layers = [1, 2, 3]\n",
    "ffn_hidden_dim_sizes = [[64,64],[256,256],[512,512]]\n",
    "dropout_rates = [0.5, 0.1]\n",
    "learning_rates = [1e-3, 1e-4, 5e-4]\n",
    "seeds = [1, 12, 123]\n",
    "loss = \"focal\"\n",
    "gamma = 2\n",
    "validation_metric = \"f1\"\n",
    "patience = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3739e09a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# size = 20\n",
    "# swmhau_network_umap_kfold_20, best_swmhau_network_umap_kfold_20, _, __ = swmhau_network_hyperparameter_search(\n",
    "#     num_epochs=num_epochs,\n",
    "#     df=anno_mi,\n",
    "#     id_column=\"transcript_id\",\n",
    "#     label_column=\"client_talk_type\",\n",
    "#     embeddings=sbert_embeddings,\n",
    "#     y_data=y_data_client,\n",
    "#     embedding_dim=embedding_dim,\n",
    "#     output_dim=output_dim_client,\n",
    "#     history_lengths=[size],\n",
    "#     dim_reduce_methods=[\"umap\"],\n",
    "#     dimensions=dimensions,\n",
    "#     log_signature=True,\n",
    "#     swmhau_parameters=swmhau_parameters,\n",
    "#     num_layers=num_layers,\n",
    "#     ffn_hidden_dim_sizes=ffn_hidden_dim_sizes,\n",
    "#     dropout_rates=dropout_rates,\n",
    "#     learning_rates=learning_rates,\n",
    "#     seeds=seeds,\n",
    "#     loss=loss,\n",
    "#     gamma=gamma,\n",
    "#     features=time_features,\n",
    "#     standardise_method=standardise_method,\n",
    "#     add_time_in_path=add_time_in_path,\n",
    "#     path_indices=client_index,\n",
    "#     k_fold=True,\n",
    "#     patience=patience,\n",
    "#     validation_metric=validation_metric,\n",
    "#     results_output=f\"{output_dir}/swmhau_network_umap_focal_{gamma}_{size}_kfold.csv\",\n",
    "#     verbose=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13a67a66",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2da49d54f5948cca83c435aeb6909ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fd3d7dd9ca54ab8a71e0498fdd9ed4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "057020f8c78f4bc0b383d44e30f18164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##################################################\n",
      "dimension: 15 | method: gaussian_random_projection\n",
      "[INFO] Concatenating the embeddings to the dataframe...\n",
      "[INFO] - columns beginning with 'e' denote the full embddings.\n",
      "[INFO] - columns beginning with 'd' denote the dimension reduced embeddings.\n",
      "[INFO] Adding time feature columns into dataframe in `.df`.\n",
      "[INFO] Adding 'time_encoding' feature...\n",
      "[INFO] Adding 'time_diff' feature...\n",
      "[INFO] Adding 'timeline_index' feature...\n",
      "[INFO] Padding ids and storing in `.df_padded` and `.array_padded` attributes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e211eb0e9e07477db232b13ab2036d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13551 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] The path was created for each item in the dataframe, by looking at its history, so to include embeddings in the FFN input, we concatenate the embeddings for each sentence / text.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e53d105b76345cabb6c27f248fdc874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "643358908f674da78b82be02f9049d6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7286b17833ca4552ad732726052d0459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5c94e84e29c47fe97389f05cafb19e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef6ac037313c474db44d7f6782c4fdce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m size \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m\n\u001b[0;32m----> 2\u001b[0m swmhau_network_grp_kfold_20, best_swmhau_network_grp_kfold_20, _, __ \u001b[39m=\u001b[39m swmhau_network_hyperparameter_search(\n\u001b[1;32m      3\u001b[0m     num_epochs\u001b[39m=\u001b[39;49mnum_epochs,\n\u001b[1;32m      4\u001b[0m     df\u001b[39m=\u001b[39;49manno_mi,\n\u001b[1;32m      5\u001b[0m     id_column\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtranscript_id\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m     label_column\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mclient_talk_type\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m     embeddings\u001b[39m=\u001b[39;49msbert_embeddings,\n\u001b[1;32m      8\u001b[0m     y_data\u001b[39m=\u001b[39;49my_data_client,\n\u001b[1;32m      9\u001b[0m     embedding_dim\u001b[39m=\u001b[39;49membedding_dim,\n\u001b[1;32m     10\u001b[0m     output_dim\u001b[39m=\u001b[39;49moutput_dim_client,\n\u001b[1;32m     11\u001b[0m     history_lengths\u001b[39m=\u001b[39;49m[size],\n\u001b[1;32m     12\u001b[0m     dim_reduce_methods\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mgaussian_random_projection\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     13\u001b[0m     dimensions\u001b[39m=\u001b[39;49mdimensions,\n\u001b[1;32m     14\u001b[0m     log_signature\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     15\u001b[0m     swmhau_parameters\u001b[39m=\u001b[39;49mswmhau_parameters,\n\u001b[1;32m     16\u001b[0m     num_layers\u001b[39m=\u001b[39;49mnum_layers,\n\u001b[1;32m     17\u001b[0m     ffn_hidden_dim_sizes\u001b[39m=\u001b[39;49mffn_hidden_dim_sizes,\n\u001b[1;32m     18\u001b[0m     dropout_rates\u001b[39m=\u001b[39;49mdropout_rates,\n\u001b[1;32m     19\u001b[0m     learning_rates\u001b[39m=\u001b[39;49mlearning_rates,\n\u001b[1;32m     20\u001b[0m     seeds\u001b[39m=\u001b[39;49mseeds,\n\u001b[1;32m     21\u001b[0m     loss\u001b[39m=\u001b[39;49mloss,\n\u001b[1;32m     22\u001b[0m     gamma\u001b[39m=\u001b[39;49mgamma,\n\u001b[1;32m     23\u001b[0m     features\u001b[39m=\u001b[39;49mtime_features,\n\u001b[1;32m     24\u001b[0m     standardise_method\u001b[39m=\u001b[39;49mstandardise_method,\n\u001b[1;32m     25\u001b[0m     path_indices\u001b[39m=\u001b[39;49mclient_index,\n\u001b[1;32m     26\u001b[0m     k_fold\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     27\u001b[0m     patience\u001b[39m=\u001b[39;49mpatience,\n\u001b[1;32m     28\u001b[0m     validation_metric\u001b[39m=\u001b[39;49mvalidation_metric,\n\u001b[1;32m     29\u001b[0m     results_output\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00moutput_dir\u001b[39m}\u001b[39;49;00m\u001b[39m/swmhau_network_grp_focal_\u001b[39;49m\u001b[39m{\u001b[39;49;00mgamma\u001b[39m}\u001b[39;49;00m\u001b[39m_\u001b[39;49m\u001b[39m{\u001b[39;49;00msize\u001b[39m}\u001b[39;49;00m\u001b[39m_kfold.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     30\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m )\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-TheAlanTuringInstitute/rough_paths/nlpsig-networks/nlpsig_networks/scripts/swmhau_network_functions.py:202\u001b[0m, in \u001b[0;36mswmhau_network_hyperparameter_search\u001b[0;34m(num_epochs, df, id_column, label_column, embeddings, y_data, embedding_dim, output_dim, history_lengths, dim_reduce_methods, dimensions, log_signature, swmhau_parameters, num_layers, ffn_hidden_dim_sizes, dropout_rates, learning_rates, seeds, loss, gamma, batch_size, features, standardise_method, add_time_in_path, augmentation_type, hidden_dim_aug, comb_method, path_indices, data_split_seed, split_ids, split_indices, k_fold, n_splits, patience, validation_metric, results_output, verbose)\u001b[0m\n\u001b[1;32m    200\u001b[0m verbose_model \u001b[39m=\u001b[39m verbose\n\u001b[1;32m    201\u001b[0m \u001b[39mfor\u001b[39;00m seed \u001b[39min\u001b[39;00m seeds:\n\u001b[0;32m--> 202\u001b[0m     _, results \u001b[39m=\u001b[39m implement_swmhau_network(\n\u001b[1;32m    203\u001b[0m         num_epochs\u001b[39m=\u001b[39;49mnum_epochs,\n\u001b[1;32m    204\u001b[0m         x_data\u001b[39m=\u001b[39;49mx_data,\n\u001b[1;32m    205\u001b[0m         y_data\u001b[39m=\u001b[39;49my_data,\n\u001b[1;32m    206\u001b[0m         input_channels\u001b[39m=\u001b[39;49minput_channels,\n\u001b[1;32m    207\u001b[0m         output_channels\u001b[39m=\u001b[39;49moutput_channels,\n\u001b[1;32m    208\u001b[0m         num_features\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(features),\n\u001b[1;32m    209\u001b[0m         embedding_dim\u001b[39m=\u001b[39;49membedding_dim,\n\u001b[1;32m    210\u001b[0m         log_signature\u001b[39m=\u001b[39;49mlog_signature,\n\u001b[1;32m    211\u001b[0m         sig_depth\u001b[39m=\u001b[39;49msig_depth,\n\u001b[1;32m    212\u001b[0m         num_heads\u001b[39m=\u001b[39;49mnum_heads,\n\u001b[1;32m    213\u001b[0m         num_layers\u001b[39m=\u001b[39;49mn_layers,\n\u001b[1;32m    214\u001b[0m         ffn_hidden_dim\u001b[39m=\u001b[39;49mffn_hidden_dim,\n\u001b[1;32m    215\u001b[0m         output_dim\u001b[39m=\u001b[39;49moutput_dim,\n\u001b[1;32m    216\u001b[0m         dropout_rate\u001b[39m=\u001b[39;49mdropout,\n\u001b[1;32m    217\u001b[0m         learning_rate\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    218\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m    219\u001b[0m         loss\u001b[39m=\u001b[39;49mloss,\n\u001b[1;32m    220\u001b[0m         gamma\u001b[39m=\u001b[39;49mgamma,\n\u001b[1;32m    221\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    222\u001b[0m         augmentation_type\u001b[39m=\u001b[39;49maugmentation_type,\n\u001b[1;32m    223\u001b[0m         hidden_dim_aug\u001b[39m=\u001b[39;49mhidden_dim_aug,\n\u001b[1;32m    224\u001b[0m         comb_method\u001b[39m=\u001b[39;49mcomb_method,\n\u001b[1;32m    225\u001b[0m         data_split_seed\u001b[39m=\u001b[39;49mdata_split_seed,\n\u001b[1;32m    226\u001b[0m         split_ids\u001b[39m=\u001b[39;49msplit_ids,\n\u001b[1;32m    227\u001b[0m         split_indices\u001b[39m=\u001b[39;49msplit_indices,\n\u001b[1;32m    228\u001b[0m         k_fold\u001b[39m=\u001b[39;49mk_fold,\n\u001b[1;32m    229\u001b[0m         n_splits\u001b[39m=\u001b[39;49mn_splits,\n\u001b[1;32m    230\u001b[0m         patience\u001b[39m=\u001b[39;49mpatience,\n\u001b[1;32m    231\u001b[0m         verbose_training\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    232\u001b[0m         verbose_results\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    233\u001b[0m         verbose_model\u001b[39m=\u001b[39;49mverbose_model\n\u001b[1;32m    234\u001b[0m     )\n\u001b[1;32m    235\u001b[0m     \u001b[39m# save metric that we want to validate on\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[39m# taking the mean over the performance on the folds for the seed\u001b[39;00m\n\u001b[1;32m    237\u001b[0m     \u001b[39m# if k_fold=False, .mean() just returns the performance for the seed\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     scores\u001b[39m.\u001b[39mappend(results[\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvalid_\u001b[39m\u001b[39m{\u001b[39;00mvalidation_metric\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mmean())\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-TheAlanTuringInstitute/rough_paths/nlpsig-networks/nlpsig_networks/scripts/swmhau_network_functions.py:81\u001b[0m, in \u001b[0;36mimplement_swmhau_network\u001b[0;34m(num_epochs, x_data, y_data, input_channels, num_features, embedding_dim, log_signature, sig_depth, num_heads, num_layers, ffn_hidden_dim, output_dim, dropout_rate, learning_rate, seed, loss, gamma, batch_size, output_channels, augmentation_type, hidden_dim_aug, comb_method, data_split_seed, split_ids, split_indices, k_fold, n_splits, patience, verbose_training, verbose_results, verbose_model)\u001b[0m\n\u001b[1;32m     78\u001b[0m     y_data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(y_data)\n\u001b[1;32m     79\u001b[0m x_data \u001b[39m=\u001b[39m x_data\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m---> 81\u001b[0m \u001b[39mreturn\u001b[39;00m implement_model(model\u001b[39m=\u001b[39;49mswmhau_network_model,\n\u001b[1;32m     82\u001b[0m                        num_epochs\u001b[39m=\u001b[39;49mnum_epochs,\n\u001b[1;32m     83\u001b[0m                        x_data\u001b[39m=\u001b[39;49mx_data,\n\u001b[1;32m     84\u001b[0m                        y_data\u001b[39m=\u001b[39;49my_data,\n\u001b[1;32m     85\u001b[0m                        learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[1;32m     86\u001b[0m                        seed\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m     87\u001b[0m                        loss\u001b[39m=\u001b[39;49mloss,\n\u001b[1;32m     88\u001b[0m                        gamma\u001b[39m=\u001b[39;49mgamma,\n\u001b[1;32m     89\u001b[0m                        batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m     90\u001b[0m                        data_split_seed\u001b[39m=\u001b[39;49mdata_split_seed,\n\u001b[1;32m     91\u001b[0m                        split_ids\u001b[39m=\u001b[39;49msplit_ids,\n\u001b[1;32m     92\u001b[0m                        split_indices\u001b[39m=\u001b[39;49msplit_indices,\n\u001b[1;32m     93\u001b[0m                        k_fold\u001b[39m=\u001b[39;49mk_fold,\n\u001b[1;32m     94\u001b[0m                        n_splits\u001b[39m=\u001b[39;49mn_splits,\n\u001b[1;32m     95\u001b[0m                        patience\u001b[39m=\u001b[39;49mpatience,\n\u001b[1;32m     96\u001b[0m                        verbose_training\u001b[39m=\u001b[39;49mverbose_training,\n\u001b[1;32m     97\u001b[0m                        verbose_results\u001b[39m=\u001b[39;49mverbose_results)\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-TheAlanTuringInstitute/rough_paths/nlpsig-networks/nlpsig_networks/scripts/implement_model.py:67\u001b[0m, in \u001b[0;36mimplement_model\u001b[0;34m(model, num_epochs, x_data, y_data, learning_rate, seed, loss, gamma, batch_size, data_split_seed, split_ids, split_indices, k_fold, n_splits, patience, verbose_training, verbose_results)\u001b[0m\n\u001b[1;32m     60\u001b[0m     optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(),\n\u001b[1;32m     61\u001b[0m                                  lr\u001b[39m=\u001b[39mlearning_rate,\n\u001b[1;32m     62\u001b[0m                                  weight_decay\u001b[39m=\u001b[39mweight_decay_adam)\n\u001b[1;32m     64\u001b[0m     \u001b[39m# perform k-fold evaluation which returns a dataframe with columns for the\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[39m# loss, accuracy, f1 (macro) and individual f1-scores for each fold\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[39m# (for both validation and test set)\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m     results \u001b[39m=\u001b[39m KFold_pytorch(folds\u001b[39m=\u001b[39;49mfolds,\n\u001b[1;32m     68\u001b[0m                             model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m     69\u001b[0m                             criterion\u001b[39m=\u001b[39;49mcriterion,\n\u001b[1;32m     70\u001b[0m                             optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m     71\u001b[0m                             num_epochs\u001b[39m=\u001b[39;49mnum_epochs,\n\u001b[1;32m     72\u001b[0m                             batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m     73\u001b[0m                             seed\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m     74\u001b[0m                             return_best\u001b[39m=\u001b[39;49mreturn_best,\n\u001b[1;32m     75\u001b[0m                             early_stopping\u001b[39m=\u001b[39;49mearly_stopping,\n\u001b[1;32m     76\u001b[0m                             patience\u001b[39m=\u001b[39;49mpatience,\n\u001b[1;32m     77\u001b[0m                             verbose\u001b[39m=\u001b[39;49mverbose_training)\n\u001b[1;32m     78\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m     \u001b[39m# split dataset\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     data_loader_args \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m\"\u001b[39m: batch_size, \u001b[39m\"\u001b[39m\u001b[39mshuffle\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mTrue\u001b[39;00m}\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-TheAlanTuringInstitute/rough_paths/nlpsig-networks/nlpsig_networks/pytorch_utils.py:708\u001b[0m, in \u001b[0;36mKFold_pytorch\u001b[0;34m(folds, model, criterion, optimizer, num_epochs, batch_size, return_metric_for_each_fold, seed, return_best, early_stopping, patience, verbose, verbose_epoch)\u001b[0m\n\u001b[1;32m    705\u001b[0m train, valid, test \u001b[39m=\u001b[39m folds\u001b[39m.\u001b[39mget_splits(fold_index\u001b[39m=\u001b[39mfold, as_DataLoader\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, data_loader_args\u001b[39m=\u001b[39mdata_loader_args)\n\u001b[1;32m    707\u001b[0m \u001b[39m# train pytorch model\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m model \u001b[39m=\u001b[39m training_pytorch(\n\u001b[1;32m    709\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    710\u001b[0m     train_loader\u001b[39m=\u001b[39;49mtrain,\n\u001b[1;32m    711\u001b[0m     valid_loader\u001b[39m=\u001b[39;49mvalid,\n\u001b[1;32m    712\u001b[0m     criterion\u001b[39m=\u001b[39;49mcriterion,\n\u001b[1;32m    713\u001b[0m     optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m    714\u001b[0m     num_epochs\u001b[39m=\u001b[39;49mnum_epochs,\n\u001b[1;32m    715\u001b[0m     seed\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m    716\u001b[0m     return_best\u001b[39m=\u001b[39;49mreturn_best,\n\u001b[1;32m    717\u001b[0m     save_best\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    718\u001b[0m     early_stopping\u001b[39m=\u001b[39;49mearly_stopping,\n\u001b[1;32m    719\u001b[0m     patience\u001b[39m=\u001b[39;49mpatience,\n\u001b[1;32m    720\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    721\u001b[0m     verbose_epoch\u001b[39m=\u001b[39;49mverbose_epoch,\n\u001b[1;32m    722\u001b[0m )\n\u001b[1;32m    724\u001b[0m \u001b[39m# test model\u001b[39;00m\n\u001b[1;32m    725\u001b[0m test_results \u001b[39m=\u001b[39m testing_pytorch(model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m    726\u001b[0m                                test_loader\u001b[39m=\u001b[39mtest,\n\u001b[1;32m    727\u001b[0m                                criterion\u001b[39m=\u001b[39mcriterion,\n\u001b[1;32m    728\u001b[0m                                verbose\u001b[39m=\u001b[39mverbose)\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-TheAlanTuringInstitute/rough_paths/nlpsig-networks/nlpsig_networks/pytorch_utils.py:422\u001b[0m, in \u001b[0;36mtraining_pytorch\u001b[0;34m(model, train_loader, criterion, optimizer, num_epochs, scheduler, valid_loader, seed, return_best, save_best, output, early_stopping, validation_metric, patience, verbose, verbose_epoch)\u001b[0m\n\u001b[1;32m    420\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m    421\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m--> 422\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    424\u001b[0m \u001b[39m# show training progress\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[39mif\u001b[39;00m verbose:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/nlpsig-networks/lib/python3.8/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/nlpsig-networks/lib/python3.8/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/nlpsig-networks/lib/python3.8/site-packages/torch/optim/adam.py:107\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[39m# record the step after step update\u001b[39;00m\n\u001b[1;32m    105\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 107\u001b[0m     F\u001b[39m.\u001b[39;49madam(params_with_grad,\n\u001b[1;32m    108\u001b[0m            grads,\n\u001b[1;32m    109\u001b[0m            exp_avgs,\n\u001b[1;32m    110\u001b[0m            exp_avg_sqs,\n\u001b[1;32m    111\u001b[0m            max_exp_avg_sqs,\n\u001b[1;32m    112\u001b[0m            state_steps,\n\u001b[1;32m    113\u001b[0m            amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    114\u001b[0m            beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    115\u001b[0m            beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    116\u001b[0m            lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    117\u001b[0m            weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    118\u001b[0m            eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    119\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/nlpsig-networks/lib/python3.8/site-packages/torch/optim/_functional.py:94\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     92\u001b[0m     denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39msqrt(bias_correction2))\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m     93\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39;49m math\u001b[39m.\u001b[39;49msqrt(bias_correction2))\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m     96\u001b[0m step_size \u001b[39m=\u001b[39m lr \u001b[39m/\u001b[39m bias_correction1\n\u001b[1;32m     98\u001b[0m param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "size = 20\n",
    "swmhau_network_grp_kfold_20, best_swmhau_network_grp_kfold_20, _, __ = swmhau_network_hyperparameter_search(\n",
    "    num_epochs=num_epochs,\n",
    "    df=anno_mi,\n",
    "    id_column=\"transcript_id\",\n",
    "    label_column=\"client_talk_type\",\n",
    "    embeddings=sbert_embeddings,\n",
    "    y_data=y_data_client,\n",
    "    embedding_dim=embedding_dim,\n",
    "    output_dim=output_dim_client,\n",
    "    history_lengths=[size],\n",
    "    dim_reduce_methods=[\"gaussian_random_projection\"],\n",
    "    dimensions=dimensions,\n",
    "    log_signature=True,\n",
    "    swmhau_parameters=swmhau_parameters,\n",
    "    num_layers=num_layers,\n",
    "    ffn_hidden_dim_sizes=ffn_hidden_dim_sizes,\n",
    "    dropout_rates=dropout_rates,\n",
    "    learning_rates=learning_rates,\n",
    "    seeds=seeds,\n",
    "    loss=loss,\n",
    "    gamma=gamma,\n",
    "    features=time_features,\n",
    "    standardise_method=standardise_method,\n",
    "    path_indices=client_index,\n",
    "    k_fold=True,\n",
    "    patience=patience,\n",
    "    validation_metric=validation_metric,\n",
    "    results_output=f\"{output_dir}/swmhau_network_grp_focal_{gamma}_{size}_kfold.csv\",\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daab8de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpsig-networks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
