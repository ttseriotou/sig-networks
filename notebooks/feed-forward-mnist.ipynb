{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "852d93ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "\n",
    "import nlpsig_networks\n",
    "from nlpsig_networks.ffn import FeedforwardNeuralNetModel\n",
    "from nlpsig_networks.focal_loss import FocalLoss\n",
    "from nlpsig_networks.pytorch_utils import training_pytorch, testing_pytorch\n",
    "\n",
    "seed = 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "358bdd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rchan/opt/miniconda3/envs/nlpsig-networks/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "# MNIST\n",
    "train_dataset = torchvision.datasets.MNIST(root=\"./data\", train=True,\n",
    "                                           transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                                         transforms.Lambda(lambda x: torch.flatten(x))]),\n",
    "                                           download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root=\"./data\", train=False,\n",
    "                                          transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                                        transforms.Lambda(lambda x: torch.flatten(x))]),\n",
    "                                          download=True)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=batch_size,\n",
    "                                           shuffle=True,)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=batch_size,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8be0e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 784]) torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "examples = iter(train_loader)\n",
    "samples, label = examples.next()\n",
    "print(samples.shape,label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76d615ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "input_size = 784 # 28X28\n",
    "hidden_size = 100\n",
    "num_classes = 10\n",
    "num_epochs = 1\n",
    "learning_rate = 0.001\n",
    "gamma = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fb81c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn_model = FeedforwardNeuralNetModel(input_dim=input_size,\n",
    "                                      hidden_dim=hidden_size,\n",
    "                                      output_dim=num_classes,\n",
    "                                      dropout_rate=0.5)\n",
    "\n",
    "# define loss\n",
    "criterion = FocalLoss(gamma = gamma)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.Adam(ffn_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# define scheduler for adjusting the learning rate\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "# scheduler = StepLR(optimizer, step_size = 4, gamma = 0.5)\n",
    "# scheduler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89ca1856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on dataset of size 10000: 10.770000457763672 %.\n",
      "Average loss: 2.0794160413742064\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.1077)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy before training\n",
    "pred, label = testing_pytorch(ffn_model, test_loader, criterion)\n",
    "sum(pred==label) / len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c3c2fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22c2aea6eec04b3b891a8ee6f6490d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10 || Item: 0/600 || Loss: 2.076205253601074\n",
      "--------------------------------------------------\n",
      "##### Epoch: 1/10 || Loss: 0.4600061774253845\n",
      "--------------------------------------------------\n",
      "Epoch: 2/10 || Item: 0/600 || Loss: 0.32463809847831726\n",
      "--------------------------------------------------\n",
      "##### Epoch: 2/10 || Loss: 0.14252567291259766\n",
      "--------------------------------------------------\n",
      "Epoch: 3/10 || Item: 0/600 || Loss: 0.18404611945152283\n",
      "--------------------------------------------------\n",
      "##### Epoch: 3/10 || Loss: 0.24533876776695251\n",
      "--------------------------------------------------\n",
      "Epoch: 4/10 || Item: 0/600 || Loss: 0.13073308765888214\n",
      "--------------------------------------------------\n",
      "##### Epoch: 4/10 || Loss: 0.19907833635807037\n",
      "--------------------------------------------------\n",
      "Epoch: 5/10 || Item: 0/600 || Loss: 0.20021291077136993\n",
      "--------------------------------------------------\n",
      "##### Epoch: 5/10 || Loss: 0.20535892248153687\n",
      "--------------------------------------------------\n",
      "Epoch: 6/10 || Item: 0/600 || Loss: 0.2722066044807434\n",
      "--------------------------------------------------\n",
      "##### Epoch: 6/10 || Loss: 0.14927081763744354\n",
      "--------------------------------------------------\n",
      "Epoch: 7/10 || Item: 0/600 || Loss: 0.10876638442277908\n",
      "--------------------------------------------------\n",
      "##### Epoch: 7/10 || Loss: 0.1771077662706375\n",
      "--------------------------------------------------\n",
      "Epoch: 8/10 || Item: 0/600 || Loss: 0.09352333843708038\n",
      "--------------------------------------------------\n",
      "##### Epoch: 8/10 || Loss: 0.16944065690040588\n",
      "--------------------------------------------------\n",
      "Epoch: 9/10 || Item: 0/600 || Loss: 0.12405263632535934\n",
      "--------------------------------------------------\n",
      "##### Epoch: 9/10 || Loss: 0.15824304521083832\n",
      "--------------------------------------------------\n",
      "Epoch: 10/10 || Item: 0/600 || Loss: 0.14140437543392181\n",
      "--------------------------------------------------\n",
      "##### Epoch: 10/10 || Loss: 0.14719915390014648\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ffn_model = training_pytorch(model=ffn_model,\n",
    "                             train_loader=train_loader,\n",
    "                             criterion=criterion,\n",
    "                             optimizer=optimizer,\n",
    "                             num_epochs=10,\n",
    "                             scheduler=scheduler,\n",
    "                             seed=seed,\n",
    "                             verbose=True,\n",
    "                             verbose_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "173903de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on dataset of size 10000: 96.66999816894531 %.\n",
      "Average loss: 0.07194365375558846\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.9667)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy after training\n",
    "pred, label = testing_pytorch(ffn_model, test_loader, criterion)\n",
    "sum(pred==label) / len(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpsig-networks",
   "language": "python",
   "name": "nlpsig-networks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
