{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ttseriotou/anaconda3/envs/py38-MoC/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Specifics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# ================================\n",
    "# model specifics\n",
    "model_specifics = {\n",
    "    \"data\": \"Rumoureval\",\n",
    "    \"global_embedding_tp\": \"SBERT\",  # options: SBERT, BERT_cls , BERT_mean, BERT_max\n",
    "    \"dimensionality_reduction_tp\": \"umap\",  # options: ppapca, ppapcappa, umap\n",
    "    \"dimensionality_reduction_components\": 15,  # options: any int number between 1 and embedding dimensions\n",
    "    \"dimensionality_reduction\": True,  # options: True, False\n",
    "    \"time_injection_history_tp\": None,  # options: timestamp, None\n",
    "    \"time_injection_post_tp\": \"timestamp\",  # options: timestamp, None\n",
    "    \"signature_dimensions\": 3,  # options: any int number larger than 1\n",
    "    \"post_embedding_tp\": \"windowsigsentence\",  # options: sentence, reduced, sentencelstm (1), sentencesiglstm (2), windowsigsentence (3)\n",
    "    \"feature_combination_method\": \"concatenation\",  # options: concatenation, gated_addition, gated_concatenation\n",
    "    \"signature_tp\": \"log\",  # options: log, sig\n",
    "    \"augmentation_tp\": \"Conv1d\",  # options: Conv1d, CNN\n",
    "    \"loss_function\": \"focal\",  # options: focal, cbfocal\n",
    "    \"reduced_network_components\": 10,  # any integer greater than 1\n",
    "    \"k_window\": 5,  # window of looking back k-1 posts + current\n",
    "    \"classifier_name\": \"Seq-Sig-Net\",  #'Conv1d3kernel13channelSigLSTMSig-LSTM', # options: FFN2hidden (any future classifiers added)\n",
    "    \"classes_num\": \"2class\",  # options: 3class (5class to be added in the future)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read converstations data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "f_name = \"/storage/ttseriotou/rumour_eval/data/conversations.json\"\n",
    "\n",
    "with open(f_name, \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Conversion for the Longitudinal Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of threads with switches / total number of threads: 155/325\n",
      "# of switches of stance from opposition to support of claim:  103\n",
      "# of switches of stance from support to opposition of claim:  132\n"
     ]
    }
   ],
   "source": [
    "# Convert conversation thread to linear timeline: we use timestamps of each post in the twitter thread to obtain a chronologically ordered list.\n",
    "def tree2timeline(conversation):\n",
    "    timeline = []\n",
    "    timeline.append(\n",
    "        (\n",
    "            conversation[\"source\"][\"id\"],\n",
    "            conversation[\"source\"][\"created_at\"],\n",
    "            conversation[\"source\"][\"stance\"],\n",
    "        )\n",
    "    )\n",
    "    replies = conversation[\"replies\"]\n",
    "    replies_idstr = []\n",
    "    replies_timestamp = []\n",
    "    for reply in replies:\n",
    "        replies_idstr.append((reply[\"id\"], reply[\"created_at\"], reply[\"stance\"]))\n",
    "        replies_timestamp.append(reply[\"created_at\"])\n",
    "\n",
    "    sorted_replies = [x for (y, x) in sorted(zip(replies_timestamp, replies_idstr))]\n",
    "    timeline.extend(sorted_replies)\n",
    "    return timeline\n",
    "\n",
    "\n",
    "stance_timelines = {\"dev\": [], \"train\": [], \"test\": []}\n",
    "switch_timelines = {\"dev\": [], \"train\": [], \"test\": []}\n",
    "check = []\n",
    "count_switch_threads = 0\n",
    "all_support_switches = 0\n",
    "all_oppose_switches = 0\n",
    "count_threads = 0\n",
    "\n",
    "for subset in list(data.keys()):\n",
    "    count_threads += len(data[subset])\n",
    "    for conv in data[subset]:\n",
    "        timeline = tree2timeline(conv)\n",
    "        stance_timelines[subset].append(timeline)\n",
    "        support = 0\n",
    "        deny = 0\n",
    "        old_sum = 0\n",
    "        switch_events = []\n",
    "        for i, s in enumerate(timeline):\n",
    "            if s[2] == \"support\":\n",
    "                support = support + 1\n",
    "            elif s[2] == \"query\" or s[2] == \"deny\":\n",
    "                deny = deny + 1\n",
    "\n",
    "            new_sum = support - deny\n",
    "            check.append(new_sum)\n",
    "\n",
    "            if i != 0 and old_sum == 0 and new_sum != 0:\n",
    "                # A switch in stance from supporting to opposing the claim starts\n",
    "                if new_sum < 0:\n",
    "                    switch_events.append((s[0], s[1], -1))\n",
    "                # A switch in stance from opposing to supporting the claim starts\n",
    "                elif new_sum > 0:\n",
    "                    switch_events.append((s[0], s[1], 1))\n",
    "            elif (\n",
    "                i != 0\n",
    "                and old_sum < 0\n",
    "                and new_sum < 0\n",
    "                and -1 in [x[2] for x in switch_events]\n",
    "            ):\n",
    "                # A switch in stance from supporting to opposing the claim continues\n",
    "                switch_events.append((s[0], s[1], -2))\n",
    "            elif (\n",
    "                i != 0\n",
    "                and old_sum > 0\n",
    "                and new_sum > 0\n",
    "                and 1 in [x[2] for x in switch_events]\n",
    "            ):\n",
    "                # A switch in stance from opposing to supporting the claim continues\n",
    "                switch_events.append((s[0], s[1], 2))\n",
    "\n",
    "            else:\n",
    "                switch_events.append((s[0], s[1], 0))\n",
    "            old_sum = new_sum\n",
    "\n",
    "        support_switch = [x[2] for x in switch_events].count(1)\n",
    "        oppose_switch = [x[2] for x in switch_events].count(-1)\n",
    "\n",
    "        if support_switch + oppose_switch > 0:\n",
    "            count_switch_threads = count_switch_threads + 1\n",
    "            all_support_switches += support_switch\n",
    "            all_oppose_switches += oppose_switch\n",
    "\n",
    "        switch_timelines[subset].append(switch_events)\n",
    "print(\n",
    "    \"Ratio of threads with switches / total number of threads: {}/{}\".format(\n",
    "        count_switch_threads, count_threads\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"# of switches of stance from opposition to support of claim: \",\n",
    "    all_support_switches,\n",
    ")\n",
    "print(\n",
    "    \"# of switches of stance from support to opposition of claim: \", all_oppose_switches\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count all posts in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of train/dev/test posts: 4238 281 1049\n"
     ]
    }
   ],
   "source": [
    "posts_train_n = 0\n",
    "posts_dev_n = 0\n",
    "posts_test_n = 0\n",
    "\n",
    "for i in switch_timelines[\"train\"]:\n",
    "    posts_train_n += len(i)\n",
    "for i in switch_timelines[\"dev\"]:\n",
    "    posts_dev_n += len(i)\n",
    "for i in switch_timelines[\"test\"]:\n",
    "    posts_test_n += len(i)\n",
    "\n",
    "print(\"Total of train/dev/test posts:\", posts_train_n, posts_dev_n, posts_test_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read alredy formed embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# read data\n",
    "sbert_file = \"/storage/ttseriotou/rumour_eval/data/longrumoureval_sbert/sbert.pkl\"\n",
    "with open(sbert_file, \"rb\") as g:\n",
    "    emb_data = pickle.load(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converstion of Labels to binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def simplify_label(y):\n",
    "    # If the label is -2,-1,2 this is is relabeled to 1\n",
    "    if y != 0:\n",
    "        y = 1\n",
    "    return y\n",
    "\n",
    "\n",
    "for subset in [\"train\", \"dev\", \"test\"]:\n",
    "    for i, thread in enumerate(switch_timelines[subset]):\n",
    "        switch_timelines[subset][i] = [\n",
    "            (x, z, simplify_label(y)) for (x, z, y) in thread\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all embeddings and get a df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5568, 386)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subset</th>\n",
       "      <th>e1</th>\n",
       "      <th>e2</th>\n",
       "      <th>e3</th>\n",
       "      <th>e4</th>\n",
       "      <th>e5</th>\n",
       "      <th>e6</th>\n",
       "      <th>e7</th>\n",
       "      <th>e8</th>\n",
       "      <th>...</th>\n",
       "      <th>e375</th>\n",
       "      <th>e376</th>\n",
       "      <th>e377</th>\n",
       "      <th>e378</th>\n",
       "      <th>e379</th>\n",
       "      <th>e380</th>\n",
       "      <th>e381</th>\n",
       "      <th>e382</th>\n",
       "      <th>e383</th>\n",
       "      <th>e384</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.249902e+17</td>\n",
       "      <td>train</td>\n",
       "      <td>0.064823</td>\n",
       "      <td>0.009375</td>\n",
       "      <td>-0.033364</td>\n",
       "      <td>-0.040282</td>\n",
       "      <td>0.083190</td>\n",
       "      <td>0.043792</td>\n",
       "      <td>-0.012862</td>\n",
       "      <td>-0.038167</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021066</td>\n",
       "      <td>0.047782</td>\n",
       "      <td>-0.041925</td>\n",
       "      <td>-0.029498</td>\n",
       "      <td>0.028939</td>\n",
       "      <td>-0.030646</td>\n",
       "      <td>0.005240</td>\n",
       "      <td>-0.011905</td>\n",
       "      <td>0.018394</td>\n",
       "      <td>0.001854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.250038e+17</td>\n",
       "      <td>train</td>\n",
       "      <td>0.018231</td>\n",
       "      <td>0.079517</td>\n",
       "      <td>0.032132</td>\n",
       "      <td>0.026828</td>\n",
       "      <td>0.007890</td>\n",
       "      <td>0.042798</td>\n",
       "      <td>-0.037635</td>\n",
       "      <td>0.013557</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033683</td>\n",
       "      <td>0.015816</td>\n",
       "      <td>-0.077750</td>\n",
       "      <td>-0.000398</td>\n",
       "      <td>0.026768</td>\n",
       "      <td>-0.050967</td>\n",
       "      <td>-0.027182</td>\n",
       "      <td>0.021025</td>\n",
       "      <td>-0.028620</td>\n",
       "      <td>0.008729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.249927e+17</td>\n",
       "      <td>train</td>\n",
       "      <td>0.053822</td>\n",
       "      <td>0.009492</td>\n",
       "      <td>-0.009654</td>\n",
       "      <td>-0.032107</td>\n",
       "      <td>0.098263</td>\n",
       "      <td>0.069386</td>\n",
       "      <td>0.010227</td>\n",
       "      <td>-0.036979</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023394</td>\n",
       "      <td>0.075511</td>\n",
       "      <td>-0.036010</td>\n",
       "      <td>-0.033392</td>\n",
       "      <td>0.034138</td>\n",
       "      <td>-0.032089</td>\n",
       "      <td>0.003894</td>\n",
       "      <td>-0.037643</td>\n",
       "      <td>0.022180</td>\n",
       "      <td>0.011833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.249908e+17</td>\n",
       "      <td>train</td>\n",
       "      <td>-0.031863</td>\n",
       "      <td>0.055065</td>\n",
       "      <td>-0.024028</td>\n",
       "      <td>-0.020351</td>\n",
       "      <td>-0.030556</td>\n",
       "      <td>-0.030321</td>\n",
       "      <td>0.017300</td>\n",
       "      <td>0.002520</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060842</td>\n",
       "      <td>0.043160</td>\n",
       "      <td>-0.102785</td>\n",
       "      <td>-0.049965</td>\n",
       "      <td>0.001174</td>\n",
       "      <td>0.078667</td>\n",
       "      <td>0.044994</td>\n",
       "      <td>0.104832</td>\n",
       "      <td>0.009009</td>\n",
       "      <td>0.093310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.249906e+17</td>\n",
       "      <td>train</td>\n",
       "      <td>-0.029260</td>\n",
       "      <td>-0.053905</td>\n",
       "      <td>-0.039863</td>\n",
       "      <td>0.023316</td>\n",
       "      <td>0.133505</td>\n",
       "      <td>0.001998</td>\n",
       "      <td>0.001952</td>\n",
       "      <td>-0.012355</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091279</td>\n",
       "      <td>0.088230</td>\n",
       "      <td>-0.047634</td>\n",
       "      <td>-0.044625</td>\n",
       "      <td>0.036614</td>\n",
       "      <td>-0.095384</td>\n",
       "      <td>0.040821</td>\n",
       "      <td>-0.102738</td>\n",
       "      <td>-0.034560</td>\n",
       "      <td>0.093606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 386 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id subset        e1        e2        e3        e4        e5  \\\n",
       "0  5.249902e+17  train  0.064823  0.009375 -0.033364 -0.040282  0.083190   \n",
       "1  5.250038e+17  train  0.018231  0.079517  0.032132  0.026828  0.007890   \n",
       "2  5.249927e+17  train  0.053822  0.009492 -0.009654 -0.032107  0.098263   \n",
       "3  5.249908e+17  train -0.031863  0.055065 -0.024028 -0.020351 -0.030556   \n",
       "4  5.249906e+17  train -0.029260 -0.053905 -0.039863  0.023316  0.133505   \n",
       "\n",
       "         e6        e7        e8  ...      e375      e376      e377      e378  \\\n",
       "0  0.043792 -0.012862 -0.038167  ... -0.021066  0.047782 -0.041925 -0.029498   \n",
       "1  0.042798 -0.037635  0.013557  ...  0.033683  0.015816 -0.077750 -0.000398   \n",
       "2  0.069386  0.010227 -0.036979  ... -0.023394  0.075511 -0.036010 -0.033392   \n",
       "3 -0.030321  0.017300  0.002520  ...  0.060842  0.043160 -0.102785 -0.049965   \n",
       "4  0.001998  0.001952 -0.012355  ...  0.091279  0.088230 -0.047634 -0.044625   \n",
       "\n",
       "       e379      e380      e381      e382      e383      e384  \n",
       "0  0.028939 -0.030646  0.005240 -0.011905  0.018394  0.001854  \n",
       "1  0.026768 -0.050967 -0.027182  0.021025 -0.028620  0.008729  \n",
       "2  0.034138 -0.032089  0.003894 -0.037643  0.022180  0.011833  \n",
       "3  0.001174  0.078667  0.044994  0.104832  0.009009  0.093310  \n",
       "4  0.036614 -0.095384  0.040821 -0.102738 -0.034560  0.093606  \n",
       "\n",
       "[5 rows x 386 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def embedding_df(emb_data):\n",
    "    # embeddings in df\n",
    "    row_list = []\n",
    "\n",
    "    for subset in [\"train\", \"dev\", \"test\"]:\n",
    "        for thread in emb_data[subset]:\n",
    "            record = np.concatenate(\n",
    "                (\n",
    "                    np.array(thread[\"source\"][\"id\"]).reshape(\n",
    "                        1,\n",
    "                    ),\n",
    "                    np.array(subset).reshape(\n",
    "                        1,\n",
    "                    ),\n",
    "                    thread[\"source\"][\"emb\"],\n",
    "                )\n",
    "            )\n",
    "            row_list.append(record)\n",
    "\n",
    "            for tweet in thread[\"replies\"]:\n",
    "                record = np.concatenate(\n",
    "                    (\n",
    "                        np.array(tweet[\"id\"]).reshape(\n",
    "                            1,\n",
    "                        ),\n",
    "                        np.array(subset).reshape(\n",
    "                            1,\n",
    "                        ),\n",
    "                        tweet[\"emb\"],\n",
    "                    )\n",
    "                )\n",
    "                row_list.append(record)\n",
    "\n",
    "    df_emb = pd.DataFrame(row_list)\n",
    "    df_emb.columns = [\"id\", \"subset\"] + [\n",
    "        \"e\" + str(i + 1)\n",
    "        for i in range(emb_data[\"train\"][0][\"replies\"][0][\"emb\"].shape[0])\n",
    "    ]\n",
    "    df_emb[\"id\"] = df_emb[\"id\"].astype(\"float\")\n",
    "    df_emb[[c for c in df_emb.columns if re.match(\"^e\\w*[0-9]\", c)]] = df_emb[\n",
    "        [c for c in df_emb.columns if re.match(\"^e\\w*[0-9]\", c)]\n",
    "    ].astype(\"float\")\n",
    "\n",
    "    return df_emb\n",
    "\n",
    "\n",
    "df_emb = embedding_df(emb_data)\n",
    "print(df_emb.shape)\n",
    "df_emb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5568, 15)\n",
      "(5568, 401)\n"
     ]
    }
   ],
   "source": [
    "# dimensionality reduction\n",
    "from dimensionality_reduction import DimensionalityReduction\n",
    "\n",
    "reduction = DimensionalityReduction(\n",
    "    method=model_specifics[\"dimensionality_reduction_tp\"],\n",
    "    components=model_specifics[\"dimensionality_reduction_components\"],\n",
    ")\n",
    "embeddings_reduced = reduction.fit_transform(\n",
    "    np.array(df_emb[[c for c in df_emb.columns if re.match(\"^e\\w*[0-9]\", c)]])\n",
    ")\n",
    "\n",
    "print(embeddings_reduced.shape)\n",
    "\n",
    "# form dataframe\n",
    "df_emb_reduced = pd.concat(\n",
    "    [\n",
    "        df_emb.reset_index(drop=True),\n",
    "        pd.DataFrame(\n",
    "            embeddings_reduced,\n",
    "            columns=[\"d\" + str(i + 1) for i in range(embeddings_reduced.shape[1])],\n",
    "        ),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "print(df_emb_reduced.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Classes and Preparation for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4238, 404)\n",
      "(4238, 5, 18)\n",
      "(281, 404)\n",
      "(281, 5, 18)\n",
      "(1049, 404)\n",
      "(1049, 5, 18)\n"
     ]
    }
   ],
   "source": [
    "# TRAIN/DEV/TEST\n",
    "total_year_hours = 365 * 24\n",
    "\n",
    "\n",
    "def time_fraction(x):\n",
    "    return (\n",
    "        x.year\n",
    "        + abs(x - datetime.datetime(x.year, 1, 1, 0)).total_seconds()\n",
    "        / 3600.0\n",
    "        / total_year_hours\n",
    "    )\n",
    "\n",
    "\n",
    "def get_data_format(df_emb_x, switch_timelines, subset=\"train\", k=3):\n",
    "    ####################################\n",
    "\n",
    "    # format for data\n",
    "    zeros = np.concatenate(\n",
    "        (\n",
    "            np.array([100]),\n",
    "            np.repeat(\n",
    "                0,\n",
    "                df_emb_x[\n",
    "                    [c for c in df_emb_x.columns if not re.match(\"^e\\w*[0-9]\", c)]\n",
    "                ].shape[1],\n",
    "            ),\n",
    "        ),\n",
    "        axis=0,\n",
    "    )\n",
    "\n",
    "    sample_list = []\n",
    "\n",
    "    start_i = 0\n",
    "    end_i = 0\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        [],\n",
    "        columns=[\"id\", \"label\", \"timestamp\"]\n",
    "        + [c for c in df_emb_x.columns if re.match(\"^e\\w*[0-9]\", c)]\n",
    "        + [c for c in df_emb_x.columns if re.match(\"^d\\w*[0-9]\", c)],\n",
    "    )\n",
    "\n",
    "    for e, thread in enumerate(switch_timelines[subset]):\n",
    "        df_thread = pd.DataFrame(thread, columns=[\"id\", \"timestamp\", \"label\"])\n",
    "        df_thread = df_thread.reindex(columns=[\"id\", \"label\", \"timestamp\"])\n",
    "\n",
    "        df_thread[\"timeline_id\"] = str(e)\n",
    "        df_thread[\"id\"] = df_thread[\"id\"].astype(\"float\")\n",
    "        df_thread[\"timestamp\"] = pd.to_datetime(df_thread[\"timestamp\"])\n",
    "        df_thread[\"timestamp\"] = df_thread[\"timestamp\"].map(\n",
    "            lambda t: time_fraction(t.replace(tzinfo=None))\n",
    "        )\n",
    "\n",
    "        df_thread = df_thread.merge(df_emb_x, on=\"id\", how=\"left\")\n",
    "\n",
    "        df = pd.concat([df, df_thread])\n",
    "        df_thread = df_thread[\n",
    "            [\"id\", \"label\", \"timestamp\"]\n",
    "            + [c for c in df_emb_x.columns if re.match(\"^d\\w*[0-9]\", c)]\n",
    "        ]\n",
    "\n",
    "        for tweet in thread:\n",
    "            end_i += 1\n",
    "            if (end_i - start_i) > k:\n",
    "                start_i = end_i - k\n",
    "            df_add = df_thread[start_i:end_i]\n",
    "\n",
    "            # padding\n",
    "            padding_n = k - (end_i - start_i)\n",
    "            zeros_tile = np.tile(zeros, (padding_n, 1))\n",
    "            df_padi = np.concatenate((df_add, zeros_tile), axis=0)[np.newaxis, :, :]\n",
    "            sample_list.append(df_padi)\n",
    "\n",
    "        start_i = 0\n",
    "        end_i = 0\n",
    "\n",
    "    # append all samples together\n",
    "    print(df.shape)\n",
    "\n",
    "    df_padded = np.concatenate((sample_list), axis=0)\n",
    "    print(df_padded.shape)\n",
    "\n",
    "    return df_padded, df\n",
    "\n",
    "\n",
    "# GET SETS FOR TRAIN/DEV/TEST\n",
    "df_padded_train, df_train = get_data_format(\n",
    "    df_emb_reduced, switch_timelines, subset=\"train\", k=model_specifics[\"k_window\"]\n",
    ")\n",
    "df_padded_dev, df_dev = get_data_format(\n",
    "    df_emb_reduced, switch_timelines, subset=\"dev\", k=model_specifics[\"k_window\"]\n",
    ")\n",
    "df_padded_test, df_test = get_data_format(\n",
    "    df_emb_reduced, switch_timelines, subset=\"test\", k=model_specifics[\"k_window\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing data for Seq-Sig-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4238, 400, 5, 9])\n",
      "torch.Size([281, 400, 5, 9])\n",
      "torch.Size([1049, 400, 5, 9])\n"
     ]
    }
   ],
   "source": [
    "# torch conversion and removal of label and time dimensions for now\n",
    "def get_time_currentpost(df_padded, df, model_specifics):\n",
    "    k = model_specifics[\"k_window\"]\n",
    "    path = torch.from_numpy(df_padded[:, :, 3:].astype(float))\n",
    "\n",
    "    if model_specifics[\"time_injection_post_tp\"] == \"timestamp\":\n",
    "        mean = df_padded[:, :, 2][df_padded[:, :, 2] != 0].mean()\n",
    "        std = df_padded[:, :, 2][df_padded[:, :, 2] != 0].std()\n",
    "        time_feature = (\n",
    "            torch.from_numpy(df_padded[:, :, 2].astype(float)).unsqueeze(1) - mean\n",
    "        ) / std\n",
    "    else:\n",
    "        time_feature = None\n",
    "\n",
    "    if (\n",
    "        (model_specifics[\"post_embedding_tp\"] == \"sentence\")\n",
    "        | (model_specifics[\"post_embedding_tp\"] == \"sentencelstm\")\n",
    "        | (model_specifics[\"post_embedding_tp\"] == \"sentencesiglstm\")\n",
    "        | (model_specifics[\"post_embedding_tp\"] == \"windowsigsentence\")\n",
    "    ):\n",
    "        bert_embeddings = (\n",
    "            torch.tensor(\n",
    "                df[[c for c in df.columns if re.match(\"^e\\w*[0-9]\", c)]]\n",
    "                .astype(float)\n",
    "                .values\n",
    "            )\n",
    "            .unsqueeze(2)\n",
    "            .repeat(1, 1, k)\n",
    "        )\n",
    "    elif model_specifics[\"post_embedding_tp\"] == \"reduced\":\n",
    "        bert_embeddings = (\n",
    "            torch.tensor(\n",
    "                df[[c for c in df.columns if re.match(\"^d\\w*[0-9]\", c)]]\n",
    "                .astype(float)\n",
    "                .values\n",
    "            )\n",
    "            .unsqueeze(2)\n",
    "            .repeat(1, 1, k)\n",
    "        )\n",
    "    else:\n",
    "        bert_embeddings = None\n",
    "\n",
    "    x_data = torch.transpose(path, 1, 2)\n",
    "\n",
    "    if time_feature != None:\n",
    "        x_data = torch.cat((x_data, time_feature), dim=1)\n",
    "    if bert_embeddings != None:\n",
    "        x_data = torch.cat((x_data, bert_embeddings), dim=1)\n",
    "\n",
    "    if model_specifics[\"post_embedding_tp\"] == \"windowsigsentence\":\n",
    "        # shift\n",
    "        emb_str = \"^e\\w*[0-9]\"\n",
    "        embed_list = [c for c in df.columns if re.match(emb_str, c)]\n",
    "        df[\"timeline_match1\"] = df[\"timeline_id\"].eq(df.timeline_id.shift(1))\n",
    "        df[\"timeline_match2\"] = df[\"timeline_id\"].eq(df.timeline_id.shift(2))\n",
    "        df[\"timeline_match3\"] = df[\"timeline_id\"].eq(df.timeline_id.shift(3))\n",
    "        df[\"timeline_match4\"] = df[\"timeline_id\"].eq(df.timeline_id.shift(4))\n",
    "        df[\"timeline_match5\"] = df[\"timeline_id\"].eq(df.timeline_id.shift(5))\n",
    "        df[\"timeline_match6\"] = df[\"timeline_id\"].eq(df.timeline_id.shift(6))\n",
    "        df[\"timeline_match7\"] = df[\"timeline_id\"].eq(df.timeline_id.shift(7))\n",
    "        df[\"timeline_match8\"] = df[\"timeline_id\"].eq(df.timeline_id.shift(8))\n",
    "        df[\"timeline_match9\"] = df[\"timeline_id\"].eq(df.timeline_id.shift(9))\n",
    "        df[\"timeline_match10\"] = df[\"timeline_id\"].eq(df.timeline_id.shift(10))\n",
    "        df[\"timeline_match12\"] = df[\"timeline_id\"].eq(df.timeline_id.shift(12))\n",
    "        df[\"timeline_match15\"] = df[\"timeline_id\"].eq(df.timeline_id.shift(15))\n",
    "        df[\"timeline_match16\"] = df[\"timeline_id\"].eq(df.timeline_id.shift(16))\n",
    "        df[\"timeline_match18\"] = df[\"timeline_id\"].eq(df.timeline_id.shift(18))\n",
    "        df[\"timeline_match21\"] = df[\"timeline_id\"].eq(df.timeline_id.shift(21))\n",
    "        df[\"timeline_match24\"] = df[\"timeline_id\"].eq(df.timeline_id.shift(24))\n",
    "        # CONSTRACT NEW SHIFTED MATRCES\n",
    "        x_datam1 = torch.roll(x_data, 3, 0)\n",
    "        x_datam2 = torch.roll(x_data, 6, 0)\n",
    "        x_datam3 = torch.roll(x_data, 9, 0)\n",
    "        x_datam4 = torch.roll(x_data, 12, 0)\n",
    "        x_datam5 = torch.roll(x_data, 15, 0)\n",
    "        x_datam6 = torch.roll(x_data, 18, 0)\n",
    "        x_datam7 = torch.roll(x_data, 21, 0)\n",
    "        x_datam8 = torch.roll(x_data, 24, 0)\n",
    "        # CREATE MASK AND THEN ASSING 0S BASED ON THAT MASK\n",
    "        mask_m1 = torch.zeros_like(x_data)\n",
    "        mask_m1[~torch.tensor(df[\"timeline_match3\"].values)] = 2\n",
    "        mask_m1 = mask_m1.ge(1)\n",
    "        ####\n",
    "        mask_m2 = torch.zeros_like(x_data)\n",
    "        mask_m2[~torch.tensor(df[\"timeline_match6\"].values)] = 2\n",
    "        mask_m2 = mask_m2.ge(1)\n",
    "        #####\n",
    "        mask_m3 = torch.zeros_like(x_data)\n",
    "        mask_m3[~torch.tensor(df[\"timeline_match9\"].values)] = 2\n",
    "        mask_m3 = mask_m3.ge(1)\n",
    "        #####\n",
    "        mask_m4 = torch.zeros_like(x_data)\n",
    "        mask_m4[~torch.tensor(df[\"timeline_match12\"].values)] = 2\n",
    "        mask_m4 = mask_m4.ge(1)\n",
    "        #####\n",
    "        mask_m5 = torch.zeros_like(x_data)\n",
    "        mask_m5[~torch.tensor(df[\"timeline_match15\"].values)] = 2\n",
    "        mask_m5 = mask_m5.ge(1)\n",
    "        #####\n",
    "        mask_m6 = torch.zeros_like(x_data)\n",
    "        mask_m6[~torch.tensor(df[\"timeline_match18\"].values)] = 2\n",
    "        mask_m6 = mask_m6.ge(1)\n",
    "        #####\n",
    "        mask_m7 = torch.zeros_like(x_data)\n",
    "        mask_m7[~torch.tensor(df[\"timeline_match21\"].values)] = 2\n",
    "        mask_m7 = mask_m7.ge(1)\n",
    "        #####\n",
    "        mask_m8 = torch.zeros_like(x_data)\n",
    "        mask_m8[~torch.tensor(df[\"timeline_match24\"].values)] = 2\n",
    "        mask_m8 = mask_m8.ge(1)\n",
    "        #####\n",
    "        x_datam1[mask_m1] = 0\n",
    "        x_datam2[mask_m2] = 0\n",
    "        x_datam3[mask_m3] = 0\n",
    "        x_datam4[mask_m4] = 0\n",
    "        x_datam5[mask_m5] = 0\n",
    "        x_datam6[mask_m6] = 0\n",
    "        x_datam7[mask_m7] = 0\n",
    "        x_datam8[mask_m8] = 0\n",
    "        #####\n",
    "        x_data = torch.cat(\n",
    "            (\n",
    "                x_data.unsqueeze(3),\n",
    "                x_datam1.unsqueeze(3),\n",
    "                x_datam2.unsqueeze(3),\n",
    "                x_datam3.unsqueeze(3),\n",
    "                x_datam4.unsqueeze(3),\n",
    "                x_datam5.unsqueeze(3),\n",
    "                x_datam6.unsqueeze(3),\n",
    "                x_datam7.unsqueeze(3),\n",
    "                x_datam8.unsqueeze(3),\n",
    "            ),\n",
    "            dim=3,\n",
    "        )\n",
    "\n",
    "    print(x_data.shape)\n",
    "\n",
    "    return x_data, path\n",
    "\n",
    "\n",
    "# final data for model\n",
    "x_train, path = get_time_currentpost(df_padded_train, df_train, model_specifics)\n",
    "x_valid, _ = get_time_currentpost(df_padded_dev, df_dev, model_specifics)\n",
    "x_test, _ = get_time_currentpost(df_padded_test, df_test, model_specifics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Get y labels and test ids\n",
    "y_train = torch.tensor(df_train[\"label\"].astype(float).values, dtype=torch.int64)\n",
    "y_valid = torch.tensor(df_dev[\"label\"].astype(float).values, dtype=torch.int64)\n",
    "y_test = torch.tensor(df_test[\"label\"].astype(float).values, dtype=torch.int64)\n",
    "\n",
    "test_pids = torch.tensor(df_padded_test[:, 0, 0].astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import signatory\n",
    "import numpy as np\n",
    "\n",
    "# ARCHITECTURE 3 - BEST\n",
    "\n",
    "\n",
    "class StackedDeepLSTMSigNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels,\n",
    "        output_channels,\n",
    "        sig_d,\n",
    "        hidden_dim_lstm,\n",
    "        post_dim,\n",
    "        hidden_dim,\n",
    "        output_dim,\n",
    "        dropout_rate,\n",
    "        add_time=False,\n",
    "        augmentation_tp=\"Conv1d\",\n",
    "        augmentation_layers=(),\n",
    "        comb_method=\"concatenation\",\n",
    "        attention=False,\n",
    "    ):\n",
    "        super(StackedDeepLSTMSigNet, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.add_time = add_time\n",
    "        self.augmentation_tp = augmentation_tp\n",
    "        self.comb_method = comb_method\n",
    "        self.BiLSTM = BiLSTM\n",
    "        self.attention = attention\n",
    "        input_bert_dim = 384\n",
    "\n",
    "        # self attention\n",
    "        self.self_attn = nn.MultiheadAttention(\n",
    "            input_channels, num_heads=3, bias=True\n",
    "        ).double()\n",
    "\n",
    "        # Convolution\n",
    "        self.conv = nn.Conv1d(\n",
    "            input_channels, output_channels, 3, stride=1, padding=1\n",
    "        ).double()\n",
    "        self.augment = signatory.Augment(\n",
    "            in_channels=input_channels,\n",
    "            layer_sizes=augmentation_layers,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            stride=1,\n",
    "            include_original=False,\n",
    "            include_time=False,\n",
    "        ).double()\n",
    "        # Non-linearity\n",
    "        self.tanh1 = nn.Tanh()\n",
    "        # Signature with lift\n",
    "        self.signature1 = signatory.LogSignature(depth=sig_d, stream=True)\n",
    "        if self.add_time:\n",
    "            input_dim_lstm = signatory.logsignature_channels(output_channels + 1, sig_d)\n",
    "        else:\n",
    "            input_dim_lstm = signatory.logsignature_channels(output_channels, sig_d)\n",
    "\n",
    "        # Signatures and LSTMs for signature windows\n",
    "        self.lstm_sig1 = nn.LSTM(\n",
    "            input_size=input_dim_lstm,\n",
    "            hidden_size=hidden_dim_lstm[-2],\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=False,\n",
    "        ).double()\n",
    "        self.signature2 = signatory.LogSignature(depth=sig_d, stream=False)\n",
    "\n",
    "        input_dim_lstmsig = signatory.logsignature_channels(hidden_dim_lstm[-2], sig_d)\n",
    "        self.lstm_sig2 = nn.LSTM(\n",
    "            input_size=input_dim_lstmsig,\n",
    "            hidden_size=hidden_dim_lstm[-1],\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        ).double()\n",
    "        self.signature3 = signatory.LogSignature(depth=sig_d, stream=False)\n",
    "\n",
    "        # combination method\n",
    "        if comb_method == \"concatenation\":\n",
    "            # input_dim = signatory.logsignature_channels(hidden_dim_lstm[-1], sig_d) + post_dim\n",
    "            input_dim = hidden_dim_lstm[-1] + post_dim\n",
    "        elif comb_method == \"gated_addition\":\n",
    "            input_dim = input_bert_dim\n",
    "            input_gated_linear = (\n",
    "                signatory.logsignature_channels(hidden_dim_lstm[-1], sig_d) + 1\n",
    "            )\n",
    "            self.fc_scale = nn.Linear(input_gated_linear, input_bert_dim)\n",
    "            # define the scaler parameter\n",
    "            self.scaler = torch.nn.Parameter(torch.zeros(1, input_bert_dim))\n",
    "        elif comb_method == \"gated_concatenation\":\n",
    "            input_gated_linear = (\n",
    "                signatory.logsignature_channels(hidden_dim_lstm[-1], sig_d) + 1\n",
    "            )\n",
    "            input_dim = input_bert_dim + input_gated_linear\n",
    "            # define the scaler parameter\n",
    "            self.scaler1 = torch.nn.Parameter(torch.zeros(1, input_gated_linear))\n",
    "        elif comb_method == \"scaled_concatenation\":\n",
    "            input_dim = (\n",
    "                signatory.logsignature_channels(hidden_dim_lstm[-1], sig_d) + post_dim\n",
    "            )\n",
    "            # define the scaler parameter\n",
    "            self.scaler2 = torch.nn.Parameter(torch.tensor([0.0]))\n",
    "\n",
    "        # Linear function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        # Non-linearity\n",
    "        self.relu1 = nn.ReLU()\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        # Linear function 2:\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # Non-linearity 2\n",
    "        self.relu2 = nn.ReLU()\n",
    "        # Linear function 3 (readout):\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def _unit_deepsignet(self, u):\n",
    "        if self.attention:\n",
    "            # self attention\n",
    "            out = torch.transpose(u, 0, 1)\n",
    "            out = torch.transpose(out, 0, 2)\n",
    "            out_att = self.self_attn(out, out, out)\n",
    "            out = torch.transpose(out_att[0], 0, 1)\n",
    "            out = torch.transpose(out, 1, 2)\n",
    "        else:\n",
    "            out = u\n",
    "\n",
    "        # Convolution\n",
    "        if self.augmentation_tp == \"Conv1d\":\n",
    "            out = self.conv(out)  # get only the path information\n",
    "            out = self.tanh1(out)\n",
    "            out = torch.transpose(out, 1, 2)  # swap dimensions\n",
    "        else:\n",
    "            out = self.augment(torch.transpose(out, 1, 2))\n",
    "\n",
    "        # Add time for signature\n",
    "        if self.add_time:\n",
    "            out = torch.cat(\n",
    "                (\n",
    "                    out,\n",
    "                    torch.transpose(\n",
    "                        u[:, self.input_channels : (self.input_channels + 1), :], 1, 2\n",
    "                    ),\n",
    "                ),\n",
    "                dim=2,\n",
    "            )\n",
    "\n",
    "        # Signature\n",
    "        out = self.signature1(out)\n",
    "        out, (_, _) = self.lstm_sig1(out)\n",
    "        # Signature\n",
    "        out = self.signature2(out)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        # deepsig net for each history window\n",
    "        out = self._unit_deepsignet(x[:, : self.input_channels, :, -1])\n",
    "        out = out.unsqueeze(1)\n",
    "        for window in range(x.shape[3] - 1, 0, -1):\n",
    "            out_unit = self._unit_deepsignet(x[:, : self.input_channels, :, window - 1])\n",
    "            out_unit = out_unit.unsqueeze(1)\n",
    "            out = torch.cat((out, out_unit), dim=1)\n",
    "\n",
    "        # LSTM that combines all deepsignet windows together\n",
    "        _, (out, _) = self.lstm_sig2(out)\n",
    "        out = out[-1, :, :] + out[-2, :, :]\n",
    "        # out = self.signature3(out)\n",
    "\n",
    "        # Combine Last Post Embedding\n",
    "        if self.comb_method == \"concatenation\":\n",
    "            out = torch.cat(\n",
    "                (\n",
    "                    out,\n",
    "                    x[:, self.input_channels : (self.input_channels + 1), :, 0].max(2)[\n",
    "                        0\n",
    "                    ],\n",
    "                    x[:, (self.input_channels + 1) :, 0, 0],\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "        elif self.comb_method == \"gated_addition\":\n",
    "            out_gated = torch.cat(\n",
    "                (\n",
    "                    out,\n",
    "                    x[:, self.input_channels : (self.input_channels + 1), :, 0].max(2)[\n",
    "                        0\n",
    "                    ],\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "            out_gated = self.fc_scale(out_gated.float())\n",
    "            out_gated = self.tanh1(out_gated)\n",
    "            out_gated = torch.mul(self.scaler, out_gated)\n",
    "            # concatenation with bert output\n",
    "            out = out_gated + x[:, (self.input_channels + 1) :, 0]\n",
    "        elif self.comb_method == \"gated_concatenation\":\n",
    "            out_gated = torch.cat(\n",
    "                (\n",
    "                    out,\n",
    "                    x[:, self.input_channels : (self.input_channels + 1), :, 0].max(2)[\n",
    "                        0\n",
    "                    ],\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "            out_gated = torch.mul(self.scaler1, out_gated)\n",
    "            # concatenation with bert output\n",
    "            out = torch.cat((out_gated, x[:, (self.input_channels + 1) :, 0, 0]), dim=1)\n",
    "        elif self.comb_method == \"scaled_concatenation\":\n",
    "            out_gated = torch.cat(\n",
    "                (\n",
    "                    out,\n",
    "                    x[:, self.input_channels : (self.input_channels + 1), :, 0].max(2)[\n",
    "                        0\n",
    "                    ],\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "            out_gated = self.scaler2 * out_gated\n",
    "            # concatenation with bert output\n",
    "            out = torch.cat((out_gated, x[:, (self.input_channels + 1) :, 0, 0]), dim=1)\n",
    "\n",
    "        # FFN: Linear function 1\n",
    "        out = self.fc1(out.float())\n",
    "        # Non-linearity 1\n",
    "        out = self.relu1(out)\n",
    "        # Dropout\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        # FFN: Linear function 2\n",
    "        out = self.fc2(out)\n",
    "        # Non-linearity 2\n",
    "        out = self.relu2(out)\n",
    "        # Dropout\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        # FFN: Linear function 3 (readout)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Seq-Sig-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr= 0.0003  g= 3  h_dim= 64\n",
      "Starting random seed # 0\n",
      "[0/100, 0/67] loss: 0.11752356\n",
      "Current Macro F1: 66.59561385529823\n",
      "Trigger Times: 0\n",
      "[1/100, 0/67] loss: 0.097271957\n",
      "Current Macro F1: 64.03723094822571\n",
      "Trigger Times: 1\n",
      "[2/100, 0/67] loss: 0.090587251\n",
      "Current Macro F1: 62.6982463494725\n",
      "Trigger Times: 2\n",
      "[3/100, 0/67] loss: 0.075255901\n",
      "Current Macro F1: 61.185953166387705\n",
      "Trigger Times: 3\n",
      "[4/100, 0/67] loss: 0.08509589\n",
      "Current Macro F1: 61.23592342342342\n",
      "Trigger Times: 0\n",
      "[5/100, 0/67] loss: 0.088144451\n",
      "Current Macro F1: 63.23779968358282\n",
      "Trigger Times: 0\n",
      "[6/100, 0/67] loss: 0.10385884\n",
      "Current Macro F1: 61.83724467622773\n",
      "Trigger Times: 1\n",
      "[7/100, 0/67] loss: 0.074643873\n",
      "Current Macro F1: 63.483546426325724\n",
      "Trigger Times: 0\n",
      "[8/100, 0/67] loss: 0.082003854\n",
      "Current Macro F1: 60.285137972199735\n",
      "Trigger Times: 1\n",
      "[9/100, 0/67] loss: 0.096544996\n",
      "Current Macro F1: 61.37851262162346\n",
      "Trigger Times: 0\n",
      "[10/100, 0/67] loss: 0.075283684\n",
      "Current Macro F1: 63.627958579881664\n",
      "Trigger Times: 0\n",
      "[11/100, 0/67] loss: 0.090951517\n",
      "Current Macro F1: 63.71393958757672\n",
      "Trigger Times: 0\n",
      "[12/100, 0/67] loss: 0.077881441\n",
      "Current Macro F1: 62.449547289189475\n",
      "Trigger Times: 1\n",
      "[13/100, 0/67] loss: 0.066856563\n",
      "Current Macro F1: 63.94433641546622\n",
      "Trigger Times: 0\n",
      "[14/100, 0/67] loss: 0.07515642\n",
      "Current Macro F1: 62.84111400520938\n",
      "Trigger Times: 1\n",
      "[15/100, 0/67] loss: 0.066919513\n",
      "Current Macro F1: 64.05275304978568\n",
      "Trigger Times: 0\n",
      "[16/100, 0/67] loss: 0.069028109\n",
      "Current Macro F1: 61.72683488492916\n",
      "Trigger Times: 1\n",
      "[17/100, 0/67] loss: 0.066269293\n",
      "Current Macro F1: 64.93709056878444\n",
      "Trigger Times: 0\n",
      "[18/100, 0/67] loss: 0.073135331\n",
      "Current Macro F1: 61.95831378593928\n",
      "Trigger Times: 1\n",
      "[19/100, 0/67] loss: 0.062975273\n",
      "Current Macro F1: 63.798555533866875\n",
      "Trigger Times: 0\n",
      "[20/100, 0/67] loss: 0.065090373\n",
      "Current Macro F1: 62.40098479982873\n",
      "Trigger Times: 1\n",
      "[21/100, 0/67] loss: 0.053659275\n",
      "Current Macro F1: 63.51941461111766\n",
      "Trigger Times: 0\n",
      "[22/100, 0/67] loss: 0.048899189\n",
      "Current Macro F1: 65.90023631602479\n",
      "Trigger Times: 0\n",
      "[23/100, 0/67] loss: 0.070685677\n",
      "Current Macro F1: 65.74592800082996\n",
      "Trigger Times: 1\n",
      "[24/100, 0/67] loss: 0.05884105\n",
      "Current Macro F1: 63.94433641546622\n",
      "Trigger Times: 2\n",
      "[25/100, 0/67] loss: 0.066949785\n",
      "Current Macro F1: 62.449547289189475\n",
      "Trigger Times: 3\n",
      "[26/100, 0/67] loss: 0.056906097\n",
      "Current Macro F1: 62.027027027027025\n",
      "Trigger Times: 4\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import random\n",
    "from datetime import date\n",
    "import math\n",
    "\n",
    "from classification_utils import Folds, set_seed, validation, training, testing\n",
    "from deepsignatureffn import FocalLoss, ClassBalanced_FocalLoss\n",
    "from deepsignatureffn import StackedDeepSigNet\n",
    "\n",
    "# ================================\n",
    "save_results = True\n",
    "# ================================\n",
    "k = model_specifics[\"k_window\"]\n",
    "# GLOBAL MODEL PARAMETERS\n",
    "input_dim = x_train.shape[1]\n",
    "output_channels = [10]  # [10,12]\n",
    "hidden_dim = [64]  # [32, 64]\n",
    "output_dim = 2\n",
    "loss = model_specifics[\"loss_function\"]  #'focal' #cbfocal\n",
    "dropout_rate = [0.1]\n",
    "if model_specifics[\"time_injection_history_tp\"] == \"timestamp\":\n",
    "    add_time = True\n",
    "else:\n",
    "    add_time = False\n",
    "\n",
    "sig_d = 3\n",
    "hidden_dim_lstm = [(10, 300)]\n",
    "input_channels = path.shape[2]\n",
    "history_len = x_train.shape[3]\n",
    "post_dim = x_train.shape[1] - input_channels\n",
    "add_time = False\n",
    "augmentation_tp = \"Conv1d\"\n",
    "augmentation_layers = ()\n",
    "attention = False\n",
    "BiLSTM = False\n",
    "blocks = 2\n",
    "# ================================\n",
    "num_epochs = 100\n",
    "learning_rate = [0.0003]  # [0.0001, 0.0003]\n",
    "gamma = [3]  # [2,3]\n",
    "beta = 0.999\n",
    "BATCH_SIZE = 64\n",
    "NUM_folds = 1\n",
    "patience = 4\n",
    "weight_decay_adam = 0.0001\n",
    "RANDOM_SEED_list = [0]  # [0, 1, 12, 123, 1234]\n",
    "\n",
    "# ================================\n",
    "if model_specifics[\"dimensionality_reduction\"] == True:\n",
    "    model_code_name = (\n",
    "        model_specifics[\"data\"]\n",
    "        + \"_\"\n",
    "        + model_specifics[\"global_embedding_tp\"]\n",
    "        + \"_\"\n",
    "        + str(model_specifics[\"dimensionality_reduction_tp\"])\n",
    "        + str(model_specifics[\"dimensionality_reduction_components\"])\n",
    "        + \"_\"\n",
    "        + str(model_specifics[\"time_injection_history_tp\"])\n",
    "        + str(model_specifics[\"time_injection_post_tp\"])\n",
    "        + \"_\"\n",
    "        + str(model_specifics[\"post_embedding_tp\"])\n",
    "        + \"_\"\n",
    "        + str(model_specifics[\"feature_combination_method\"])\n",
    "        + \"_\"\n",
    "        + str(model_specifics[\"signature_tp\"])\n",
    "        + \"_\"\n",
    "        + str(model_specifics[\"signature_dimensions\"])\n",
    "        + \"_\"\n",
    "        + str(model_specifics[\"classifier_name\"])\n",
    "        + \"_\"\n",
    "        + str(model_specifics[\"loss_function\"])\n",
    "        + \"_\"\n",
    "        + str(model_specifics[\"k_window\"])\n",
    "        + \"_\"\n",
    "        + str(model_specifics[\"classes_num\"])\n",
    "    )\n",
    "else:\n",
    "    model_code_name = (\n",
    "        model_specifics[\"data\"]\n",
    "        + \"_\"\n",
    "        + model_specifics[\"global_embedding_tp\"]\n",
    "        + \"_\"\n",
    "        + str(model_specifics[\"time_injection_history_tp\"])\n",
    "        + str(model_specifics[\"time_injection_post_tp\"])\n",
    "        + \"_\"\n",
    "        + str(model_specifics[\"post_embedding_tp\"])\n",
    "        + \"_\"\n",
    "        + str(model_specifics[\"feature_combination_method\"])\n",
    "        + \"_\"\n",
    "        + str(model_specifics[\"signature_tp\"])\n",
    "        + \"_\"\n",
    "        + str(model_specifics[\"signature_dimensions\"])\n",
    "        + \"_\"\n",
    "        + str(model_specifics[\"classifier_name\"])\n",
    "        + \"_\"\n",
    "        + str(model_specifics[\"loss_function\"])\n",
    "        + \"_\"\n",
    "        + str(model_specifics[\"k_window\"])\n",
    "        + \"_\"\n",
    "        + str(model_specifics[\"classes_num\"])\n",
    "    )\n",
    "\n",
    "\n",
    "FOLDER_models = \"/storage/ttseriotou/rumour_eval/models/v1/\"\n",
    "FOLDER_results = \"/storage/ttseriotou/rumour_eval/results/v1/\"\n",
    "\n",
    "# ================================\n",
    "# K FOLD RUNS\n",
    "ft_i = 0  # run number\n",
    "for out_ch in output_channels:\n",
    "    for lr in learning_rate:\n",
    "        for g in gamma:\n",
    "            for dp in dropout_rate:\n",
    "                for h_dim in hidden_dim:\n",
    "                    for lstm_dim in hidden_dim_lstm:\n",
    "                        # out_ch =  aug_l[2]\n",
    "                        str_version = \"tuning\" + str(ft_i)\n",
    "                        print(\n",
    "                            \"lr=\",\n",
    "                            lr,\n",
    "                            \" g=\",\n",
    "                            g,\n",
    "                            \" dp=\",\n",
    "                            dp,\n",
    "                            \" h_dim=\",\n",
    "                            h_dim,\n",
    "                            \" lstm_dim=\",\n",
    "                            lstm_dim,\n",
    "                        )\n",
    "                        ft_i += 1\n",
    "\n",
    "                        classifier_params = {\n",
    "                            \"augmentation_tp\": augmentation_tp,\n",
    "                            \"input_channels\": input_channels,\n",
    "                            \"output_channels\": out_ch,\n",
    "                            \"augmentation_layers\": augmentation_layers,\n",
    "                            \"sig_d\": sig_d,\n",
    "                            \"post_dim\": post_dim,\n",
    "                            \"hidden_dim_lstm\": lstm_dim,\n",
    "                            \"hidden_dim\": h_dim,\n",
    "                            \"output_dim\": output_dim,\n",
    "                            \"dropout_rate\": dp,\n",
    "                            \"num_epochs\": num_epochs,\n",
    "                            \"learning_rate\": lr,\n",
    "                            \"BiLSTM\": BiLSTM,\n",
    "                            \"blocks\": blocks,\n",
    "                            \"gamma\": g,\n",
    "                            \"k_window\": k,\n",
    "                            \"BATCH_SIZE\": BATCH_SIZE,\n",
    "                            \"NUM_folds\": NUM_folds,\n",
    "                            \"patience\": patience,\n",
    "                            \"weight_decay_adam\": weight_decay_adam,\n",
    "                            \"RANDOM_SEED_list\": RANDOM_SEED_list,\n",
    "                        }\n",
    "\n",
    "                        for my_ran_seed in RANDOM_SEED_list:\n",
    "                            set_seed(my_ran_seed)\n",
    "                            myGenerator = torch.Generator()\n",
    "                            myGenerator.manual_seed(my_ran_seed)\n",
    "                            for test_fold in range(NUM_folds):\n",
    "                                print(\"Starting random seed #\", my_ran_seed)\n",
    "\n",
    "                                # data loaders with batches\n",
    "                                train = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "                                valid = torch.utils.data.TensorDataset(x_valid, y_valid)\n",
    "                                # test = torch.utils.data.TensorDataset( torch.cat((x_test,torch.zeros(test_pids.shape).unsqueeze(1).unsqueeze(2).repeat(1, 1, k)),1) , y_test)\n",
    "                                test = torch.utils.data.TensorDataset(\n",
    "                                    torch.cat(\n",
    "                                        (\n",
    "                                            x_test,\n",
    "                                            torch.zeros(test_pids.shape)\n",
    "                                            .unsqueeze(1)\n",
    "                                            .unsqueeze(2)\n",
    "                                            .unsqueeze(3)\n",
    "                                            .repeat(1, 1, k, history_len),\n",
    "                                        ),\n",
    "                                        1,\n",
    "                                    ),\n",
    "                                    y_test,\n",
    "                                )\n",
    "\n",
    "                                train_loader = torch.utils.data.DataLoader(\n",
    "                                    dataset=train, batch_size=BATCH_SIZE, shuffle=True\n",
    "                                )\n",
    "                                valid_loader = torch.utils.data.DataLoader(\n",
    "                                    dataset=valid, batch_size=BATCH_SIZE, shuffle=True\n",
    "                                )\n",
    "                                test_loader = torch.utils.data.DataLoader(\n",
    "                                    dataset=test, batch_size=BATCH_SIZE, shuffle=True\n",
    "                                )\n",
    "\n",
    "                                # early stopping params\n",
    "                                last_metric = 0\n",
    "                                trigger_times = 0\n",
    "                                best_metric = 0\n",
    "\n",
    "                                # model definitions\n",
    "                                # model = StackedDeepSigNet(input_channels, out_ch, sig_d, lstm_dim, post_dim, h_dim, output_dim, dp, add_time, augmentation_tp, augmentation_layers, BiLSTM, comb_method=model_specifics['feature_combination_method'], blocks=blocks)\n",
    "                                model = StackedDeepLSTMSigNet(\n",
    "                                    input_channels,\n",
    "                                    out_ch,\n",
    "                                    sig_d,\n",
    "                                    lstm_dim,\n",
    "                                    post_dim,\n",
    "                                    h_dim,\n",
    "                                    output_dim,\n",
    "                                    dp,\n",
    "                                    add_time,\n",
    "                                    augmentation_tp,\n",
    "                                    augmentation_layers,\n",
    "                                    comb_method=model_specifics[\n",
    "                                        \"feature_combination_method\"\n",
    "                                    ],\n",
    "                                    attention=attention,\n",
    "                                )\n",
    "\n",
    "                                # loss function\n",
    "                                if loss == \"focal\":\n",
    "                                    alpha_values = torch.Tensor(\n",
    "                                        [\n",
    "                                            math.sqrt(\n",
    "                                                1\n",
    "                                                / (\n",
    "                                                    y_train[y_train == 0].shape[0]\n",
    "                                                    / y_train.shape[0]\n",
    "                                                )\n",
    "                                            ),\n",
    "                                            math.sqrt(\n",
    "                                                1\n",
    "                                                / (\n",
    "                                                    y_train[y_train == 1].shape[0]\n",
    "                                                    / y_train.shape[0]\n",
    "                                                )\n",
    "                                            ),\n",
    "                                        ]\n",
    "                                    )\n",
    "                                    criterion = FocalLoss(gamma=g, alpha=alpha_values)\n",
    "                                elif loss == \"cbfocal\":\n",
    "                                    classifier_params[\"beta\"] = beta\n",
    "                                    samples_count = torch.Tensor(\n",
    "                                        [\n",
    "                                            y_train[y_train == 0].shape[0],\n",
    "                                            y_train[y_train == 1].shape[0],\n",
    "                                            y_train[y_train == 2].shape[0],\n",
    "                                        ]\n",
    "                                    )\n",
    "                                    criterion = ClassBalanced_FocalLoss(\n",
    "                                        gamma=g,\n",
    "                                        beta=beta,\n",
    "                                        no_of_classes=3,\n",
    "                                        samples_per_cls=samples_count,\n",
    "                                    )\n",
    "                                optimizer = torch.optim.Adam(\n",
    "                                    model.parameters(),\n",
    "                                    lr=lr,\n",
    "                                    weight_decay=weight_decay_adam,\n",
    "                                )\n",
    "\n",
    "                                # model train/validation per epoch\n",
    "                                for epoch in range(num_epochs):\n",
    "                                    training(\n",
    "                                        model,\n",
    "                                        train_loader,\n",
    "                                        criterion,\n",
    "                                        optimizer,\n",
    "                                        epoch,\n",
    "                                        num_epochs,\n",
    "                                    )\n",
    "\n",
    "                                    # Early stopping\n",
    "                                    _, f1_v, labels_val, predicted_val = validation(\n",
    "                                        model, valid_loader, criterion\n",
    "                                    )\n",
    "\n",
    "                                    print(\"Current Macro F1:\", f1_v)\n",
    "\n",
    "                                    if f1_v > best_metric:\n",
    "                                        best_metric = f1_v\n",
    "\n",
    "                                        # test and save so far best model\n",
    "                                        (\n",
    "                                            predicted_test,\n",
    "                                            labels_test,\n",
    "                                            pids_test,\n",
    "                                        ) = testing(model, test_loader)\n",
    "\n",
    "                                        results = {\n",
    "                                            \"model_code_name\": model_code_name,\n",
    "                                            \"model_specifics\": model_specifics,\n",
    "                                            \"classifier_params\": classifier_params,\n",
    "                                            \"date_run\": date.today().strftime(\n",
    "                                                \"%d/%m/%Y\"\n",
    "                                            ),\n",
    "                                            \"test_pids\": pids_test,  # test_pids,\n",
    "                                            \"labels\": labels_test,\n",
    "                                            \"predictions\": predicted_test,\n",
    "                                            \"labels_val\": labels_val,\n",
    "                                            \"predicted_val\": predicted_val,\n",
    "                                            \"test_fold\": test_fold,\n",
    "                                            \"random_seed\": my_ran_seed,\n",
    "                                            \"epoch\": epoch,\n",
    "                                        }\n",
    "\n",
    "                                        if save_results == True:\n",
    "                                            # file_name_results = FOLDER_results + model_code_name + \"_\" + str(my_ran_seed) + \"seed\" + \"_\" + str_version + '.pkl'\n",
    "                                            # file_name_model = FOLDER_models + model_code_name + \"_\" + str(my_ran_seed) + \"seed\"  + \"_\" + str_version +'.pkl'\n",
    "                                            file_name_results = (\n",
    "                                                FOLDER_results\n",
    "                                                + model_code_name\n",
    "                                                + \"_\"\n",
    "                                                + str(my_ran_seed)\n",
    "                                                + \"seed\"\n",
    "                                                + \".pkl\"\n",
    "                                            )\n",
    "                                            file_name_model = (\n",
    "                                                FOLDER_models\n",
    "                                                + model_code_name\n",
    "                                                + \"_\"\n",
    "                                                + str(my_ran_seed)\n",
    "                                                + \"seed\"\n",
    "                                                + \".pkl\"\n",
    "                                            )\n",
    "                                            pickle.dump(\n",
    "                                                results, open(file_name_results, \"wb\")\n",
    "                                            )\n",
    "                                            # torch.save(model.state_dict(), file_name_model)\n",
    "\n",
    "                                    if f1_v < last_metric:\n",
    "                                        trigger_times += 1\n",
    "                                        print(\"Trigger Times:\", trigger_times)\n",
    "\n",
    "                                        if trigger_times >= patience:\n",
    "                                            print(\"Early stopping!\")\n",
    "                                            break\n",
    "\n",
    "                                    else:\n",
    "                                        print(\"Trigger Times: 0\")\n",
    "                                        trigger_times = 0\n",
    "\n",
    "                                    last_metric = f1_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rumoureval_SBERT_umap15_Nonetimestamp_windowsigsentence_concatenation_log_3_Seq-Sig-Net_focal_2class'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_code_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "\n",
    "def process_model_results2(model_code_name, FOLDER_results, type=\"Talklife\"):\n",
    "    if type == \"Talklife\":\n",
    "        per_model_files = [\n",
    "            f\n",
    "            for f in listdir(FOLDER_results)\n",
    "            if model_code_name in f\n",
    "            if \"tuning\" not in f\n",
    "            if \"Reddit\" not in f\n",
    "        ]\n",
    "    else:\n",
    "        per_model_files = [\n",
    "            f\n",
    "            for f in listdir(FOLDER_results)\n",
    "            if model_code_name in f\n",
    "            if \"tuning\" not in f\n",
    "        ]\n",
    "\n",
    "    print(\"There are \", len(per_model_files), \" files\")\n",
    "    metrics_overall = pd.DataFrame(\n",
    "        0,\n",
    "        index=[\"No Change\", \"Change\", \"accuracy\", \"macro avg\", \"weighted avg\"],\n",
    "        columns=[\"precision\", \"recall\", \"f1-score\", \"support\"],\n",
    "    )\n",
    "    with open(FOLDER_results + per_model_files[0], \"rb\") as fin:\n",
    "        results0 = pickle.load(fin)\n",
    "\n",
    "    for my_ran_seed in results0[\"classifier_params\"][\"RANDOM_SEED_list\"]:\n",
    "        labels_final = torch.empty((0))\n",
    "        predicted_final = torch.empty((0))\n",
    "\n",
    "        seed_files = [f for f in per_model_files if (str(my_ran_seed) + \"seed\") in f]\n",
    "        for sf in seed_files:\n",
    "            with open(FOLDER_results + sf, \"rb\") as fin:\n",
    "                results = pickle.load(fin)\n",
    "                labels_results = results[\"labels\"]\n",
    "                predictions_results = results[\"predictions\"]\n",
    "\n",
    "            # for each seed combine fold results\n",
    "            labels_final = torch.cat([labels_final, labels_results])\n",
    "            predicted_final = torch.cat([predicted_final, predictions_results])\n",
    "\n",
    "        # calculate metrics for each seed\n",
    "        metrics_tab = metrics.classification_report(\n",
    "            labels_final,\n",
    "            predicted_final,\n",
    "            target_names=[\"No Change\", \"Change\"],\n",
    "            output_dict=True,\n",
    "        )\n",
    "        metrics_tab = pd.DataFrame(metrics_tab).transpose()\n",
    "        # combine the metrics with the rest of the seeds in order to take average at the end\n",
    "        metrics_overall += metrics_tab\n",
    "\n",
    "    return metrics_overall / len(results0[\"classifier_params\"][\"RANDOM_SEED_list\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  1  files\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>No Change</th>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.422587</td>\n",
       "      <td>0.565164</td>\n",
       "      <td>549.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Change</th>\n",
       "      <td>0.592021</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.720439</td>\n",
       "      <td>500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.659676</td>\n",
       "      <td>0.659676</td>\n",
       "      <td>0.659676</td>\n",
       "      <td>0.659676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.722481</td>\n",
       "      <td>0.671293</td>\n",
       "      <td>0.642801</td>\n",
       "      <td>1049.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.728575</td>\n",
       "      <td>0.659676</td>\n",
       "      <td>0.639175</td>\n",
       "      <td>1049.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "No Change      0.852941  0.422587  0.565164   549.000000\n",
       "Change         0.592021  0.920000  0.720439   500.000000\n",
       "accuracy       0.659676  0.659676  0.659676     0.659676\n",
       "macro avg      0.722481  0.671293  0.642801  1049.000000\n",
       "weighted avg   0.728575  0.659676  0.639175  1049.000000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##CURRENT - TEST\n",
    "from classification_utils import process_model_results\n",
    "\n",
    "FOLDER_results = \"/storage/ttseriotou/rumour_eval/results/v1/\"\n",
    "\n",
    "model_code_name = \"Rumoureval_SBERT_umap15_Nonetimestamp_windowsigsentence_concatenation_log_3_Seq-Sig-Net_focal_2class\"\n",
    "process_model_results2(model_code_name, FOLDER_results, type=\"Rumoureval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]\n",
      "lr= 0.0001 dropout=  0.25 h_dim= 32 lstm_dim= (12, 300) gamma= 2 out channels= 12\n",
      "              precision    recall  f1-score      support\n",
      "No Change      0.728814  0.391621  0.509479   549.000000\n",
      "Change         0.557029  0.840000  0.669856   500.000000\n",
      "accuracy       0.605338  0.605338  0.605338     0.605338\n",
      "macro avg      0.642921  0.615811  0.589668  1049.000000\n",
      "weighted avg   0.646933  0.605338  0.585922  1049.000000\n",
      "lr= 0.0001 dropout=  0.25 h_dim= 32 lstm_dim= (12, 300) gamma= 2 out channels= 12\n",
      "              precision    recall  f1-score      support\n",
      "No Change      0.728814  0.391621  0.509479   549.000000\n",
      "Change         0.557029  0.840000  0.669856   500.000000\n",
      "accuracy       0.605338  0.605338  0.605338     0.605338\n",
      "macro avg      0.642921  0.615811  0.589668  1049.000000\n",
      "weighted avg   0.646933  0.605338  0.585922  1049.000000\n",
      "lr= 0.0001 dropout=  0.25 h_dim= 32 lstm_dim= (10, 200) gamma= 2 out channels= 12\n",
      "              precision    recall  f1-score      support\n",
      "No Change      0.769759  0.408015  0.533333   549.000000\n",
      "Change         0.571240  0.866000  0.688394   500.000000\n",
      "accuracy       0.626311  0.626311  0.626311     0.626311\n",
      "macro avg      0.670500  0.637007  0.610864  1049.000000\n",
      "weighted avg   0.675136  0.626311  0.607242  1049.000000\n",
      "lr= 0.0001 dropout=  0.25 h_dim= 32 lstm_dim= (10, 200) gamma= 2 out channels= 12\n",
      "              precision    recall  f1-score      support\n",
      "No Change      0.769759  0.408015  0.533333   549.000000\n",
      "Change         0.571240  0.866000  0.688394   500.000000\n",
      "accuracy       0.626311  0.626311  0.626311     0.626311\n",
      "macro avg      0.670500  0.637007  0.610864  1049.000000\n",
      "weighted avg   0.675136  0.626311  0.607242  1049.000000\n",
      "lr= 0.0001 dropout=  0.5 h_dim= 32 lstm_dim= (12, 300) gamma= 3 out channels= 12\n",
      "              precision    recall  f1-score      support\n",
      "No Change      0.757188  0.431694  0.549884   549.000000\n",
      "Change         0.576087  0.848000  0.686084   500.000000\n",
      "accuracy       0.630124  0.630124  0.630124     0.630124\n",
      "macro avg      0.666638  0.639847  0.617984  1049.000000\n",
      "weighted avg   0.670867  0.630124  0.614803  1049.000000\n"
     ]
    }
   ],
   "source": [
    "# BEST K MODELS - TEST LOOP (window k=5)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "\n",
    "k = 5\n",
    "FOLDER_results = \"/storage/ttseriotou/rumour_eval/results/v1/\"\n",
    "\n",
    "model_code_name = \"Rumoureval_SBERT_umap15_Nonetimestamp_windowsigsentence_concatenation_log_3_Seq-Sig-Net_focal_5_2class\"\n",
    "metrics_overall = pd.DataFrame(\n",
    "    0,\n",
    "    index=[\"No Change\", \"Change\", \"accuracy\", \"macro avg\", \"weighted avg\"],\n",
    "    columns=[\"precision\", \"recall\", \"f1-score\", \"support\"],\n",
    ")\n",
    "\n",
    "# get all tuning files\n",
    "per_model_files = [\n",
    "    f\n",
    "    for f in listdir(FOLDER_results)\n",
    "    if \"tuning\" in f\n",
    "    if model_code_name in f\n",
    "    if \"Reddit\" not in f\n",
    "]\n",
    "\n",
    "# get the indices of tuning files\n",
    "files_ind = [\n",
    "    int(f[: f.index(\".\")].split(\"_\")[-1].replace(\"tuning\", \"\")) for f in per_model_files\n",
    "]\n",
    "files_ind = list(set(files_ind))\n",
    "dict_f1 = {}\n",
    "\n",
    "print(files_ind)\n",
    "for t in files_ind:\n",
    "    labels_final = torch.empty((0))\n",
    "    predicted_final = torch.empty((0))\n",
    "\n",
    "    tuning_files = [f for f in per_model_files if (\"tuning\" + str(t) + \".\") in f]\n",
    "    for sf in tuning_files:\n",
    "        with open(FOLDER_results + sf, \"rb\") as fin:\n",
    "            results = pickle.load(fin)\n",
    "            labels_results = results[\"labels_val\"]\n",
    "            predictions_results = results[\"predicted_val\"]\n",
    "\n",
    "        # for each seed combine fold results\n",
    "        labels_final = torch.cat([labels_final, labels_results])\n",
    "        predicted_final = torch.cat([predicted_final, predictions_results])\n",
    "\n",
    "    # calculate metrics for each seed\n",
    "    metrics_tab = metrics.classification_report(\n",
    "        labels_final,\n",
    "        predicted_final,\n",
    "        target_names=[\"No Change\", \"Change\"],\n",
    "        output_dict=True,\n",
    "    )\n",
    "    metrics_tab = pd.DataFrame(metrics_tab).transpose()\n",
    "    params = results[\"classifier_params\"]\n",
    "    f1 = metrics_tab[\"f1-score\"][\"macro avg\"]\n",
    "    dict_f1[t] = f1\n",
    "\n",
    "dict_f1 = Counter(dict_f1)\n",
    "\n",
    "for top in dict_f1.most_common()[:k]:\n",
    "    labels_final = torch.empty((0))\n",
    "    predicted_final = torch.empty((0))\n",
    "\n",
    "    tuning_files = [f for f in per_model_files if (\"tuning\" + str(top[0]) + \".\") in f]\n",
    "\n",
    "    for sf in tuning_files:\n",
    "        with open(FOLDER_results + sf, \"rb\") as fin:\n",
    "            results = pickle.load(fin)\n",
    "            labels_results = results[\"labels\"]\n",
    "            predictions_results = results[\"predictions\"]\n",
    "\n",
    "        # for each seed combine fold results\n",
    "        labels_final = torch.cat([labels_final, labels_results])\n",
    "        predicted_final = torch.cat([predicted_final, predictions_results])\n",
    "\n",
    "    # calculate metrics for each seed\n",
    "    metrics_tab = metrics.classification_report(\n",
    "        labels_final,\n",
    "        predicted_final,\n",
    "        target_names=[\"No Change\", \"Change\"],\n",
    "        output_dict=True,\n",
    "    )\n",
    "    metrics_tab = pd.DataFrame(metrics_tab).transpose()\n",
    "    params = results[\"classifier_params\"]\n",
    "    print(\n",
    "        \"lr=\",\n",
    "        params[\"learning_rate\"],\n",
    "        \"dropout= \",\n",
    "        params[\"dropout_rate\"],\n",
    "        \"h_dim=\",\n",
    "        params[\"hidden_dim\"],\n",
    "        \"lstm_dim=\",\n",
    "        params[\"hidden_dim_lstm\"],\n",
    "        \"gamma=\",\n",
    "        params[\"gamma\"],\n",
    "        \"out channels=\",\n",
    "        params[\"output_channels\"],\n",
    "    )\n",
    "    print(metrics_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38-MoC",
   "language": "python",
   "name": "py38-moc"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
