{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ttseriotou/anaconda3/envs/py38-MoC/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.path.insert(0, \"../../timeline_generation/\")  # Adds higher directory to python modules path\n",
    "import data_handler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18604\n"
     ]
    }
   ],
   "source": [
    "TalkLifeDataset = data_handler.TalkLifeDataset()\n",
    "annotations = TalkLifeDataset.return_annotated_timelines(load_from_pickle=False)\n",
    "annotations = annotations[annotations['content']!='nan']\n",
    "\n",
    "sample_size = annotations.shape[0]\n",
    "print(sample_size)\n",
    "#annotations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# ================================\n",
    "#model specifics\n",
    "model_specifics = {\"global_embedding_tp\": 'SBERT', #options: SBERT, BERT_cls , BERT_mean, BERT_max\n",
    "    \"dimensionality_reduction_tp\": 'umap', #options: ppapca, ppapcappa, umap\n",
    "    \"dimensionality_reduction_components\": 15, # options: any int number between 1 and embedding dimensions\n",
    "    \"dimensionality_reduction\": True, #options: True, False\n",
    "    \"time_injection_history_tp\": None, #options: timestamp, None\n",
    "    \"time_injection_post_tp\": 'timestamp', #options: timestamp, None\n",
    "    \"signature_dimensions\": 3, #options: any int number larger than 1\n",
    "    \"post_embedding_tp\": 'sentence', #options: sentence, reduced\n",
    "    \"feature_combination_method\": 'concatenation', #options: concatenation, gated_addition \n",
    "    \"signature_tp\": 'log', # options: log, sig\n",
    "    \"augmentation_tp\": 'Conv1d', #options: Conv1d, CNN\n",
    "    \"loss_function\": 'focal', #options: focal, cbfocal\n",
    "    \"reduced_network_components\": 13 , #any integer greater than 1 \n",
    "    \"classifier_name\": 'Conv1d3kernel13channelSigLSTMSigLSTMSigFFN2hidden', #'Conv1d3kernel10channelSigFFN2hidden', # options: FFN2hidden (any future classifiers added)\n",
    "    \"classes_num\": '3class', #options: 3class (5class to be added in the future)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post Embeddings, Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18604, 384)\n",
      "(18604, 15)\n"
     ]
    }
   ],
   "source": [
    "#post embedding\n",
    "from embeddings import Representations\n",
    "\n",
    "rep = Representations(type = model_specifics['global_embedding_tp'])\n",
    "embeddings_sentence = rep.get_embeddings()\n",
    "\n",
    "print(embeddings_sentence.shape)\n",
    "\n",
    "#dimensionality reduction\n",
    "from dimensionality_reduction import DimensionalityReduction\n",
    "\n",
    "reduction = DimensionalityReduction(method= model_specifics['dimensionality_reduction_tp'], components=model_specifics['dimensionality_reduction_components'])\n",
    "embeddings_reduced = reduction.fit_transform(embeddings_sentence)\n",
    "\n",
    "print(embeddings_reduced.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ttseriotou/signature_transforms/experiments/timeinjection.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['time_diff'][i] = (df['datetime'][i] - df['datetime'][i-1] ).total_seconds() / 60\n",
      "/home/ttseriotou/signature_transforms/experiments/timeinjection.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['timeline_index'][first_index:last_index] = np.arange(t_id_len)\n"
     ]
    }
   ],
   "source": [
    "#concatenate new dataframe\n",
    "from dataset import get_modeling_dataframe\n",
    "df = get_modeling_dataframe(annotations, embeddings_sentence, embeddings_reduced)\n",
    "\n",
    "#get time features\n",
    "from timeinjection import TimeFeatures, Padding\n",
    "tf = TimeFeatures()\n",
    "df = tf.get_time_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18604, 5, 17)\n"
     ]
    }
   ],
   "source": [
    "#implement a variation that gets only the last k posts (k=5) so that we avoid the issue of variable lengths in signature\n",
    "from time import time\n",
    "\n",
    "if (model_specifics['dimensionality_reduction'] == True):\n",
    "    emb_str = \"^d\\w*[0-9]\"\n",
    "else:\n",
    "    emb_str = \"^e\\w*[0-9]\"\n",
    "\n",
    "id_counts = df.groupby(['timeline_id'])['timeline_id'].count()#.set_index(['timeline_id'])\n",
    "time_n = id_counts.max()\n",
    "df_new = np.array(df[['timeline_id','label','time_encoding']+[c for c in df.columns if re.match(emb_str, c)]])\n",
    "\n",
    "#iterate to create slices\n",
    "start_i = 0\n",
    "end_i = 0\n",
    "dims = df_new.shape[1]\n",
    "zeros = np.concatenate(( np.array([100]), np.repeat(0,dims-2) ),axis=0)\n",
    "sample_list = []\n",
    "zero_padding = True\n",
    "k_last = True\n",
    "k = 5\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    t1 = time()\n",
    "    if (i==0):\n",
    "        i_prev = 0\n",
    "    else:\n",
    "        i_prev = i-1\n",
    "    if (df['timeline_id'][i]==df['timeline_id'][i_prev]):\n",
    "        end_i +=1\n",
    "        if ((k_last==True) & ((end_i - start_i) > k)):\n",
    "            start_i = end_i - k\n",
    "    else: \n",
    "        start_i = i\n",
    "        end_i = i+1\n",
    "\n",
    "    #data point with history\n",
    "    df_add = df_new[start_i:end_i, 1:][np.newaxis, :, :]\n",
    "    #padding length\n",
    "    if (k_last == True):\n",
    "        padding_n = k - (end_i- start_i)\n",
    "    else:\n",
    "        padding_n = time_n - (end_i- start_i)\n",
    "    #create padding\n",
    "    if zero_padding:\n",
    "        #padding = np.concatenate((np.array([100]), np.array(df_new[end_i-1, 2]).reshape(1), zeros[2:]), axis=0)\n",
    "        #zeros_tile = np.tile(padding,(padding_n,1))[np.newaxis, :, :]\n",
    "        zeros_tile = np.tile(zeros,(padding_n,1))[np.newaxis, :, :]\n",
    "    else:\n",
    "        zeros_tile = np.tile(df_new[end_i-1, 1:],(padding_n,1))[np.newaxis, :, :]\n",
    "    #append zero padding\n",
    "    df_padi = np.concatenate((df_add , zeros_tile),axis=1) \n",
    "    #append each sample to final list\n",
    "    sample_list.append(df_padi)\n",
    "  \n",
    "df_padded = np.concatenate(sample_list)\n",
    "print(df_padded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparation of Data for Deep Signature Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18604, 400, 5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#torch conversion and removal of label and time dimensions for now\n",
    "path = torch.from_numpy(df_padded[: , : , 2:].astype(float))\n",
    "  \n",
    "if (model_specifics['time_injection_post_tp']== 'timestamp'):\n",
    "    mean = df_padded[: , : , 1][df_padded[: , : , 1]!=0].mean()\n",
    "    std = df_padded[: , : , 1][df_padded[: , : , 1]!=0].std()\n",
    "    time_feature = (torch.from_numpy(df_padded[: , : , 1].astype(float)).unsqueeze(1) - mean) /std\n",
    "    time_feature[time_feature < -100] = 0\n",
    "else:\n",
    "    time_feature = None\n",
    "\n",
    "if (model_specifics['post_embedding_tp'] == 'sentence'):\n",
    "    bert_embeddings = torch.tensor(df[[c for c in df.columns if re.match(\"^e\\w*[0-9]\", c)]].values).unsqueeze(2).repeat(1, 1, k)\n",
    "elif (model_specifics['post_embedding_tp'] == 'reduced'):\n",
    "    bert_embeddings = torch.tensor(df[[c for c in df.columns if re.match(\"^d\\w*[0-9]\", c)]].values).unsqueeze(2).repeat(1, 1, k)\n",
    "else:\n",
    "    bert_embeddings = None\n",
    "\n",
    "x_data = torch.transpose(path, 1,2)\n",
    "\n",
    "if (time_feature != None):\n",
    "    x_data = torch.cat((x_data, time_feature), dim=1)\n",
    "if (bert_embeddings != None):\n",
    "    x_data = torch.cat((x_data, bert_embeddings), dim=1)\n",
    "\n",
    "x_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K fold training with repeatition of random seeds and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr= 0.005  g= 2  dp= 0.25  h_dim= 32  lstm_dim= (12, 10) conv output layers= [13]\n",
      "Starting random seed # 0  and fold # 0\n",
      "The size of train/valid/test timelines are:  268 132 100\n",
      "Samples in test set:  3773\n",
      "[0/100, 0/158] loss: 0.82982844\n",
      "[0/100, 100/158] loss: 0.52820849\n",
      "Current Macro F1: 54.14672170430298\n",
      "Trigger Times: 0\n",
      "[1/100, 0/158] loss: 0.38461044\n",
      "[1/100, 100/158] loss: 0.32350436\n",
      "Current Macro F1: 54.884949114920644\n",
      "Trigger Times: 0\n",
      "[2/100, 0/158] loss: 0.36699501\n",
      "[2/100, 100/158] loss: 0.34093463\n",
      "Current Macro F1: 47.574964731943176\n",
      "Trigger Times: 1\n",
      "[3/100, 0/158] loss: 0.26555252\n",
      "[3/100, 100/158] loss: 0.2768907\n",
      "Current Macro F1: 54.08805368098921\n",
      "Trigger Times: 0\n",
      "[4/100, 0/158] loss: 0.38603261\n",
      "[4/100, 100/158] loss: 0.31638068\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ttseriotou/signature_transforms/experiments/pathbert_talklife_deepsignature.ipynb Cell 17'\u001b[0m in \u001b[0;36m<cell line: 70>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B104.40.187.47/home/ttseriotou/signature_transforms/experiments/pathbert_talklife_deepsignature.ipynb#ch0000016vscode-remote?line=140'>141</a>\u001b[0m \u001b[39m#model train/validation per epoch\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B104.40.187.47/home/ttseriotou/signature_transforms/experiments/pathbert_talklife_deepsignature.ipynb#ch0000016vscode-remote?line=141'>142</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B104.40.187.47/home/ttseriotou/signature_transforms/experiments/pathbert_talklife_deepsignature.ipynb#ch0000016vscode-remote?line=143'>144</a>\u001b[0m     training(model, train_loader, criterion, optimizer, epoch, num_epochs)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B104.40.187.47/home/ttseriotou/signature_transforms/experiments/pathbert_talklife_deepsignature.ipynb#ch0000016vscode-remote?line=145'>146</a>\u001b[0m     \u001b[39m# Early stopping\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B104.40.187.47/home/ttseriotou/signature_transforms/experiments/pathbert_talklife_deepsignature.ipynb#ch0000016vscode-remote?line=147'>148</a>\u001b[0m     _ , f1_v, labels_val, predicted_val \u001b[39m=\u001b[39m validation(model, valid_loader, criterion)\n",
      "File \u001b[0;32m~/signature_transforms/experiments/classification_utils.py:200\u001b[0m, in \u001b[0;36mtraining\u001b[0;34m(model, train_loader, criterion, optimizer, epoch, num_epochs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ttseriotou/signature_transforms/experiments/classification_utils.py?line=196'>197</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m    <a href='file:///home/ttseriotou/signature_transforms/experiments/classification_utils.py?line=198'>199</a>\u001b[0m \u001b[39m# Updating parameters\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/ttseriotou/signature_transforms/experiments/classification_utils.py?line=199'>200</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    <a href='file:///home/ttseriotou/signature_transforms/experiments/classification_utils.py?line=201'>202</a>\u001b[0m \u001b[39m# Show training progress\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ttseriotou/signature_transforms/experiments/classification_utils.py?line=202'>203</a>\u001b[0m \u001b[39mif\u001b[39;00m (i \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/py38-MoC/lib/python3.8/site-packages/torch/optim/optimizer.py:89\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/ttseriotou/anaconda3/envs/py38-MoC/lib/python3.8/site-packages/torch/optim/optimizer.py?line=86'>87</a>\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m     <a href='file:///home/ttseriotou/anaconda3/envs/py38-MoC/lib/python3.8/site-packages/torch/optim/optimizer.py?line=87'>88</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> <a href='file:///home/ttseriotou/anaconda3/envs/py38-MoC/lib/python3.8/site-packages/torch/optim/optimizer.py?line=88'>89</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38-MoC/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/ttseriotou/anaconda3/envs/py38-MoC/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=23'>24</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     <a href='file:///home/ttseriotou/anaconda3/envs/py38-MoC/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=24'>25</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     <a href='file:///home/ttseriotou/anaconda3/envs/py38-MoC/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=25'>26</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m():\n\u001b[0;32m---> <a href='file:///home/ttseriotou/anaconda3/envs/py38-MoC/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=26'>27</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38-MoC/lib/python3.8/site-packages/torch/optim/adam.py:108\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ttseriotou/anaconda3/envs/py38-MoC/lib/python3.8/site-packages/torch/optim/adam.py?line=104'>105</a>\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    <a href='file:///home/ttseriotou/anaconda3/envs/py38-MoC/lib/python3.8/site-packages/torch/optim/adam.py?line=106'>107</a>\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m--> <a href='file:///home/ttseriotou/anaconda3/envs/py38-MoC/lib/python3.8/site-packages/torch/optim/adam.py?line=107'>108</a>\u001b[0m     F\u001b[39m.\u001b[39;49madam(params_with_grad,\n\u001b[1;32m    <a href='file:///home/ttseriotou/anaconda3/envs/py38-MoC/lib/python3.8/site-packages/torch/optim/adam.py?line=108'>109</a>\u001b[0m            grads,\n\u001b[1;32m    <a href='file:///home/ttseriotou/anaconda3/envs/py38-MoC/lib/python3.8/site-packages/torch/optim/adam.py?line=109'>110</a>\u001b[0m            exp_avgs,\n\u001b[1;32m    <a href='file:///home/ttseriotou/anaconda3/envs/py38-MoC/lib/python3.8/site-packages/torch/optim/adam.py?line=110'>111</a>\u001b[0m            exp_avg_sqs,\n\u001b[1;32m    <a href='file:///home/ttseriotou/anaconda3/envs/py38-MoC/lib/python3.8/site-packages/torch/optim/adam.py?line=111'>112</a>\u001b[0m            max_exp_avg_sqs,\n\u001b[1;32m    <a href='file:///home/ttseriotou/anaconda3/envs/py38-MoC/lib/python3.8/site-packages/torch/optim/adam.py?line=112'>113</a>\u001b[0m            state_steps,\n\u001b[1;32m    <a href='file:///home/ttseriotou/anaconda3/envs/py38-MoC/lib/python3.8/site-packages/torch/optim/adam.py?line=113'>114</a>\u001b[0m            group[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    <a href='file:///home/ttseriotou/anaconda3/envs/py38-MoC/lib/python3.8/site-packages/torch/optim/adam.py?line=114'>115</a>\u001b[0m            beta1,\n\u001b[1;32m    <a href='file:///home/ttseriotou/anaconda3/envs/py38-MoC/lib/python3.8/site-packages/torch/optim/adam.py?line=115'>116</a>\u001b[0m            beta2,\n\u001b[1;32m    <a href='file:///home/ttseriotou/anaconda3/envs/py38-MoC/lib/python3.8/site-packages/torch/optim/adam.py?line=116'>117</a>\u001b[0m            group[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    <a href='file:///home/ttseriotou/anaconda3/envs/py38-MoC/lib/python3.8/site-packages/torch/optim/adam.py?line=117'>118</a>\u001b[0m            group[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    <a href='file:///home/ttseriotou/anaconda3/envs/py38-MoC/lib/python3.8/site-packages/torch/optim/adam.py?line=118'>119</a>\u001b[0m            group[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    <a href='file:///home/ttseriotou/anaconda3/envs/py38-MoC/lib/python3.8/site-packages/torch/optim/adam.py?line=119'>120</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/py38-MoC/lib/python3.8/site-packages/torch/optim/_functional.py:92\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     <a href='file:///home/ttseriotou/anaconda3/envs/py38-MoC/lib/python3.8/site-packages/torch/optim/_functional.py?line=89'>90</a>\u001b[0m     denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39msqrt(bias_correction2))\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m     <a href='file:///home/ttseriotou/anaconda3/envs/py38-MoC/lib/python3.8/site-packages/torch/optim/_functional.py?line=90'>91</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///home/ttseriotou/anaconda3/envs/py38-MoC/lib/python3.8/site-packages/torch/optim/_functional.py?line=91'>92</a>\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39;49m math\u001b[39m.\u001b[39;49msqrt(bias_correction2))\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m     <a href='file:///home/ttseriotou/anaconda3/envs/py38-MoC/lib/python3.8/site-packages/torch/optim/_functional.py?line=93'>94</a>\u001b[0m step_size \u001b[39m=\u001b[39m lr \u001b[39m/\u001b[39m bias_correction1\n\u001b[1;32m     <a href='file:///home/ttseriotou/anaconda3/envs/py38-MoC/lib/python3.8/site-packages/torch/optim/_functional.py?line=95'>96</a>\u001b[0m param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import random\n",
    "from datetime import date\n",
    "import math\n",
    "\n",
    "from classification_utils import Folds, set_seed, validation, training, testing\n",
    "from deepsignatureffn import DeepSigNet, StackedDeepSigNet, FocalLoss, ClassBalanced_FocalLoss\n",
    "\n",
    "# ================================\n",
    "save_results = True\n",
    "# ================================\n",
    "\n",
    "#GLOBAL MODEL PARAMETERS\n",
    "augmentation_tp = model_specifics[\"augmentation_tp\"]\n",
    "input_channels = path.shape[2]\n",
    "output_channels =  [model_specifics[\"reduced_network_components\"]] #13#[10,13]\n",
    "augmentation_layers = () #[(32, 16, 10)] #(50, 20, output_channels) #\n",
    "BiLSTM = False\n",
    "sig_d = 3 \n",
    "blocks = 3\n",
    "post_dim = x_data.shape[1]- input_channels\n",
    "hidden_dim_lstm =  [(12, 8)] #12 [10,12]\n",
    "hidden_dim = [32] #32 [32,64] \n",
    "output_dim = 3\n",
    "loss = model_specifics[\"loss_function\"] #'focal' #cbfocal\n",
    "dropout_rate = [0.25]  #0.25 [0.25,0.35]\n",
    "if (model_specifics['time_injection_history_tp'] == 'timestamp'):\n",
    "    add_time = True\n",
    "else: \n",
    "    add_time = False\n",
    "\n",
    "# ================================\n",
    "num_epochs = 100\n",
    "learning_rate =  [0.0003] #0.0003 [0.0003, 0.0005]\n",
    "gamma = [2] #2 [2,3]\n",
    "beta = 0.999\n",
    "BATCH_SIZE = 64\n",
    "NUM_folds = 5\n",
    "patience = 2\n",
    "weight_decay_adam = 0.0001\n",
    "RANDOM_SEED_list = [0, 1, 12, 123, 1234]\n",
    "\n",
    "# ================================\n",
    "if (model_specifics['dimensionality_reduction'] == True):\n",
    "    model_code_name = model_specifics[\"global_embedding_tp\"]  \\\n",
    "    + \"_\" + str(model_specifics['dimensionality_reduction_tp']) + str(model_specifics['dimensionality_reduction_components']) \\\n",
    "    + \"_\" + str(model_specifics['time_injection_history_tp']) + str(model_specifics['time_injection_post_tp']) \\\n",
    "    + \"_\" + str(model_specifics['post_embedding_tp']) + \"_\" + str(model_specifics['feature_combination_method']) \\\n",
    "    + \"_\" + str(model_specifics['signature_tp']) + \"_\" + str(model_specifics['signature_dimensions']) \\\n",
    "    + \"_\" + str(model_specifics['classifier_name']) + \"_\" + str(model_specifics[\"loss_function\"]) \\\n",
    "    + \"_\" + str(model_specifics['classes_num']) \n",
    "else:\n",
    "    model_code_name = model_specifics[\"global_embedding_tp\"]  \\\n",
    "    + \"_\" + str(model_specifics['time_injection_history_tp']) + str(model_specifics['time_injection_post_tp']) \\\n",
    "    + \"_\" + str(model_specifics['post_embedding_tp']) + \"_\" + str(model_specifics['feature_combination_method']) \\\n",
    "    + \"_\" + str(model_specifics['signature_tp']) + \"_\" + str(model_specifics['signature_dimensions']) \\\n",
    "    + \"_\" + str(model_specifics['classifier_name']) + \"_\" + str(model_specifics[\"loss_function\"]) \\\n",
    "    + \"_\" + str(model_specifics['classes_num']) \n",
    "\n",
    "\n",
    "FOLDER_models = '/storage/ttseriotou/pathbert/models/v2/' \n",
    "FOLDER_results = '/storage/ttseriotou/pathbert/results/v2/' \n",
    "\n",
    "# ================================\n",
    "KFolds = Folds(num_folds=NUM_folds)\n",
    "y_data = KFolds.get_labels(df)\n",
    "# ================================\n",
    "#K FOLD RUNS\n",
    "ft_i = 0 #run number\n",
    "for out_ch in output_channels:\n",
    "    for lr in learning_rate:\n",
    "        for g in gamma:\n",
    "            for dp in dropout_rate:\n",
    "                for h_dim in hidden_dim:\n",
    "                    for lstm_dim in hidden_dim_lstm:\n",
    "                        #out_ch =  aug_l[2] \n",
    "                        str_version = 'tuning' + str(ft_i)\n",
    "                        print('lr=',lr, ' g=',g,' dp=',dp, ' h_dim=',h_dim, ' lstm_dim=', lstm_dim, 'conv output layers=',output_channels)\n",
    "                        ft_i+=1\n",
    "\n",
    "                        classifier_params = {\"augmentation_tp\": augmentation_tp,\n",
    "                        \"input_channels\": input_channels,\n",
    "                        \"output_channels\": out_ch,\n",
    "                        \"augmentation_layers\": augmentation_layers,\n",
    "                        \"sig_d\": sig_d,\n",
    "                        \"post_dim\": post_dim,\n",
    "                        \"hidden_dim_lstm\": lstm_dim,\n",
    "                        \"hidden_dim\": h_dim,\n",
    "                        \"output_dim\": output_dim,\n",
    "                        \"dropout_rate\": dp,\n",
    "                        \"num_epochs\": num_epochs,\n",
    "                        \"learning_rate\": lr,\n",
    "                        \"BiLSTM\": BiLSTM,\n",
    "                        \"blocks\": blocks,\n",
    "                        \"gamma\": g,\n",
    "                        \"BATCH_SIZE\": BATCH_SIZE,\n",
    "                        \"NUM_folds\": NUM_folds,\n",
    "                        \"patience\": patience,\n",
    "                        \"weight_decay_adam\": weight_decay_adam,\n",
    "                        \"RANDOM_SEED_list\": RANDOM_SEED_list,\n",
    "                        } \n",
    "                                        \n",
    "                        for my_ran_seed in RANDOM_SEED_list:\n",
    "                            set_seed(my_ran_seed)\n",
    "                            myGenerator = torch.Generator()\n",
    "                            myGenerator.manual_seed(my_ran_seed)    \n",
    "                            for test_fold in range(NUM_folds):\n",
    "\n",
    "                                print('Starting random seed #',my_ran_seed, ' and fold #', test_fold)\n",
    "                                #get ith-fold data\n",
    "                                x_test, y_test, x_valid, y_valid, x_train , y_train, test_tl_ids, test_pids = KFolds.get_splits(df, x_data, y_data, test_fold= test_fold)\n",
    "\n",
    "                                #data loaders with batches\n",
    "                                train = torch.utils.data.TensorDataset( x_train, y_train)\n",
    "                                valid = torch.utils.data.TensorDataset( x_valid, y_valid)\n",
    "                                test = torch.utils.data.TensorDataset( torch.cat((x_test,test_pids.unsqueeze(2).repeat(1, 1, k)),1) , y_test)\n",
    "\n",
    "                                train_loader = torch.utils.data.DataLoader(dataset=train, batch_size = BATCH_SIZE, shuffle = True)\n",
    "                                valid_loader = torch.utils.data.DataLoader(dataset=valid, batch_size = BATCH_SIZE, shuffle = True)\n",
    "                                test_loader = torch.utils.data.DataLoader(dataset=test, batch_size = BATCH_SIZE, shuffle = True)\n",
    "\n",
    "                    \n",
    "                                #early stopping params\n",
    "                                last_metric = 0\n",
    "                                trigger_times = 0\n",
    "                                best_metric = 0\n",
    "\n",
    "                                #model definitions\n",
    "                                #model = DeepSigNet(input_channels, output_channels, sig_d,  post_dim, hidden_dim, output_dim, dropout_rate, add_time, augmentation_tp, augmentation_layers)\n",
    "                                model = StackedDeepSigNet(input_channels, out_ch, sig_d, lstm_dim, post_dim, h_dim, output_dim, dp, add_time, augmentation_tp, augmentation_layers, BiLSTM, comb_method=model_specifics['feature_combination_method'], blocks=blocks)\n",
    "                                #loss function\n",
    "                                if (loss=='focal') :\n",
    "                                    alpha_values = torch.Tensor([math.sqrt(1/(y_train[y_train==0].shape[0]/y_train.shape[0])), math.sqrt(1/(y_train[y_train==1].shape[0]/y_train.shape[0])), math.sqrt(1/(y_train[y_train==2].shape[0]/y_train.shape[0]))])\n",
    "                                    criterion = FocalLoss(gamma = g, alpha = alpha_values)\n",
    "                                elif (loss == 'cbfocal'):\n",
    "                                    classifier_params[\"beta\"] = beta\n",
    "                                    samples_count = torch.Tensor([y_train[y_train==0].shape[0], y_train[y_train==1].shape[0], y_train[y_train==2].shape[0]])\n",
    "                                    criterion = ClassBalanced_FocalLoss(gamma = g, beta = beta, no_of_classes=3, samples_per_cls=samples_count)                               \n",
    "                                optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay= weight_decay_adam)\n",
    "\n",
    "                                #model train/validation per epoch\n",
    "                                for epoch in range(num_epochs):\n",
    "\n",
    "                                    training(model, train_loader, criterion, optimizer, epoch, num_epochs)\n",
    "                                \n",
    "                                    # Early stopping\n",
    "                                    \n",
    "                                    _ , f1_v, labels_val, predicted_val = validation(model, valid_loader, criterion)\n",
    "                                    print('Current Macro F1:', f1_v)\n",
    "\n",
    "                                    if f1_v > best_metric :\n",
    "                                        best_metric = f1_v\n",
    "\n",
    "                                        #test and save so far best model\n",
    "                                        predicted_test, labels_test, pids_test = testing(model, test_loader)\n",
    "\n",
    "                                        results = {\n",
    "                                            \"model_code_name\": model_code_name, \n",
    "                                            \"model_specifics\": model_specifics, \n",
    "                                            \"classifier_params\": classifier_params, \n",
    "                                            \"date_run\": date.today().strftime(\"%d/%m/%Y\"),\n",
    "                                            \"test_tl_ids\": test_tl_ids,\n",
    "                                            \"test_pids\": pids_test, #test_pids,\n",
    "                                            \"labels\": labels_test,\n",
    "                                            \"predictions\": predicted_test,\n",
    "                                            \"labels_val\": labels_val,\n",
    "                                            \"predicted_val\": predicted_val,\n",
    "                                            \"test_fold\": test_fold,\n",
    "                                            \"random_seed\": my_ran_seed,\n",
    "                                            \"epoch\": epoch,\n",
    "                                        }\n",
    "\n",
    "                                        if (save_results==True):\n",
    "                                            #file_name_results = FOLDER_results + model_code_name + \"_\" + str(my_ran_seed) + \"seed\" + \"_\" + str(test_fold) + \"fold\" + \"_\" + str_version + '.pkl'\n",
    "                                            #file_name_model = FOLDER_models + model_code_name + \"_\" + str(my_ran_seed) + \"seed\" + \"_\" + str(test_fold) + \"fold\" + \"_\" + str_version +'.pkl'\n",
    "                                            file_name_results = FOLDER_results + model_code_name + \"_\" + str(my_ran_seed) + \"seed\" + \"_\" + str(test_fold) + \"fold\"+ '.pkl'\n",
    "                                            file_name_model = FOLDER_models + model_code_name + \"_\" + str(my_ran_seed) + \"seed\" + \"_\" + str(test_fold) + \"fold\"  +'.pkl'\n",
    "                                            pickle.dump(results, open(file_name_results, 'wb'))\n",
    "                                            torch.save(model.state_dict(), file_name_model)\n",
    "\n",
    "                                    if f1_v < last_metric:\n",
    "                                        trigger_times += 1\n",
    "                                        print('Trigger Times:', trigger_times)\n",
    "\n",
    "                                        if trigger_times >= patience:\n",
    "                                            print('Early stopping!')\n",
    "                                            break\n",
    "\n",
    "                                    else:\n",
    "                                        print('Trigger Times: 0')\n",
    "                                        trigger_times = 0\n",
    "\n",
    "                                    last_metric = f1_v\n",
    "                            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]\n",
      "lr= 0.0003 g= 2 dp= 0.25 h_dim= 32 lstm_dim= (12, 8) conv out= 13\n",
      "              precision    recall  f1-score       support\n",
      "O              0.909072  0.870078  0.889148  20189.000000\n",
      "IE             0.438231  0.589409  0.502700   2606.000000\n",
      "IS             0.309544  0.301003  0.305214   1196.000000\n",
      "accuracy       0.811221  0.811221  0.811221      0.811221\n",
      "macro avg      0.552282  0.586830  0.565687  23991.000000\n",
      "weighted avg   0.828040  0.811221  0.818060  23991.000000\n",
      "lr= 0.0003 g= 2 dp= 0.25 h_dim= 64 lstm_dim= (12, 8) conv out= 13\n",
      "              precision    recall  f1-score       support\n",
      "O              0.905679  0.878449  0.891856  20189.000000\n",
      "IE             0.464736  0.566385  0.510550   2606.000000\n",
      "IS             0.286294  0.295151  0.290655   1196.000000\n",
      "accuracy       0.815472  0.815472  0.815472      0.815472\n",
      "macro avg      0.552236  0.579995  0.564353  23991.000000\n",
      "weighted avg   0.826904  0.815472  0.820466  23991.000000\n",
      "lr= 0.0001 g= 2 dp= 0.25 h_dim= 32 lstm_dim= (12, 10) conv out= 13\n",
      "              precision    recall  f1-score       support\n",
      "O              0.907701  0.873396  0.890218  20189.000000\n",
      "IE             0.434709  0.594014  0.502027   2606.000000\n",
      "IS             0.322709  0.270903  0.294545   1196.000000\n",
      "accuracy       0.813013  0.813013  0.813013      0.813013\n",
      "macro avg      0.555040  0.579438  0.562264  23991.000000\n",
      "weighted avg   0.827160  0.813013  0.818356  23991.000000\n",
      "lr= 0.0003 g= 2 dp= 0.25 h_dim= 32 lstm_dim= (12, 6) conv out= 13\n",
      "              precision    recall  f1-score       support\n",
      "O              0.905813  0.873644  0.889438  20189.000000\n",
      "IE             0.447622  0.570606  0.501687   2606.000000\n",
      "IS             0.293233  0.293478  0.293356   1196.000000\n",
      "accuracy       0.811804  0.811804  0.811804      0.811804\n",
      "macro avg      0.548889  0.579243  0.561494  23991.000000\n",
      "weighted avg   0.825504  0.811804  0.817603  23991.000000\n",
      "lr= 0.0005 g= 2 dp= 0.25 h_dim= 32 lstm_dim= (12, 10) conv out= 13\n",
      "              precision    recall  f1-score       support\n",
      "O              0.907260  0.867849  0.887117  20189.000000\n",
      "IE             0.437428  0.582118  0.499506   2606.000000\n",
      "IS             0.293972  0.297659  0.295804   1196.000000\n",
      "accuracy       0.808386  0.808386  0.808386      0.808386\n",
      "macro avg      0.546220  0.582542  0.560809  23991.000000\n",
      "weighted avg   0.825651  0.808386  0.815535  23991.000000\n"
     ]
    }
   ],
   "source": [
    "#BEST K MODELS - VALIDATION LOOP\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "\n",
    "k=5\n",
    "FOLDER_results = '/storage/ttseriotou/pathbert/results/v2/' \n",
    "model_code_name = 'SBERT_umap15_Nonetimestamp_sentence_concatenation_log_3_Conv1d3kernelSigLSTMSigLSTMSigFFN2hidden_focal_3class'\n",
    "metrics_overall = pd.DataFrame(0, index = ['O', 'IE', 'IS', 'accuracy', 'macro avg', 'weighted avg'], columns = ['precision', 'recall', 'f1-score', 'support'])\n",
    "\n",
    "#get all tuning files\n",
    "per_model_files = [f for f in listdir(FOLDER_results) if 'tuning' in f if model_code_name in f]\n",
    "\n",
    "#get the indices of tuning files\n",
    "files_ind = [int(f[:f.index(\".\")].split(\"_\")[-1].replace('tuning', '')) for f in per_model_files]\n",
    "files_ind = list(set(files_ind))\n",
    "dict_f1 = {}\n",
    "\n",
    "print(files_ind)\n",
    "for t in files_ind:\n",
    "    labels_final = torch.empty((0))\n",
    "    predicted_final = torch.empty((0))\n",
    "\n",
    "    tuning_files = [f for f in per_model_files if ('tuning'+str(t)) in f]\n",
    "\n",
    "    for sf in tuning_files:\n",
    "        with open(FOLDER_results+sf, 'rb') as fin:\n",
    "            results = pickle.load(fin)\n",
    "            labels_results = results['labels_val']\n",
    "            predictions_results = results['predicted_val']\n",
    "        \n",
    "        #for each seed combine fold results\n",
    "        labels_final = torch.cat([labels_final, labels_results])\n",
    "        predicted_final = torch.cat([predicted_final, predictions_results])\n",
    "\n",
    "    #calculate metrics for each seed\n",
    "    metrics_tab = metrics.classification_report(labels_final, predicted_final, target_names = ['O','IE','IS'], output_dict=True)\n",
    "    metrics_tab = pd.DataFrame(metrics_tab).transpose()\n",
    "    f1 = metrics_tab['f1-score']['macro avg']\n",
    "    dict_f1[t] = f1\n",
    "\n",
    "dict_f1 = Counter(dict_f1)\n",
    "\n",
    "for top in dict_f1.most_common()[:k]:\n",
    "    labels_final = torch.empty((0))\n",
    "    predicted_final = torch.empty((0))\n",
    "\n",
    "    tuning_files = [f for f in per_model_files if ('tuning'+str(top[0])) in f]\n",
    "\n",
    "    for sf in tuning_files:\n",
    "        with open(FOLDER_results+sf, 'rb') as fin:\n",
    "            results = pickle.load(fin)\n",
    "            labels_results = results['labels_val']\n",
    "            predictions_results = results['predicted_val']\n",
    "        \n",
    "        #for each seed combine fold results\n",
    "        labels_final = torch.cat([labels_final, labels_results])\n",
    "        predicted_final = torch.cat([predicted_final, predictions_results])\n",
    "\n",
    "    #calculate metrics for each seed\n",
    "    metrics_tab = metrics.classification_report(labels_final, predicted_final, target_names = ['O','IE','IS'], output_dict=True)\n",
    "    metrics_tab = pd.DataFrame(metrics_tab).transpose()\n",
    "    params = results['classifier_params']\n",
    "    print('lr=',params['learning_rate'], 'g=',params['gamma'],'dp=',params['dropout_rate'], 'h_dim=',params['hidden_dim'], 'lstm_dim=', params['hidden_dim_lstm'], 'conv out=',params['output_channels'])\n",
    "    print(metrics_tab)\n",
    "    #print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py38esig)",
   "language": "python",
   "name": "py38esig"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
